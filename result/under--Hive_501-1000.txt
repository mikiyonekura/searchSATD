DataOutputBuffer port_dob = new DataOutputBuffer();
// renaming test to make test framework skip it
too long.
too long.
}
return;
// projections from child?
Token<LlapTokenIdentifier> token = new Token<LlapTokenIdentifier>(llapId, this);
for (RelOptRule r : rules)
too long.
// we can generate ranges from e.g. rowid > (4 + 5)
constantPropDistinctUDAFParam = SemanticAnalyzer
String clientId = getClientId(jobId);
if (ifName.equals(format)) {
too long.
inputVrb.cols[ixInVrb] = cvb.cols[ixInReadSet];
schema.setProperty(serdeConstants.QUOTE_CHAR, "(\"|\\[|\\])");
List<String> tabBucketCols = tab.getBucketCols();
too long.
return false;
cachedObjectInspector = ObjectInspectorFactory
for (FileStatus fileStatus : contents) {
// data with the separator bytes before creating a "Put" object
if (obAST != null && !(selForWindow != null && selExprList.getToken().getType() == HiveParser.TOK_SELECTDI) && !isAllColRefRewrite) {
return table;
final JobConf cloneJobConf = new JobConf(jobConf);
// fall through
if (joinRel instanceof SemiJoin) {
for (int j = 0; j < 10; ++j) {
too long.
validateRestrictedConfigValues(var.varname, userValue, serverValue);
ctx.hiveConf = new HiveConf(IDriver.class);
too long.
// This is only required to support the deprecated methods in HCatAddPartitionDesc.Builder.
for (Partition partition : metadata.getPartitions()) {
too long.
too long.
throw new HiveException("Converting list bucketed tables stored as subdirectories "
if (oldtbl
boolean hasFileId = this.fileKey != null;
// From the value arrays and our isRepeated, selected, isNull arrays, generate the batch!
// NullPointerException, remote throws TTransportException
LOG.info("DP can be rewritten to SP!");
if (SessionState.get() != null) {
try {
private final FixedSizedObjectPool<IoTrace> tracePool;
public class TableExport {
@Override
public String getApplyPathScriptPath() {
too long.
String[] jarPaths = auxJars.split(delimiter);
coordinator = LlapCoordinator.getInstance();
private static final String BASE_PREFIX = "base_", DELTA_PREFIX = "delta_",
// in all hadoop versions.
dummyOp.initialize(jconf, null);
UserGroupInformation ugi = Utils.getUGI();
return OrcFile.createWriter(path, createOrcWriterOptions(oi, conf, cacheWriter, allocSize));
long evicted = evictor.evictSomeBlocks(remainingToReserve);
private List<AggrColStats> nodes = new ArrayList<>();
//this uses VectorizedOrcAcidRowBatchReader
too long.
localDirs = conf.getTrimmedStrings(SHUFFLE_HANDLER_LOCAL_DIRS);
List<String> versionFiles = QTestUtil.getVersionFiles(queryDirectory, tname);
too long.
// Finally add the partitioning columns
int badCallCount = 0;
return sortColNames.subList(0, joinCols.size()).containsAll(joinCols);
verify(sessionState).ensureLocalResources(any(Configuration.class), eq(inputOutputJars));
return get(fieldName, recordSchema);
// moved...this may change
too long.
too long.
Assert.assertTrue("Unexpected: HCAT_PARTITION_DONE_EVENT not supported (yet).", false);
for (IOSpecProto inputSpec : vertex.getInputSpecsList()) {
caughtException = tae;
Integer numReducersFromWork = rWork == null ? 0 : rWork.getNumReduceTasks();
if (null == rangeSplit.getIterators()
too long.
pw.println("-- These commands may be executed by Hive 1.x later");
too long.
private static ExprNodeDesc propConstDistUDAFParams() {
OrcProto.ColumnEncoding columnEncoding = encodings.get(columnIndex);
boolean nonDefaultUser = conf.getBoolVar(HiveConf.ConfVars.HIVE_SERVER2_ENABLE_DOAS);
/*
if (!isBlacklistWhitelistEmpty(conf) || !isCachePrewarmed.get()) {
return super.explainTerms(pw)
// prefix = work.getAggKey();
if (context.linkChildOpWithDummyOp.containsKey(mj)) {
public synchronized String getDelegationTokenFromMetaStore(String owner)
if (cacheStripeDetails) {
if (fsOp.getConf().getDynPartCtx() != null) {
ret.numRows = newRowCount;
runStatementOnDriver("create table myctas2 stored as ORC TBLPROPERTIES ('transactional" +
public class OpWalkerCtx implements NodeProcessorCtx {
ByteBuffer copy = ByteBuffer.allocate(bb.capacity());
private TCLIService.Iface client;
private Builder(String dbName, String tableName, List<HCatFieldSchema> columns) {
ExprNodeConverter exprConv = new ExprNodeConverter(inputTabAlias, inputRel.getRowType(),
validateTableCols(table, colNames);
t.addDependentTask(alterTable);
// 3. Return result
setOperationException(e);
return new Text(objInspector.getWritableConstantValue().toString());
too long.
final RelMetadataQuery mq = call.getMetadataQuery();
System.exit(rc);
ctx.close();
ColumnStreamData[] datas = ecb.getColumnData(colIx);
break;
// Tracks tasks which could not be allocated immediately.
RelCollation canonizedCollation = traitSet.canonize(newCollation);
public void testAddPartitionEmptyValue() throws Exception {
resultObjects[rowIndex++] = scrqtchRow[0];
ASTNode queryForCbo = ast;
SignatureUtils.write(sigMap, op.getConf());
if (valToPaths.size() > maxPartitions) {
if (failed > 0) {
@VisibleForTesting
resultObjects[rowIndex++] = scrqtchRow[0];
public class CachedStore implements RawStore, Configurable {
return getRecordUpdater(jc, acidOutputFormat,
too long.
UnsignedInt128 scratch = new UnsignedInt128();
too long.
clean(compactId2CompactInfoMap.get(queueEntry.getKey()));
private long numRowsCompareHashAggr;
import org.apache.hadoop.io.retry.RetryPolicies;
//       that fractions or query parallelism add up, etc.
}
List<Integer> cwColIds = writer.isOnlyWritingIncludedColumns() ? splitColumnIds : columnIds;
private JobMetricsListener jobMetricsListener;
String attemptId = Converters.createTaskAttemptId(context).toString();
public MappingInput(String userName, List<String> groups, String wmPool, String appName) {
}
too long.
if (fos.size() > 0 && oss.size() > 0) {
return;
client.dropDatabase(dbName1, true, true, true);
QueryIdentifier queryId = executorService.findQueryByFragment(fragmentId);
throw new AssertionError("Unsupported mode");
HCatUtil.getHiveMetastoreClient(hiveConf);
public String getLocation() {
if (sendCounters) {
Assert.assertEquals(initialCount + 1,
// only mechanical data retrieval should remain here.
stats.addToDataSize(getDataSizeFromColumnStats(nr, columnStats));
if (children.contains(null)) {
for (int i = 1; i < numDistinctExprs; i++) {
tableValue += (" and " + TBLS + ".\"TBL_NAME\" = ? and " + DBS + ".\"NAME\" = ? and "
// https://issues.apache.org/jira/browse/HIVE-17627
//extra heartbeat is logically harmless, but ...
public Builder partCols(List<HCatFieldSchema> partCols) {
throw new IOException("Couldn't write to node " + id, nfe);
int formatScale = 0 + r.nextInt(38);
String eventType = event.getEventType();
too long.
Deque<Object> stack = createWorkStack(rootObj, byType);
// way!
assert result.length == files.size();
int wndSpecASTIndx = getWindowSpecIndx(windowProjAst);
if (cacheEncodings == null) {
private final static void setEnv(Map<String, String> newenv) throws Exception {
PerfLogger perfLogger = SessionState.getPerfLogger();
OpAttr visit(HiveFilter filterRel) throws SemanticException {
RelDataTypeFactory dtFactory = cluster.getRexBuilder().getTypeFactory();
None
for (OperationType opType : values()) {
/** Linked list pointers for LRFU/LRU cache policies. Given that each block is in cache
// different, or duplicate some other function).
return "";
LOG.trace("Cannot determine map type: {}", field);
@Override
// don't go  through Initiator for user initiated compactions)
// lock correctly. See the comment on the lock field - the locking needs to be reworked.
removeEnv.add("HADOOP_ROOT_LOGGER");
success = sql(line.substring("all ".length())) && success;
}
too long.
newCacheDataForCol[streamIx] = stream.data.toArray(new LlapSerDeDataBuffer[stream.data.size()]);
private static byte[] join(String[] items, char separator) {
if(ci.type == null) { ci.type = CompactionType.MINOR; }
if (isAllParts) {
AddPartitionDesc partsDesc = getBaseAddPartitionDescFromPartition(fromPath, dbname, tblDesc, partition);
Path scratchDir = utils.createTezDir(ctx.getMRScratchDir(), job);
private VectorDeserializeRow() {
} else if (isCompare && (func.getChildren().size() == 2)) {
InStream stream = ((StringDirectTreeReader) reader).getStream();
Connection conn = null;
too long.
// if the default was decided by the serde
public Builder sortCols(ArrayList<Order> sortCols) {
sendError(ctx, FORBIDDEN);
// NullPointerException, remote throws TTransportException
too long.
too long.
too long.
public String getDatabaseName() {
if (endReason != null && EnumSet
threadPool.submit(new DateTestCallable(bad, timeZone)).get();
if (buffer.declaredCachedLength != LlapDataBuffer.UNKNOWN_CACHED_LENGTH) {
void init(AtomicBoolean stop, AtomicBoolean looped) throws MetaException;
String checksumString = null;
class Direct implements OpTreeSignatureFactory {
if (opType == OpType.DELETE) {
too long.
private static final Set<String> llapDaemonVarsSet;
this.evolution = sef.createSchemaEvolution(fileMetadata.getSchema());
public String getComments() {
// child process. so we add it here explicitly
too long.
HashPartition hashPartition = new HashPartition(1024, (float) 0.75, 524288, 1, true, null);
File qf = new File(outDir, fileName);
too long.
None
VectorExpression[] child = new VectorExpression[1];
//       This only lives for the duration of the service init.
boolean dbHasJoinCastBug = DatabaseProduct.hasJoinOperationOrderBug(dbType);
public void testConstraints() throws IOException {
OpAttr visit(HiveTableScan scanRel) {
runStatementOnDriver("alter table " + Table.NONACIDORCTBL + " SET TBLPROPERTIES ('transactional'='true', 'transactional_properties'='default')");
public PPart(Table table, Partition partiton) {
if (!status.isDir()) {
too long.
// since we cannot directly set the private byte[] field inside Text.
too long.
@Test
// Assuming the used memory is equally divided among all executors.
for (org.apache.hadoop.hive.metastore.api.Partition outPart
newValue = State.switchFlag(newValue, State.FLAG_NEW_ALLOC);
int rc = setFSPermsNGrp(ss, driver.getConf());
}
boolean[] included = new boolean[schema.size()];
outputColVector.init();
too long.
public class JsonSerDe extends AbstractSerDe {
tmpNoNulls = getMaxNulls(stats, ((ExprNodeFieldDesc) pred).getDesc());
// codes and messages. This should be fixed.
JSON_MAPPER.setTimeZone(TimeZone.getTimeZone("UTC"));
if (!doesTimeMatter) return daysToMillis(d + 1) - (MILLIS_PER_DAY >> 1);
throw new RuntimeException(e.getCause());
synchronized (watchesPerAttempt) {
Preconditions.checkState(maxJvmMemory >= memRequired,
this.writeBuffers = writeBuffers;
public Builder escapeChar(char escapeChar) {
// 'n' columns where 'n' is the length of the bucketed columns.
private final static String tid =
if (ndvToBeSmoothed > 3)
private static boolean add(RowResolver rrToAddTo, RowResolver rrToAddFrom,
// For eg: select count(1) from T where t.ds = ....
too long.
if (task instanceof TaskAttempt) {
LOG.debug("Found " + deltas.size() + " delta files, threshold is " + deltaNumThreshold +
cumulativeBackoffFactor = cumulativeBackoffFactor * blacklistConf.backoffFactor;
too long.
too long.
Assert.fail("Expected a NullPointerException or TTransportException to be thrown");
LlapNodeId nodeId = LlapNodeId.getInstance(hostname, port);
too long.
boolean isBrokenUntilMapreduce7086 = "TEXTFILE".equals(format);
conf.setBoolean(ColumnProjectionUtils.READ_ALL_COLUMNS, false);
TezAttemptArray aw = new TezAttemptArray();
return null;
String loggedInUserName = SessionState.get().getUserName();
if (!distParamInRefsToOutputPos.containsKey(argLst.get(i))
too long.
SessionState.setCurrentSessionState(parentSessionState);
if (isLocationSet) {
private PrimitiveType getElementType(Type type) {
public Builder mapKeysTerminatedBy(char delimiter) {
private final QueryFragmentCounters counters;
return true;
// We need to consolidate 2 or more buffers into one to decompress.
// The wait queue should be able to fit at least (waitQueue + currentFreeExecutor slots)
primary.run("drop database if exists " + primaryDbName + " cascade");
public String getFileFormat() {
check.replaceSelfWith(new IncompleteCb(check.getOffset(), check.getEnd()));
switch (((PrimitiveTypeInfo)columnTypes.get(i)).getTypeName()) {
if (!unusedTriggers.isEmpty()) {
MockFileSystem.clearGlobalFiles();
return false;
// same as in getRecordReader?
too long.
private final Object lock = new Object();
private interface Field {
too long.
SessionState ss = SessionState.get();
too long.
None
None
// (will need to handle an alternate work-dir as well in this case - derive from branch?)
too long.
if (requestedHosts != null && requestedHosts.length != 0) {
too long.
runStatementOnDriver("delete from " + Table.ACIDTBL + " where a in(select a from " + Table.NONACIDORCTBL + ")");
String hiveQueryId;
public static void floor(int i, HiveDecimal input, DecimalColumnVector outputColVector) {
too long.
private boolean isExprResolver;
too long.
boolean isFirst = true;
public static Path generateTmpPathForPartitionPruning(Path basePath, String id) {
private static final Cache<String, LlapTokenLocalClient> localClientCache = CacheBuilder
// LOG4J2-1292 utilize gc-free Layout.encode() method: taken care of in superclass
for (int i = 0; i < locInfo.length; i++) {
too long.
super(LlapPluginEndpointClientImpl.class.getSimpleName(),
req.setCapabilities(new ClientCapabilities(
public boolean getExternal() {
// See: SPARK-21187
shouldRun.compareAndSet(true, false);
return new OpAttr("", new HashSet<Integer>(), rsGBOp2);
too long.
/** Planner rule that adjusts projects when counts are added. */
// What we need is a way to get buckets not splits
if (queryParallelism == null && likeName == null) {
None
if (work.isClearAggregatorStats()) {
List<Integer> filePhysicalColumnIds = readerLogicalColumnIds;
too long.
String line;
throw new SemanticException(ErrorMsg.CTAS_PARCOL_COEXISTENCE.getMsg());
this.parameters.put(ReplChangeManager.SOURCE_OF_REPLICATION, parameters.get(key));
// Requires schema change
for (int i = 0; i < logSize; ++i) {
if (sr.dataSize > sr.maxDataSize) {
too long.
* @deprecated Use MetastoreConf.DATANUCLEUS_INIT_COL_INFO
if (fastScale == 0) {
int threadCount = HiveConf.getIntVar(pctx.getConf(),
private void registerAllFunctionsOnce() throws HiveException {
return (T) super.getObject(columnIndex);
e.printStackTrace();
private DynamicServiceInstanceSet instances;
List<Path> finalDirs = null, dirsWithMmOriginals = null;
too long.
if (maxCacheSizeInBytes > 0) {
if (LOG.isInfoEnabled()) {
too long.
LOG.info("Converting {} to full transactional table", getQualifiedName(tableObj));
List<Partition> partitionEntries = metaStoreClient.listPartitions(table.getDbName(), table.getTableName(),
syncWork.toRestartInUse.add(session);
too long.
public static InputSplit createTableSnapshotRegionSplit() {
errorMessage = "FAILED: Hive Internal Error: " + Utilities.getNameMessage(e);
return value;
deepCopyPartitions(r.getPartitions(), result);
preemptedTaskList = preemptTasksFromMap(speculativeTasks, forPriority, forVertex,
// walking through all active nodes, if they don't have potential capacity.
while (!threadPool.awaitTermination(10, TimeUnit.SECONDS)) {
if (expr.getType() == HiveParser.TOK_DATELITERAL) {
LOG.debug("Done retrieving all objects for getPartitionsViaOrmFilter");
}
DataInputByteBuffer in = new DataInputByteBuffer();
return -1;
private static final int[] BUCKET_COLUMN_INDEXES = new int[] { 0 };
public Builder comments(String comment) {
@Test
if (existing == null) {
public static final byte NULL = 1;
FileStatus status = getFileStatus(f);
boolean includeGrpSetInGBDesc = (gbInfo.grpSets.size() > 0)
executor = new StatsRecordingThreadPool(1, 1,
None
too long.
if (splitStrategy instanceof ETLSplitStrategy) {
if (task instanceof ConditionalTask) {
public class TopNHash {
@Deprecated
int i = gbExprNDescLst.size();
LOG.warn("Skipping unknown directory " + dirName
public class VectorizedListColumnReader extends BaseVectorizedColumnReader {
//       DagClient as such should have no bearing on jobClose.
semiJoin = false;
return null;
too long.
StatsSetupConst.setBasicStatsState(parameters, StatsSetupConst.FALSE);
if (client == null) {
if (hasSpaceForCacheEntry(entry, size)) {
if (tbl.getSd().getLocation() == null) {
runtimeWatch.start();
if (!enablePreemption || preemptionQueue.isEmpty()) {
private String resultsDirectory;
if (!isTxnTable && ((loadFileType == LoadFileType.REPLACE_ALL) || (oldPart == null && !isAcidIUDoperation))) {
too long.
too long.
inpFormat = CombineHiveInputFormat.class.getName();
{
//       older-node tasks proactively. For now let the heartbeats fail them.
if (hiveConf != null) {
firePreEvent(new PreDropPartitionEvent(tbl, part, deleteData, this));
return Float.parseFloat(toFormalString());
too long.
too long.
ponderNextBufferToRead(readPos);
String sh = tbl.getStorageHandler().toString();
return StageType.REPL_DUMP;
path = System.getProperty(TEST_ENV_WORKAROUND + envVar);
too long.
sb.append("\nMetadata cache state: ").append(metadata.size()).append(
newSession.getConf().set(TezConfiguration.TEZ_QUEUE_NAME, queueName);
@Override
// firstFetchHappened == true. In reality it almost always calls joinOneGroup. Fix it?
if (_dataStream != null && _dataStream.available() > 0) {
SelectDesc sd = new SelectDesc(exprCols, exprNames);
// an exception
// in addition to that Druid allow numeric dimensions now so this check is not accurate
public String getTableName() {
if (getSubjectMethod == null) {
return new PathFilter() {
}
too long.
public final static String stringifyDiskRanges(DiskRangeList range) {
boolean[] readerIncludes = OrcInputFormat.genIncludedColumns(
if (i > 0) {
//       after all the perf changes that we might was well hardcode them separately.
public Builder linesTerminatedBy(char delimiter) {
validate = false;
// Currently using fileuri#checksum#cmrooturi#subdirs as the format
runStatementOnDriver("alter table nobuckets compact 'major'");
WindowFunctionSpec wFn = (WindowFunctionSpec) getWindowExpressions().get(0);
too long.
HashPartition hashPartition = new HashPartition(1024, (float) 0.75, 524288, 1, true, null);
user = System.getenv(ApplicationConstants.Environment.USER.name());
too long.
FileSplit sliceSplit = new FileSplit(split.getPath(), split.getStart(),
} else {
if (maxCacheSizeInBytes > 0) {
too long.
// proper index of a dummy.
// set method. This requires calcite change
too long.
return t;
if (sp >= 0 & (sp + 1) < udfClassName.length()) {
getContext().taskKilled(taskAttemptId,
FileSplit splitPart = new FileSplit(split.getPath(), uncachedSuffixStart,
appStatusBuilder.setAmInfo(
too long.
LOG.warn("Couldn't find aliases for " + splitPath);
MapOutputInfo outputInfo = new MapOutputInfo(pathInfo.dataPath, info);
public class TableExport {
dataSchema = hcatSplit.getDataSchema();
throw new IllegalArgumentException("Expected match count is 1; but got:" + all);
too long.
return Arrays.copyOf(sourceBw.getBytes(), sourceBw.getLength());
too long.
return false;
break;
if (requestedHostIdx == -1) {
throw (OutOfMemoryError) e;
LOG.error("Failed to start LLAP Daemon with exception", t);
//       move the setupPool code to ctor. For now, at least hasInitialSessions will be false.
private RowResolver outerRR;
recordReader = reader.rowsOptions(options, conf);
public Builder fileFormat(String format) {
String udafName = SemanticAnalyzer.getColumnInternalName(reduceKeys.size());
MapWork mapWork = Utilities.getMapWork(hiveConf);
//    assertEquals(null,stats.getSum());
LlapIoImpl.LOG.error("decodeBatch threw", ex);
private Map<String, Object> makeOneTablePartition(String partIdent)
if (abortTxns(dbConn, Collections.singletonList(txnid), true) != 1) {
configureAmRegistry(newSession);
}
if (input != colSrcRR) {
private final AtomicReference<InetSocketAddress> srvAddress = new AtomicReference<>(),
too long.
too long.
String intermediateRecordsCounterName = formattedName(
InputFormat format = inputFormats.get(inputFormatClass.getName());
maxLength = -1;
long totalLength;
AcidUtils.Directory dirInfo = AcidUtils.getAcidState(
// could generate different error messages
Path warehousePath;
public void setStmtId(int stmtId) {
case LIST:
checkAndSetFileOwnerPermissions(fs, tablePath,
//       into LlapNodeId. We get node info from registry; that should (or can) include it.
TypeInfo typeInfo = TypeInfoUtils.getTypeInfoFromTypeString(
Properties props = outputJobInfo.getTableInfo().getStorerInfo().getProperties();
public int getNumBuckets() {
System.setProperty("https.protocols", "TLSv1,TLSv1.1,TLSv1.2");
ByteBuffer restored = OrcInputFormatForTest.caches.cache.get(key).data;
hadoopAuth = conf.get(HADOOP_SECURITY_AUTHENTICATION, "simple");
// can be inherited from a base class.
public static Builder create(String dbName,
if (curErr instanceof org.apache.hadoop.security.AccessControlException
switch (queryState.getHiveOperation() == null ? HiveOperation.QUERY : queryState.getHiveOperation()) {
List<String> transactionalTables = tablesFromReadEntities(inputs)
}
}
}
protected transient Object[][] cachedKeys;
too long.
try {
None
for (RelDataTypeField field : rowType.getFieldList()) {
if (tableFieldTypeInfo.getCategory() != Category.PRIMITIVE) {
String query = "insert overwrite table " + tmpName + " ";
String message = null;
// just last one.
