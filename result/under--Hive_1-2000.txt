private static boolean areMergeable(ParseContext pctx, SharedWorkOptimizerCache optimizerCache,
State s;
bucketColumns.add(new ExprNodeColumnDesc(ci));
} while (result != null);
too long.
private final SemanticAnalyzer                              semanticAnalyzer;
for (Map.Entry<Long, Long> missingChunk : chunksInThisRead.entrySet()) {
None
too long.
System.out.println("Getting versions from " + queryDir);
throw new HiveException(ex, ErrorMsg.DATABSAE_ALREADY_EXISTS, crtDb.getName());
private PreInsertTableDesc preInsertTableDesc;
throw new RuntimeException("ContainerInfo not found for container: " + containerId +
byte[] bytes = hiveVarchar.getValue().getBytes();
update();
public Map<String, String> getTblProps() {
public void testTriggerSlowQueryExecutionTime() throws Exception {
int nParts = partNames.size();
//       supports all types
private FooterCache footerCache;
String parts[] = jobIdString.split("_");
desc = new FileSinkDesc(basePath, tableDesc, false, 1, false,
s = "select count(*) from COMPLETED_TXN_COMPONENTS where CTC_TXNID = " + txnid;
// Assert.assertEquals("expected uri", api.getAddedResource("jar"));
if (this.canUpdateFinishable) {
RowResolver inputRR = this.relToHiveRR.get(srcRel), starRR = inputRR;
if (allPartitions != null) {
public class HiveCost implements RelOptCost {
selectivity = computeFunctionSelectivity(call) * (call.operands.size() - 1);
//       for TOK_JOIN and TOK_FULLOUTERJOIN.
if (!resp.isIsSupported()) {
public Builder location(String location) {
for(ReadEntity re : partitionsRead) {
while (command.charAt(startPosition++) != '`' && startPosition < command.length()){
None
too long.
// First, we find the SELECT closest to the top.
@Override
HIVE_IN_TEST_REPL("hive.in.repl.test", false, "internal usage only, true in replication test mode", true),
String currDb = SessionState.get().getCurrentDatabase();
Set<String> result = new LinkedHashSet<>();
RowIndex[] getRowIndexes();
return new Partition();
theMRInput = (MRInputLegacy) inp.getValue();
return valuesReader.readBytes().getBytesUnsafe();
return Maps.fromProperties(properties);
too long.
@Test
case "-":
too long.
private static boolean hadBadBloomFilters(TypeDescription.Category category,
int numBuckets = (conf.getTable() != null) ? conf.getTable().getNumBuckets()
checkAndSetFileOwnerPermissions(fs, subFile, userName, groupName, dirPerms, filePerms, dryRun, recurse);
sb.append("\tFAILED container: ");
// args as child of func?
if (e instanceof MetaException) {
//       somewhere like ZK? Try to randomize it a bit for now...
too long.
setFsRelatedProperties(conf, fs.getScheme().equals("file"),fs);
private void initHS2(boolean enableXSRFFilter) throws Exception {
return new String((byte[])obj);
too long.
public String getDatabaseName() {
List<String> partitionNames = client.listPartitionNames(DB_NAME, TABLE_NAME,
public List<Order> getSortCols() {
throw new SerDeException("TypeInfo [" + typeInfo.getTypeName()
private Map<Integer, String> parentToInput = new HashMap<Integer, String>();
public class LlapProtocolClientImpl implements LlapProtocolBlockingPB {
}
return StructStreamReader.builder()
MapWork mergeMapWork = (MapWork) mergeWork;
// can prevent updates from being sent out to the new node.
caughtException = (TException)t;
}
if (ss != null && HiveConf.getVar(job, ConfVars.HIVE_EXECUTION_ENGINE).equals("tez")) {
too long.
too long.
curr = genPostGroupByBodyPlan(groupByOperatorInfo, dest, qb, aliasToOpInfo, null);
if (type.equals(Type.JOB)) {
conf.setBoolean("tez.runtime.optimize.local.fetch", true);
assert root.equals(orcSplit.getRootDir()) : "root mismatch: baseDir=" + orcSplit.getRootDir() +
}
// if (bb.isDirect()) {
this.createViewDesc.setViewOriginalText(originalText);
}
MemoryBuffer[] tailBufferArray = tailBuffers.getMultipleBuffers();
too long.
// SettableTreeReader so that we can avoid this check.
if (newCacheData == null) {
short twoScaleDown = (short) -exponent;
too long.
deleteDeltaIfExists(newPartitionPath, table.getWriteId(), newBucketId);
throw new RuntimeException("SubmissionState in response is expected!");
private static final LlapCoordinator INSTANCE = new LlapCoordinator();
TStatus tStatus = new TStatus(TStatusCode.ERROR_STATUS);
// TODO HIVE-16134. Differentiate between EXTERNAL_PREEMPTION_WAITQUEU vs EXTERNAL_PREEMPTION_FINISHABLE?
// This also gets us around the Enum issue since we just take the value
private static HCatSchema getOutputSchema(Configuration conf)
List<String> materializedViews = getTables(dbName, ".*", TableType.MATERIALIZED_VIEW);
too long.
output.add(new IntWritable(Integer.valueOf((String) value)));
private void checkForZKDTSMBug() {
super(LlapProtocolClientProxy.class.getSimpleName(), numThreads, conf, llapToken,
client.dropTable("no_such_database", table.getTableName());
}
streams.out.close();
if (useDelimitedJSON) {
public void createCatalogWithBadLocation() throws TException {
FileUtils.readFully(stream, length, bb);
TypeInfo tgtDT = null;
// TODO: we should be able to enable caches separately
private org.apache.hadoop.hive.ql.plan.TableDesc table;
too long.
assert isSplitUpdate : "should be true in Hive 3.0";
too long.
public VectorMapJoinOptimizedMultiKeyHashMap(boolean isOuterJoin,
copyName = generateOldPlanName(newName, ++i);
private void dumpFunctionMetadata(String dbName, Path dumpRoot) throws Exception {
return compile_resp;
assert !isRepeated;
String isExternal = table.getParameters().get("EXTERNAL");
return 0;
private Hive sessionHive;
pw.println(
expr = (ASTNode) child.getChild(0);
Utilities.clearWorkMap(conf);
too long.
// means serializing another instance.
private static class FetchInputFormatSplit extends HiveInputFormat.HiveInputSplit {
public Builder collectionItemsTerminatedBy(char delimiter) {
static String convertScanToString(Scan scan) throws IOException {
for (String partName : partNames) {
LOG.warn("Error during analyzeReplDump", e);
ShowCompactResponse currentCompactions = txnHandler.showCompact(new ShowCompactRequest());
too long.
too long.
e.printStackTrace();
newCall =
String tabAlias = addEmptyTabAlias ? "" : null;
boolean lastSeenRightOuterJoin = false;
private static final String HDFS_ID_PATH_PREFIX = "/.reserved/.inodes/";
if (lbDirSuffix.startsWith(Path.SEPARATOR)) {
too long.
// authorization checks passed.
hookContext = new PrivateHookContext(plan, queryState, ctx.getPathToCS(), SessionState.get().getUserName(),
public class HiveRelMdPredicates implements MetadataHandler<BuiltInMetadata.Predicates> {
if (LOG.isTraceEnabled()) {
// in type system for this.
too long.
too long.
}
return (files.size() > otherFiles.size())
String topicName = getTopicPrefix(tableEvent.getIHMSHandler().getConf()) + "." + table.getDbName().toLowerCase();
private SessionState parentSessionState;
boolean useNontaged = conf.getBoolVar(
srv.set(kv.getKey(), kv.getValue());
return null;
final List<OrcProto.Type> schemaTypes = OrcUtils.getOrcTypes(schema);
too long.
return new DoubleStreamReader(columnIndex, present, data, isFileCompressed, vectors);
this.version = version;
case '%':
// once the feature is stable
TGetResultSetMetadataResp  metadataResp;
// Don't acquire locks for any of these, we have already asked for them in DDLSemanticAnalyzer.
/*
LOG.warn("Session configuration is null for " + wmTezSession);
too long.
this.orcCvp = new OrcColumnVectorProducer(
too long.
for (int i = 0; i < includes.getReaderLogicalColumnIds().size(); ++i) {
HashPartition hashPartition = new HashPartition(1024, (float) 0.75, 524288, 1, true, null);
// those tables directory.
aggArgRelDTBldr.add(TypeConverter.convert(expr.getTypeInfo(), dtFactory));
return false;
LOG.debug("Unable to stat file as current user, trying as table owner");
if ("string".equalsIgnoreCase(type)) {
client.add_partitions_pspec(null);
if(tempDecimalBuffer == null || tempDecimalBuffer.length < length) {
}
zkConf.set(ZK_DTSM_ZK_AUTH_TYPE, "sasl");
if (! (data instanceof List)) {
throw new IllegalStateException("Unable to initialize Warehouse from clientside.");
}
footerCache = useExternalCache ? metaCache : localCache;
isBigTable =
None
final Set<Operator<?>> workOps1 = findWorkOperators(optimizerCache, op1);
break;
c.isRepeating = false;
boolean retval = true;
synchronized (CACHE_TEARDOWN_LOCK) {
Path keyfile= new Path(getPath(type) + "/" + id + "/" + key);
too long.
} catch (Exception e) {
int scale = HiveDecimalUtils.getScaleForType(ptinfo);
return FETCH_NEXT;
this.metrics = LlapTaskSchedulerMetrics.create(displayName, sessionId);
public Builder isTableExternal(boolean isExternal) {
}
return cpr;
private void moveWork(SparkWork sparkWork, BaseWork work, SparkWork targetWork) {
JSONObject configs = createConfigJson(containerSize, cache, xmx, java_home);
boolean isManagedTable = part.getTable().getTableType() == TableType.MANAGED_TABLE;
boolean alreadyCommitted = rs2.next() && rs2.getInt(1) > 0;
too long.
too long.
too long.
assert info.lastSetGuaranteed == null;
"there should be " + String.valueOf(expectedNumOCleanedFiles) + " deleted files in cm root",
public String getLocation() {
return false;
tableCols = new ArrayList<>();
}
private Deserializer inputKeyDeserializer;
private static class ReflectiveProgressHelper {
TezSessionPoolManager.closeIfNotDefault(ss.getTezSession(), true);
collSep = LazyUtils.getByte(tbl.getProperty(COLLECTION_DELIM),
private boolean hadCommFailure = false;
public class ProtobufProxy<BlockingInterface> {
LLAP_DAEMON_CONTAINER_ID("hive.llap.daemon.container.id", null,
too long.
@Override
// with the RS parent based on its position in the list of parents.
checkAcidConstraints(qb, table_desc, dest_tab);
keys[current].setFirst(JoinUtil.computeKeys(nextRow.o, keyFields, keyFieldOIs));
too long.
LOG.info("Could not make " + Warehouse.getQualifiedName(newTable) + " acid: it's " +
JoinAlgorithm oldAlgo = join.getJoinAlgorithm();
if (converter.getWindowFunctionSpec() != null) {
sb.append('=');
//which is not tracked directly but available on /jobs/<id> node via "mtime" in Stat
if (op1 instanceof ReduceSinkOperator) {
public class JoinCondTypeCheckProcFactory extends TypeCheckProcFactory {
private final static class CasLog {
final String mrValue = conf.get(dep.getKey());
gbKeys.addAll(ExprNodeDescUtils.genExprNodeDesc(rs, groupingSetsColPosition,
if (sinks.contains(table)) {
public static boolean isInsertOnlyTable(Map<String, String> params, boolean isCtas) {
//todo: rename files case
if (session != null) {
TreeMap<Object, Object> tm1 = new TreeMap<Object, Object>(m1);
private enum TableType {
client.add_partitions_pspec(null);
too long.
throw new IllegalStateException("Uninitialized Warehouse from MetastoreHandler");
public class LBExprProcCtx implements NodeProcessorCtx{
public Builder nullDefinedAs(char nullChar) {
private final static CasLog casLog = null; //new CasLog();
public class HBaseTableSnapshotInputFormatUtil {
replicas,
if (lastReduceKeyColName != null) {
//       Remains here as the legacy of the original higher-level interface (getInstance).
* @throws IOException
containsWindowing = true;
if (joinCond.getType() == HiveParser.TOK_TABCOLNAME
@VisibleForTesting
super.visit(node, ordinal, parent);
too long.
return SHIMS.cloneUgi(baseUgi);
return vertex;
String name =
conf = (configuration instanceof HiveConf) ? (HiveConf)configuration :
@Override
ecode = qt.executeClient(versionFile, fname);
if (candidateCached != null) {
if (commonTypeInfo instanceof DecimalTypeInfo) {
return null;
int numReducers = -1;
case DECIMAL:
enum OptionConstants {
None
private static final class DataLossLogger {
for (Task<? extends Serializable> task : plan.getRootTasks()) {
invokeFlag = false;
if (!(fs instanceof DistributedFileSystem)) return;
Ref<Integer> endpointVersion = new Ref<>(-1);
public List<ColumnStatistics> getPartitionColumnStatistics(String catName, String dbName, String tblName,
if (startPosition == null || arrayLength + 1 == startPosition.length) {
case BINARY:
// The hiveJarDir can be determined once per client.
if (hasIndexOnlyCols && (rgs == null)) {
if (getContext() == null) {
StringBuilder buf = new StringBuilder();
@Override
return;
int numThreads = HiveConf.getIntVar(conf, HiveConf.ConfVars.LLAP_IO_THREADPOOL_SIZE);
too long.
public String getStatsAggPrefix() {
processorContext.waitForAllInputsReady(li);
too long.
.run("create table t1 (i int, j int) partitioned by (load_date date) "
File qf = new File(outDir, fileName);
@VisibleForTesting
FileSystem fs = deltaDirectory.getFileSystem(conf);
inputExpression =
too long.
String table = rsMeta.getTableName(col + 1);
RPC.stopProxy(umbilical);
});
Class<? extends MetaException> exClass = JavaUtils.getClass(
return deltaFiles;
long partitionId = extractSqlLong(fields[0]);
public Builder bucketCols(List<String> bucketCols, int buckets) {
// if (oi instanceof TypeInfoBasedObjectInspector) {
result = new HiveVarchar();
throw new SemanticException("Activate a resource plan to enable workload management");
//         (* and $expr)
Map<String, ModuleConfig> moduleConfigs = extractModuleConfigs();
too long.
work.getPartitionDescs().remove(desc);
if (assertsEnabled) {
too long.
too long.
synchronized void setAssignmentInfo(NodeInfo nodeInfo, ContainerId containerId, long startTime) {
Path scratchDir = utils.createTezDir(ctx.getMRScratchDir(), conf);
public void testOperatorNames() throws Exception {
LOG.debug("Replacing SETCOLREF with ALLCOLREF because of the nested node "
FileSystem fs = dbRoot.getFileSystem(conf);
CodedInputStream cis = CodedInputStream.newInstance(
too long.
public Map<String, String> getPartitionSpec() {
public class HiveFilterProjectTSTransposeRule extends RelOptRule {
logRefreshError("Unable to localize jars: ", jars, t);
public String getStorageHandler() {
org.apache.hadoop.hive.metastore.api.Table table = getTempTable(dbname, name);
//authorize against the table operation so that location permissions can be checked if any
String mappedPool = mapping.mapSessionToPoolName(input, allowAnyPool, null);
if (fetchOrientation.equals(FetchOrientation.FETCH_FIRST)) {
too long.
OrcTail orcTail = ReaderImpl.extractFileTail(fileMetadata);
too long.
qt.shutdown();
// For now, this can simply be fetched from a single registry instance.
this.timeoutTimer = timeoutPool.schedule(
@Override
}
public List<String> getBucketCols() {
Field[] fields = c.getDeclaredFields();
if (!work.isExplicitAnalyze() && !followedColStats1) {
Partition part =
loadTableWork.setInheritTableSpecs(false);
clusterType = MiniClusterType.valueForString(modeStr);
private static final Logger LOG = LoggerFactory.getLogger(ContainerRunnerImpl.class);
// Table operations.
@Test(expected = NullPointerException.class)
// Find the bucket id, and switch buckets if need to
too long.
String confQueueName = conf.get(TezConfiguration.TEZ_QUEUE_NAME);
private TezSessionPoolSession createAndInitSession(
None
boolean selectStar = false;
RexCall equals = (RexCall) conj;
if (!leftPosListOfLastRightOuterJoin.contains(condn.getRight())) {
TezSessionPoolManager.getInstance().closeNonDefaultSessions();
PartitionDesc partDesc = pathToPartitionInfo.get(path);
List<String> materializedViews = getTables(dbName, ".*", TableType.MATERIALIZED_VIEW);
// use multiple lines for statements not terminated by the delimiter
too long.
txnHandler = getTxnHandler();
private static ExprNodeDesc propConstDistUDAFParams() {
if (left[leftOffset + length - 1] != rightBuffer[rightFrom + length - 1]) {
public EmbeddedCLIServiceClient(ICLIService cliService, Configuration conf) {
boolean mergeJoins = !pctx.getConf().getBoolVar(HIVECONVERTJOIN) &&
too long.
private static final String APPLY_PATCH_SCRIPT_PATH = "applyPatchScriptPath";
None
private List<ShowLocksResponseElement> getLocksWithFilterOptions(HiveTxnManager txnMgr,
public Builder fieldsTerminatedBy(char delimiter) {
populateQuickStats(fileStatus, params);
too long.
abstract boolean useHive130DeltaDirName();
jobProperties.put("mapred.output.dir", path);
LOG.trace("Non-parametrized map type: {}", field);
this.metrics = ms == null ? null : WmPoolMetrics.create(fullName, ms);
switch (schema.getCategory()) {
if (parent instanceof Filter || parent instanceof Join || parent instanceof SetOp ||
too long.
static DiskRangeList planIndexReading(TypeDescription fileSchema,
if (fields == null) {
try {
byte[] floatBytes = Float.toString((float) inV.vector[i]).getBytes();
throw new SemanticException(
processSetColsNode((ASTNode)child, searcher);
reduceWork.getEdgePropRef().setAutoReduce(null, false, 0, 0, 0);
ChannelFuture lastMap = null;
nonRecConf.setBoolean("mapreduce.input.fileinputformat.input.dir.nonrecursive.ignore.subdirs", true);
int urlIx = lastKnownGoodUrl, lastUrlIx = ((urlIx == 0) ? rmNodes.length : urlIx) - 1;
throw new HiveException(ErrorMsg.GENERIC_ERROR.getErrorCodedMsg());
/*
if (LlapIoImpl.LOG.isTraceEnabled()) {
pairs.add(new Pair<Text,Text>(new Text(mapMapping.getColumnFamily()), null));
// how to get around that.
Pattern internalPattern = Pattern.compile("_col([0-9]+)");
public List<HCatFieldSchema> getPartitionCols() {
switch (hiveTypeCategory) {
// ^(TOK_LATERAL_VIEW ^(TOK_SELECT ^(TOK_SELEXPR ^(TOK_FUNCTION Identifier["inline"] valuesClause) identifier* tableAlias)))
too long.
return ETypeConverter.ETIMESTAMP_CONVERTER.getConverter(type, index, parent, hiveTypeInfo);
if (data.stripes == null || data.stripes.isEmpty()) {
if (qb.getParseInfo().getAlias() != null) {
byte[] bytes = hiveVarchar.getValue().getBytes();
too long.
return value;
@Override
if (Double.isNaN((Double) value)) {
if (defaultPoolSize > 0) {
perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.TEZ_INIT_OPERATORS);
ShowCompactResponse allCompactions = db.showCompactions();
PrimitiveTypeEntry typeEntry = getTypeFor(method.getReturnType());
pm.deletePersistentAll(mfunc);
private static class IndexStream extends InputStream {
}
FunctionRegistry.unregisterTemporaryUDF("test_udaf");
List<String> partNames = null;
public enum GenerateCategory {
public List<HCatFieldSchema> getCols() {
if (pathToPartInfo == null) {
private static final Map<Map<Path, PartitionDesc>, Map<Path, PartitionDesc>> cache =
old_dep.setExpr(null);
public static Builder create(String dbName, String tableName, List<HCatFieldSchema> columns) {
}
/*perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.OPTIMIZER);
Collections.sort(batchToAbort);//easier to read logs
// Package permission so that HadoopThriftAuthBridge can construct it but others cannot.
private final Path indexPath;
DDLWork work = new DDLWork(new HashSet<>(), new HashSet<>(), createDbDesc);
//       This computes stats and should be in stats (de-duplicated too).
too long.
// Use RW, not PRIVATE because the copy-on-write is irrelevant for a deleted file
Map<?, ?> map = inputOI.getMap(input);
TableDesc keyTableDesc = mapJoinDesc.getKeyTblDesc();
OrcInputFormat.setSearchArgument(readerOptions, schemaTypes, conf, true);
int[] filterColumns = RecordReaderImpl.mapSargColumnsToOrcInternalColIdx(
too long.
PerfLogger perfLogger = SessionState.getPerfLogger();
SerDeEncodedDataReader reader = new SerDeEncodedDataReader(cache, bufferManager, conf,
public String getTableName() {
if (LOG.isDebugEnabled()) {
too long.
if (!validSetopParent(rel, parent))
return JAVA64;
public class HiveJoin extends Join implements HiveRelNode {
locks = lockMgr.getLocks(false, isExt);
InputStream errorStream = connection.getErrorStream();
sessionState.close(false);
final Reader.Options readOptions = OrcInputFormat.createOptionsForReader(conf);
sessionState.setIsHiveServerQuery(true);
UserPayload servicePluginPayload = TezUtils.createUserPayloadFromConf(tezConfig);
// to writing an instrumentation agent for object size estimation
switch (primitiveCategory) {
// We will estimate collection as an object (only if it's a field).
too long.
if (o instanceof java.sql.Date) {
checkAbortCondition();
return plan.getStatus() == Status.ACTIVE ? fullFromMResourcePlan(plan) : null;
ExecutorService executor = Executors.newFixedThreadPool(THREAD_COUNT);
Integer genColListRegex(String colRegex, String tabAlias, ASTNode sel,
public void deriveExplainAttributes() {
too long.
TableDesc ret = getDefaultTableDesc(Integer.toString(Utilities.ctrlaCode), cols,
MapJoinDesc mapJoinDesc = MapJoinTestConfig.createMapJoinDesc(testDesc);
if (parts.isEmpty()) {
String defaultTestSrcTables = "src,src1,srcbucket,srcbucket2,src_json,src_thrift," +
if (oldtbl
List<FieldSchema> partCols = tTable.getPartitionKeys();
return;
if (!isInitOk) {
for (String cmd : cmds) {
if (metadataCache != null) {
@SuppressWarnings("unused")
too long.
throw new IllegalArgumentException("Negations not yet implemented");
disabledNodesQueue.add(nodeInfo);
List<Partition> partitions = client.getPartitionsByNames(DB_NAME, TABLE_NAME,
public static byte[] decodeIfNeeded(byte[] recv) {
Pattern regex = null;
too long.
too long.
char nextChar = result.charAt(i + 1);
None
}
pathCreated = wh.renameDir(sourcePath, destPath, false);
return LlapFixedRegistryImpl.this.port;
too long.
sb.append(serdeConstants.MAP_TYPE_NAME).append("<").append(serdeConstants.STRING_TYPE_NAME)
too long.
@Test
too long.
String path = HiveHFileOutputFormat.getFamilyPath(jobConf, tableProperties);
warehousePath = new Path(fsUriString, "/build/ql/test/data/warehouse/");
if (types != null) {
Assert.fail("Expected an NullPointerException or TTransportException to be thrown");
taskWrapper.getTaskRunnerCallable().killTask();
too long.
private BufferedRows getConfInternal(boolean call) {
too long.
if (LOG.isInfoEnabled()) {
env.putAll(localEnv);
public final boolean logicalEqualsTree(Operator<?> o) {
DataOutputBuffer port_dob = new DataOutputBuffer();
// renaming test to make test framework skip it
too long.
too long.
}
return;
// projections from child?
Token<LlapTokenIdentifier> token = new Token<LlapTokenIdentifier>(llapId, this);
for (RelOptRule r : rules)
too long.
// we can generate ranges from e.g. rowid > (4 + 5)
constantPropDistinctUDAFParam = SemanticAnalyzer
String clientId = getClientId(jobId);
if (ifName.equals(format)) {
too long.
inputVrb.cols[ixInVrb] = cvb.cols[ixInReadSet];
schema.setProperty(serdeConstants.QUOTE_CHAR, "(\"|\\[|\\])");
List<String> tabBucketCols = tab.getBucketCols();
too long.
return false;
cachedObjectInspector = ObjectInspectorFactory
for (FileStatus fileStatus : contents) {
// data with the separator bytes before creating a "Put" object
if (obAST != null && !(selForWindow != null && selExprList.getToken().getType() == HiveParser.TOK_SELECTDI) && !isAllColRefRewrite) {
return table;
final JobConf cloneJobConf = new JobConf(jobConf);
// fall through
if (joinRel instanceof SemiJoin) {
for (int j = 0; j < 10; ++j) {
too long.
validateRestrictedConfigValues(var.varname, userValue, serverValue);
ctx.hiveConf = new HiveConf(IDriver.class);
too long.
// This is only required to support the deprecated methods in HCatAddPartitionDesc.Builder.
for (Partition partition : metadata.getPartitions()) {
too long.
too long.
throw new HiveException("Converting list bucketed tables stored as subdirectories "
if (oldtbl
boolean hasFileId = this.fileKey != null;
// From the value arrays and our isRepeated, selected, isNull arrays, generate the batch!
// NullPointerException, remote throws TTransportException
LOG.info("DP can be rewritten to SP!");
if (SessionState.get() != null) {
try {
private final FixedSizedObjectPool<IoTrace> tracePool;
public class TableExport {
@Override
public String getApplyPathScriptPath() {
too long.
String[] jarPaths = auxJars.split(delimiter);
coordinator = LlapCoordinator.getInstance();
private static final String BASE_PREFIX = "base_", DELTA_PREFIX = "delta_",
// in all hadoop versions.
dummyOp.initialize(jconf, null);
UserGroupInformation ugi = Utils.getUGI();
return OrcFile.createWriter(path, createOrcWriterOptions(oi, conf, cacheWriter, allocSize));
long evicted = evictor.evictSomeBlocks(remainingToReserve);
private List<AggrColStats> nodes = new ArrayList<>();
//this uses VectorizedOrcAcidRowBatchReader
too long.
localDirs = conf.getTrimmedStrings(SHUFFLE_HANDLER_LOCAL_DIRS);
List<String> versionFiles = QTestUtil.getVersionFiles(queryDirectory, tname);
too long.
// Finally add the partitioning columns
int badCallCount = 0;
return sortColNames.subList(0, joinCols.size()).containsAll(joinCols);
verify(sessionState).ensureLocalResources(any(Configuration.class), eq(inputOutputJars));
return get(fieldName, recordSchema);
// moved...this may change
too long.
too long.
Assert.assertTrue("Unexpected: HCAT_PARTITION_DONE_EVENT not supported (yet).", false);
for (IOSpecProto inputSpec : vertex.getInputSpecsList()) {
caughtException = tae;
Integer numReducersFromWork = rWork == null ? 0 : rWork.getNumReduceTasks();
if (null == rangeSplit.getIterators()
too long.
pw.println("-- These commands may be executed by Hive 1.x later");
too long.
private static ExprNodeDesc propConstDistUDAFParams() {
OrcProto.ColumnEncoding columnEncoding = encodings.get(columnIndex);
boolean nonDefaultUser = conf.getBoolVar(HiveConf.ConfVars.HIVE_SERVER2_ENABLE_DOAS);
/*
if (!isBlacklistWhitelistEmpty(conf) || !isCachePrewarmed.get()) {
return super.explainTerms(pw)
// prefix = work.getAggKey();
if (context.linkChildOpWithDummyOp.containsKey(mj)) {
public synchronized String getDelegationTokenFromMetaStore(String owner)
if (cacheStripeDetails) {
if (fsOp.getConf().getDynPartCtx() != null) {
ret.numRows = newRowCount;
runStatementOnDriver("create table myctas2 stored as ORC TBLPROPERTIES ('transactional" +
public class OpWalkerCtx implements NodeProcessorCtx {
ByteBuffer copy = ByteBuffer.allocate(bb.capacity());
private TCLIService.Iface client;
private Builder(String dbName, String tableName, List<HCatFieldSchema> columns) {
ExprNodeConverter exprConv = new ExprNodeConverter(inputTabAlias, inputRel.getRowType(),
validateTableCols(table, colNames);
t.addDependentTask(alterTable);
// 3. Return result
setOperationException(e);
return new Text(objInspector.getWritableConstantValue().toString());
too long.
final RelMetadataQuery mq = call.getMetadataQuery();
System.exit(rc);
ctx.close();
ColumnStreamData[] datas = ecb.getColumnData(colIx);
break;
// Tracks tasks which could not be allocated immediately.
RelCollation canonizedCollation = traitSet.canonize(newCollation);
public void testAddPartitionEmptyValue() throws Exception {
resultObjects[rowIndex++] = scrqtchRow[0];
ASTNode queryForCbo = ast;
SignatureUtils.write(sigMap, op.getConf());
if (valToPaths.size() > maxPartitions) {
if (failed > 0) {
@VisibleForTesting
resultObjects[rowIndex++] = scrqtchRow[0];
public class CachedStore implements RawStore, Configurable {
return getRecordUpdater(jc, acidOutputFormat,
too long.
UnsignedInt128 scratch = new UnsignedInt128();
too long.
clean(compactId2CompactInfoMap.get(queueEntry.getKey()));
private long numRowsCompareHashAggr;
import org.apache.hadoop.io.retry.RetryPolicies;
//       that fractions or query parallelism add up, etc.
}
List<Integer> cwColIds = writer.isOnlyWritingIncludedColumns() ? splitColumnIds : columnIds;
private JobMetricsListener jobMetricsListener;
String attemptId = Converters.createTaskAttemptId(context).toString();
public MappingInput(String userName, List<String> groups, String wmPool, String appName) {
}
too long.
if (fos.size() > 0 && oss.size() > 0) {
return;
client.dropDatabase(dbName1, true, true, true);
QueryIdentifier queryId = executorService.findQueryByFragment(fragmentId);
throw new AssertionError("Unsupported mode");
HCatUtil.getHiveMetastoreClient(hiveConf);
public String getLocation() {
if (sendCounters) {
Assert.assertEquals(initialCount + 1,
// only mechanical data retrieval should remain here.
stats.addToDataSize(getDataSizeFromColumnStats(nr, columnStats));
if (children.contains(null)) {
for (int i = 1; i < numDistinctExprs; i++) {
tableValue += (" and " + TBLS + ".\"TBL_NAME\" = ? and " + DBS + ".\"NAME\" = ? and "
// https://issues.apache.org/jira/browse/HIVE-17627
//extra heartbeat is logically harmless, but ...
public Builder partCols(List<HCatFieldSchema> partCols) {
throw new IOException("Couldn't write to node " + id, nfe);
int formatScale = 0 + r.nextInt(38);
String eventType = event.getEventType();
too long.
Deque<Object> stack = createWorkStack(rootObj, byType);
// way!
assert result.length == files.size();
int wndSpecASTIndx = getWindowSpecIndx(windowProjAst);
if (cacheEncodings == null) {
private final static void setEnv(Map<String, String> newenv) throws Exception {
PerfLogger perfLogger = SessionState.getPerfLogger();
OpAttr visit(HiveFilter filterRel) throws SemanticException {
RelDataTypeFactory dtFactory = cluster.getRexBuilder().getTypeFactory();
None
for (OperationType opType : values()) {
/** Linked list pointers for LRFU/LRU cache policies. Given that each block is in cache
// different, or duplicate some other function).
return "";
LOG.trace("Cannot determine map type: {}", field);
@Override
// don't go  through Initiator for user initiated compactions)
// lock correctly. See the comment on the lock field - the locking needs to be reworked.
removeEnv.add("HADOOP_ROOT_LOGGER");
success = sql(line.substring("all ".length())) && success;
}
too long.
newCacheDataForCol[streamIx] = stream.data.toArray(new LlapSerDeDataBuffer[stream.data.size()]);
private static byte[] join(String[] items, char separator) {
if(ci.type == null) { ci.type = CompactionType.MINOR; }
if (isAllParts) {
AddPartitionDesc partsDesc = getBaseAddPartitionDescFromPartition(fromPath, dbname, tblDesc, partition);
Path scratchDir = utils.createTezDir(ctx.getMRScratchDir(), job);
private VectorDeserializeRow() {
} else if (isCompare && (func.getChildren().size() == 2)) {
InStream stream = ((StringDirectTreeReader) reader).getStream();
Connection conn = null;
too long.
// if the default was decided by the serde
public Builder sortCols(ArrayList<Order> sortCols) {
sendError(ctx, FORBIDDEN);
// NullPointerException, remote throws TTransportException
too long.
too long.
too long.
public String getDatabaseName() {
if (endReason != null && EnumSet
threadPool.submit(new DateTestCallable(bad, timeZone)).get();
if (buffer.declaredCachedLength != LlapDataBuffer.UNKNOWN_CACHED_LENGTH) {
void init(AtomicBoolean stop, AtomicBoolean looped) throws MetaException;
String checksumString = null;
class Direct implements OpTreeSignatureFactory {
if (opType == OpType.DELETE) {
too long.
private static final Set<String> llapDaemonVarsSet;
this.evolution = sef.createSchemaEvolution(fileMetadata.getSchema());
public String getComments() {
// child process. so we add it here explicitly
too long.
HashPartition hashPartition = new HashPartition(1024, (float) 0.75, 524288, 1, true, null);
File qf = new File(outDir, fileName);
too long.
None
VectorExpression[] child = new VectorExpression[1];
//       This only lives for the duration of the service init.
boolean dbHasJoinCastBug = DatabaseProduct.hasJoinOperationOrderBug(dbType);
public void testConstraints() throws IOException {
OpAttr visit(HiveTableScan scanRel) {
runStatementOnDriver("alter table " + Table.NONACIDORCTBL + " SET TBLPROPERTIES ('transactional'='true', 'transactional_properties'='default')");
public PPart(Table table, Partition partiton) {
if (!status.isDir()) {
too long.
// since we cannot directly set the private byte[] field inside Text.
too long.
@Test
// Assuming the used memory is equally divided among all executors.
for (org.apache.hadoop.hive.metastore.api.Partition outPart
newValue = State.switchFlag(newValue, State.FLAG_NEW_ALLOC);
int rc = setFSPermsNGrp(ss, driver.getConf());
}
boolean[] included = new boolean[schema.size()];
outputColVector.init();
too long.
public class JsonSerDe extends AbstractSerDe {
tmpNoNulls = getMaxNulls(stats, ((ExprNodeFieldDesc) pred).getDesc());
// codes and messages. This should be fixed.
JSON_MAPPER.setTimeZone(TimeZone.getTimeZone("UTC"));
if (!doesTimeMatter) return daysToMillis(d + 1) - (MILLIS_PER_DAY >> 1);
throw new RuntimeException(e.getCause());
synchronized (watchesPerAttempt) {
Preconditions.checkState(maxJvmMemory >= memRequired,
this.writeBuffers = writeBuffers;
public Builder escapeChar(char escapeChar) {
// 'n' columns where 'n' is the length of the bucketed columns.
private final static String tid =
if (ndvToBeSmoothed > 3)
private static boolean add(RowResolver rrToAddTo, RowResolver rrToAddFrom,
// For eg: select count(1) from T where t.ds = ....
too long.
if (task instanceof TaskAttempt) {
LOG.debug("Found " + deltas.size() + " delta files, threshold is " + deltaNumThreshold +
cumulativeBackoffFactor = cumulativeBackoffFactor * blacklistConf.backoffFactor;
too long.
too long.
Assert.fail("Expected a NullPointerException or TTransportException to be thrown");
LlapNodeId nodeId = LlapNodeId.getInstance(hostname, port);
too long.
boolean isBrokenUntilMapreduce7086 = "TEXTFILE".equals(format);
conf.setBoolean(ColumnProjectionUtils.READ_ALL_COLUMNS, false);
TezAttemptArray aw = new TezAttemptArray();
return null;
String loggedInUserName = SessionState.get().getUserName();
if (!distParamInRefsToOutputPos.containsKey(argLst.get(i))
too long.
SessionState.setCurrentSessionState(parentSessionState);
if (isLocationSet) {
private PrimitiveType getElementType(Type type) {
public Builder mapKeysTerminatedBy(char delimiter) {
private final QueryFragmentCounters counters;
return true;
// We need to consolidate 2 or more buffers into one to decompress.
// The wait queue should be able to fit at least (waitQueue + currentFreeExecutor slots)
primary.run("drop database if exists " + primaryDbName + " cascade");
public String getFileFormat() {
check.replaceSelfWith(new IncompleteCb(check.getOffset(), check.getEnd()));
switch (((PrimitiveTypeInfo)columnTypes.get(i)).getTypeName()) {
if (!unusedTriggers.isEmpty()) {
MockFileSystem.clearGlobalFiles();
return false;
// same as in getRecordReader?
too long.
private final Object lock = new Object();
private interface Field {
too long.
SessionState ss = SessionState.get();
too long.
None
None
// (will need to handle an alternate work-dir as well in this case - derive from branch?)
too long.
if (requestedHosts != null && requestedHosts.length != 0) {
too long.
runStatementOnDriver("delete from " + Table.ACIDTBL + " where a in(select a from " + Table.NONACIDORCTBL + ")");
String hiveQueryId;
public static void floor(int i, HiveDecimal input, DecimalColumnVector outputColVector) {
too long.
private boolean isExprResolver;
too long.
boolean isFirst = true;
public static Path generateTmpPathForPartitionPruning(Path basePath, String id) {
private static final Cache<String, LlapTokenLocalClient> localClientCache = CacheBuilder
// LOG4J2-1292 utilize gc-free Layout.encode() method: taken care of in superclass
for (int i = 0; i < locInfo.length; i++) {
too long.
super(LlapPluginEndpointClientImpl.class.getSimpleName(),
req.setCapabilities(new ClientCapabilities(
public boolean getExternal() {
// See: SPARK-21187
shouldRun.compareAndSet(true, false);
return new OpAttr("", new HashSet<Integer>(), rsGBOp2);
too long.
/** Planner rule that adjusts projects when counts are added. */
// What we need is a way to get buckets not splits
if (queryParallelism == null && likeName == null) {
None
if (work.isClearAggregatorStats()) {
List<Integer> filePhysicalColumnIds = readerLogicalColumnIds;
too long.
String line;
throw new SemanticException(ErrorMsg.CTAS_PARCOL_COEXISTENCE.getMsg());
this.parameters.put(ReplChangeManager.SOURCE_OF_REPLICATION, parameters.get(key));
// Requires schema change
for (int i = 0; i < logSize; ++i) {
if (sr.dataSize > sr.maxDataSize) {
too long.
* @deprecated Use MetastoreConf.DATANUCLEUS_INIT_COL_INFO
if (fastScale == 0) {
int threadCount = HiveConf.getIntVar(pctx.getConf(),
private void registerAllFunctionsOnce() throws HiveException {
return (T) super.getObject(columnIndex);
e.printStackTrace();
private DynamicServiceInstanceSet instances;
List<Path> finalDirs = null, dirsWithMmOriginals = null;
too long.
if (maxCacheSizeInBytes > 0) {
if (LOG.isInfoEnabled()) {
too long.
LOG.info("Converting {} to full transactional table", getQualifiedName(tableObj));
List<Partition> partitionEntries = metaStoreClient.listPartitions(table.getDbName(), table.getTableName(),
syncWork.toRestartInUse.add(session);
too long.
public static InputSplit createTableSnapshotRegionSplit() {
errorMessage = "FAILED: Hive Internal Error: " + Utilities.getNameMessage(e);
return value;
deepCopyPartitions(r.getPartitions(), result);
preemptedTaskList = preemptTasksFromMap(speculativeTasks, forPriority, forVertex,
// walking through all active nodes, if they don't have potential capacity.
while (!threadPool.awaitTermination(10, TimeUnit.SECONDS)) {
if (expr.getType() == HiveParser.TOK_DATELITERAL) {
LOG.debug("Done retrieving all objects for getPartitionsViaOrmFilter");
}
DataInputByteBuffer in = new DataInputByteBuffer();
return -1;
private static final int[] BUCKET_COLUMN_INDEXES = new int[] { 0 };
public Builder comments(String comment) {
@Test
if (existing == null) {
public static final byte NULL = 1;
FileStatus status = getFileStatus(f);
boolean includeGrpSetInGBDesc = (gbInfo.grpSets.size() > 0)
executor = new StatsRecordingThreadPool(1, 1,
None
too long.
if (splitStrategy instanceof ETLSplitStrategy) {
if (task instanceof ConditionalTask) {
public class TopNHash {
@Deprecated
int i = gbExprNDescLst.size();
LOG.warn("Skipping unknown directory " + dirName
public class VectorizedListColumnReader extends BaseVectorizedColumnReader {
//       DagClient as such should have no bearing on jobClose.
semiJoin = false;
return null;
too long.
StatsSetupConst.setBasicStatsState(parameters, StatsSetupConst.FALSE);
if (client == null) {
if (hasSpaceForCacheEntry(entry, size)) {
if (tbl.getSd().getLocation() == null) {
runtimeWatch.start();
if (!enablePreemption || preemptionQueue.isEmpty()) {
private String resultsDirectory;
if (!isTxnTable && ((loadFileType == LoadFileType.REPLACE_ALL) || (oldPart == null && !isAcidIUDoperation))) {
too long.
too long.
inpFormat = CombineHiveInputFormat.class.getName();
{
//       older-node tasks proactively. For now let the heartbeats fail them.
if (hiveConf != null) {
firePreEvent(new PreDropPartitionEvent(tbl, part, deleteData, this));
return Float.parseFloat(toFormalString());
too long.
too long.
ponderNextBufferToRead(readPos);
String sh = tbl.getStorageHandler().toString();
return StageType.REPL_DUMP;
path = System.getProperty(TEST_ENV_WORKAROUND + envVar);
too long.
sb.append("\nMetadata cache state: ").append(metadata.size()).append(
newSession.getConf().set(TezConfiguration.TEZ_QUEUE_NAME, queueName);
@Override
// firstFetchHappened == true. In reality it almost always calls joinOneGroup. Fix it?
if (_dataStream != null && _dataStream.available() > 0) {
SelectDesc sd = new SelectDesc(exprCols, exprNames);
// an exception
// in addition to that Druid allow numeric dimensions now so this check is not accurate
public String getTableName() {
if (getSubjectMethod == null) {
return new PathFilter() {
}
too long.
public final static String stringifyDiskRanges(DiskRangeList range) {
boolean[] readerIncludes = OrcInputFormat.genIncludedColumns(
if (i > 0) {
//       after all the perf changes that we might was well hardcode them separately.
public Builder linesTerminatedBy(char delimiter) {
validate = false;
// Currently using fileuri#checksum#cmrooturi#subdirs as the format
runStatementOnDriver("alter table nobuckets compact 'major'");
WindowFunctionSpec wFn = (WindowFunctionSpec) getWindowExpressions().get(0);
too long.
HashPartition hashPartition = new HashPartition(1024, (float) 0.75, 524288, 1, true, null);
user = System.getenv(ApplicationConstants.Environment.USER.name());
too long.
FileSplit sliceSplit = new FileSplit(split.getPath(), split.getStart(),
} else {
if (maxCacheSizeInBytes > 0) {
too long.
// proper index of a dummy.
// set method. This requires calcite change
too long.
return t;
if (sp >= 0 & (sp + 1) < udfClassName.length()) {
getContext().taskKilled(taskAttemptId,
FileSplit splitPart = new FileSplit(split.getPath(), uncachedSuffixStart,
appStatusBuilder.setAmInfo(
too long.
LOG.warn("Couldn't find aliases for " + splitPath);
MapOutputInfo outputInfo = new MapOutputInfo(pathInfo.dataPath, info);
public class TableExport {
dataSchema = hcatSplit.getDataSchema();
throw new IllegalArgumentException("Expected match count is 1; but got:" + all);
too long.
return Arrays.copyOf(sourceBw.getBytes(), sourceBw.getLength());
too long.
return false;
break;
if (requestedHostIdx == -1) {
throw (OutOfMemoryError) e;
LOG.error("Failed to start LLAP Daemon with exception", t);
//       move the setupPool code to ctor. For now, at least hasInitialSessions will be false.
private RowResolver outerRR;
recordReader = reader.rowsOptions(options, conf);
public Builder fileFormat(String format) {
String udafName = SemanticAnalyzer.getColumnInternalName(reduceKeys.size());
MapWork mapWork = Utilities.getMapWork(hiveConf);
//    assertEquals(null,stats.getSum());
LlapIoImpl.LOG.error("decodeBatch threw", ex);
private Map<String, Object> makeOneTablePartition(String partIdent)
if (abortTxns(dbConn, Collections.singletonList(txnid), true) != 1) {
configureAmRegistry(newSession);
}
if (input != colSrcRR) {
private final AtomicReference<InetSocketAddress> srvAddress = new AtomicReference<>(),
too long.
too long.
String intermediateRecordsCounterName = formattedName(
InputFormat format = inputFormats.get(inputFormatClass.getName());
maxLength = -1;
long totalLength;
AcidUtils.Directory dirInfo = AcidUtils.getAcidState(
// could generate different error messages
Path warehousePath;
public void setStmtId(int stmtId) {
case LIST:
checkAndSetFileOwnerPermissions(fs, tablePath,
//       into LlapNodeId. We get node info from registry; that should (or can) include it.
TypeInfo typeInfo = TypeInfoUtils.getTypeInfoFromTypeString(
Properties props = outputJobInfo.getTableInfo().getStorerInfo().getProperties();
public int getNumBuckets() {
System.setProperty("https.protocols", "TLSv1,TLSv1.1,TLSv1.2");
ByteBuffer restored = OrcInputFormatForTest.caches.cache.get(key).data;
hadoopAuth = conf.get(HADOOP_SECURITY_AUTHENTICATION, "simple");
// can be inherited from a base class.
public static Builder create(String dbName,
if (curErr instanceof org.apache.hadoop.security.AccessControlException
switch (queryState.getHiveOperation() == null ? HiveOperation.QUERY : queryState.getHiveOperation()) {
List<String> transactionalTables = tablesFromReadEntities(inputs)
}
}
}
protected transient Object[][] cachedKeys;
too long.
try {
None
for (RelDataTypeField field : rowType.getFieldList()) {
if (tableFieldTypeInfo.getCategory() != Category.PRIMITIVE) {
String query = "insert overwrite table " + tmpName + " ";
String message = null;
// just last one.
//BaseSemanticAnalyzer sem = SemanticAnalyzerFactory.get(new QueryState(queryState.getConf()), input);
return lowLevelCache.getFileData(fileKey, range, baseOffset, factory, null, gotAllData);
// partition without column info. This should be investigated later.
Map<ASTNode, ExprNodeDesc> map = TypeCheckProcFactory
LOG.error("Fatal error: scheduler thread has failed and will now exit", t);
// BitSet::wordsInUse is transient, so force dumping into a lower form
}
public class HiveMetaStore extends ThriftHiveMetastore {
Logger.info("Table " + tbl.getTableName() + " is ACID table. Skip StatsOptimizer.");
too long.
try (Statement stmt = conn.createStatement()) {
if (rel instanceof SemiJoin) {
Deadline.resetTimeout(MetastoreConf.convertTimeStr(changeEvent.getNewValue(), TimeUnit.SECONDS,
too long.
@Test
boolean isLlapOn = HiveConf.getBoolVar(conf, ConfVars.LLAP_IO_ENABLED, llapMode);
//now make sure delete deltas are present
too long.
if (tbl.isNonNative()) {
boolean orderFound;
return new Text(getHiveChar().getStrippedValue());
too long.
cmd.append(getOpts().getDelimiter());
too long.
Object result = nodeOutput.get(node);
None
return new PartInfo(schema, storageHandler, sd.getLocation(),
cboCtx.nodeOfInterest = (ASTNode) subq.getChild(0);
WmFragmentCounters wmCounters = new WmFragmentCounters();
parentTab = Hive.get().getTable(parentDatabaseName, parentTableName);
private final IndexCache indexCache;
(desiredLock.txnId == 0 &&  desiredLock.extLockId == existingLock.extLockId);
too long.
TGetInfoReq req = new TGetInfoReq(sessionHandle.toTSessionHandle(), infoType.toTGetInfoType());
static final int DEFAULT_RETRY_COUNT = 2; // test is executed 3 times in worst case 1 original + 2 retries
xmx = options.getXmx();
too long.
}
* @param addPrimaryKeyEvent add primary key event
if (te.getType() != TApplicationException.UNKNOWN_METHOD
public Builder storageHandler(String storageHandler) throws HCatException {
}
discardableInputOps.addAll(gatherDPPBranchOps(pctx, optimizerCache, discardableOps));
return HiveJoin.getJoin(left.getCluster(), left, right, condition, joinType);
if (current.resourcePlanToApply != null) {
checkAcidConstraints(qb, table_desc, dest_tab);
conf.setBoolVar(ConfVars.HIVE_SUPPORT_CONCURRENCY, false);
// Can be converted to a Tez event, if this is sufficient to decide on pre-emption
AvroGenericRecordWritable agrw2 = new AvroGenericRecordWritable();
DataBag pigBag = (DataBag) pigObj;
HttpClient httpClient = mock(HttpClient.class);
TableHandler tableHandler = new TableHandler();
}
// by position in the row schema of the filesink operator.
JoinUtil.JoinResult joinResult;
boolean allKeyInputColumnsRepeating;
throw new SQLFeatureNotSupportedException("Method not supported");
log("Failed to log lineage graph, query is not affected\n"
throw new RuntimeException(e);
List<Operator<? extends OperatorDesc>> originalChilren = op
opHandle = executeStatementInternal(cmd_trimed, null, false, 0);
// jobConf will hold all the configuration for hadoop, tez, and hive
private void generateColumnUnaryFunc(String[] tdesc) throws Exception {
lastInputPath = currentInputPath;
return predPresent ? whereClause.append(groupByClause) : groupByClause;
return SORT_COLS;
oldCall.isDistinct(),
fs = new MockFileSystem(conf,
public void setTypeName(String typeName) {
for (int i = 0; i < methodParameterTypes.length; i++) {
private Thread separateRowGenerator;
assertEquals(HiveIntervalDayTime.valueOf("1 10:11:0"),
assertEquals(1, splits.length);
int toWrite = Math.min(toRead, wbSize - writePos.offset);
too long.
/// if COUNT returns true since COUNT produces 0 on empty result set
int numRowsReceived;
if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.LLAP_IO_ENABLED, false)) {
// the metadata system.
if (scSize == 2) {
try {
boolean partKeysPartiallyEqual = checkPartialPartKeysEqual(oldt.getPartitionKeys(),
sparkConf.put("spark.hadoop." + propertyName, value);
Table table = null;
// sparse map.
throw new MetaException("Duplicate partitions in the list: " + part);
key = key ^ (key >>> 14);
removeBlockFromFreeList(freeList, bHeaderIx, freeListIx);
PrimitiveCategory primitiveCategory = ((PrimitiveObjectInspector) arguments[i])
}
prot.readFieldBegin();
//       Project-B (may reference coVar)
None
break;
//---------------------------------------------------
if (pactx.getJoinOps() != null) {
} finally {
sb.append(state == null ? "N" : state);
LOG.info("Response to queryId=" + queryId + " " + res);
if (ref != null && ref.getToken().getType() == HiveParser.Number) {
@Override
LoadTableDesc loadTableWork = new LoadTableDesc(moveTaskSrc, Utilities.getTableDesc(table),
private TableDesc scriptInputInfo;
throw new NumberFormatException("Invalid string:"
Assert.assertEquals(HADOOP_CREDSTORE_PASSWORD_ENVVAR_VAL, getValueFromJobConf(
for (int i = numOfServicesStarted; i >= 0; i--) {
TableFunctionEvaluator tEval = def.getTFunction();
updateJobStatePercentAndChildId(conf, context.getJobID().toString(), null, childJobIdString);
return array[index].getKey();
// we also need to delete partdate=2008-01-01 to make it consistent.
Assert.assertEquals(oldDec.toString(), dec.toString());
boolean convert = canConvertMapJoinToBucketMapJoin(
key.startsWith("druid."))) {
too long.
sb.append(randomizePattern(control, chunk));
List<WriteEntity> toRemove = new ArrayList<>();
TSocket tSSLSocket = TSSLTransportFactory.getClientSocket(host, port, loginTimeout);
List<ExprNodeDesc> keyDesc = desc.getKeys().get(posBigTable);
Deadline.startTimer("getAggrPartitionColumnStatistics");
if (moveTaskToLink.getDependentTasks() != null) {
if (taskWrapper == null) return null;
private void createNewGroupingKey(List<ExprNodeDesc> groupByKeys,
runStatementOnDriver("load data local inpath '" + getWarehouseDir() + "/1/data' into table T partition(p=0)");
if (dynPart && dpCtx != null && dpCtx.getNumDPCols() > 0) {
GenericUDFToUnixTimeStamp udf2 = new GenericUDFToUnixTimeStamp();
RelNode newProject = HiveProject.create(newInput, Pair.left(projects), Pair.right(projects));
File file = new File("./sales.txt");
public void example() throws Exception {
// Original bucket files and delta directory should stay until Cleaner kicks in.
this.negative = this.negative ^ right.negative;
if (inputOI.preferWritable()) {
Calendar cal = Calendar.getInstance();
SessionState ss = DriverUtils.setUpSessionState(conf, user, false);
for (Path dir : dirs) {
s = sqlGenerator.addLimitClause(10 * TIMED_OUT_TXN_ABORT_BATCH_SIZE, s);
conf.setVar(HiveConf.ConfVars.HIVE_TXN_RETRYABLE_SQLEX_REGEX, "^Deadlock detected, roll back,.*08177.*,.*08178.*");
// So, no need to attempt to merge the files again.
LOG.error("Unable to add settable data to UDF " + genericUDF.getClass());
final ValidReadTxnList validTxnList =
outputs.add(new WriteEntity(partn, WriteEntity.WriteType.DDL_NO_LOCK));
tablename = "tab1";
}
stopMiniHS2();
"eq(last_name, Binary{\"smith\"})"    /* 'smith' = last_name  */
if (fb.skipNulls && s.valueChain.size() == 0) {
assertEquals("Query should be finished",  OperationState.FINISHED, state);
replicas,
public static class OracleCommandParser extends AbstractCommandParser {
if (level == (input.size() - 1)) {
writeResources.release();
public void dump(String prefix) {
// compute statistics for columns viewtime
public class HBaseLazyObjectFactory {
return 0;
this.timer = new ScheduledThreadPoolExecutor(1);
DbTxnManager txnMgr2 = (DbTxnManager) TxnManagerFactory.getTxnManagerFactory().getTxnManager(conf);
private static void buildJSONString(StringBuilder sb, Object o, ObjectInspector oi) throws IOException {
too long.
Object result = FunctionRegistry.invoke(udfMethod, udf, conversionHelper
return harLocn;
* org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider
start = processBoundary(type, (ASTNode) node.getChild(0));
List<ExprNodeDesc> parentPCs = pRS.getConf().getPartitionCols();
updateAvgVariableSize(batch);
while (true) {
{"{\"writeid\":10000001,\"bucketid\":536936448,\"rowid\":0}\t60\t88", "warehouse/t/delta_10000001_10000001_0000/bucket_00001"},
rowCnt = getRowCnt(pctx, tsOp, tbl);
cal = Calendar.getInstance();
try {
minFinalFnOIs.add(rsValueCols.get(0).getWritableObjectInspector());
if (skewed == false) {
private HashSet<Class<? extends Node>> nodeTypes = new HashSet<Class<? extends Node>>();
conf.unset(ValidTxnList.VALID_TXNS_KEY);
if (hiveConf.getBoolVar(HiveConf.ConfVars.HIVE_VECTORIZATION_ENABLED) ||
tsWrapper.allocateTask(task4, hostsH1, priority1, clientCookie4);
Path subDir = fileStatus.getPath();
listNewFilesRecursively(destFs, fileStatus.getPath(), newFiles);
orgHiveLoader = conf.getClassLoader();
if (join.getJoinType() != JoinRelType.INNER) {
if (isOnDisk(partitionId)) {
cpr = driver.compileAndRespond("select a from T6", true);
ctx.addViewTokenRewriteStream(viewFullyQualifiedName, tokens);
fastSerializationScale = -1;
ReduceSinkDesc rsDescFinal = PlanUtils.getReduceSinkDesc(
// e1 * e2
// TODO For now, this affects non broadcast unsorted cases as well. Make use of the edge
fs.mkdirs(tezDir);
if (isSchemaVerified.get()) {
ASTNode child = (ASTNode) exprList.getChild(i);
// fields
Map<ReadEntity, ReadEntity> readEntityMap =
if (registry != null && registry.getVisited(this).contains(node)) {
if (oldSplit == null) {
// bad files don't pollute the filesystem
closeSession(ss);
FileStatus[] files = fs.listStatus(dir, isRawFormat ? AcidUtils.originalBucketFilter
return OPERATION_ID;
assertEquals(0,aggrStatsEmpty.getPartsFound());
dbRead = cachedStore.getDatabase(DEFAULT_CATALOG_NAME, dbName2);
// None.
int bucketCount = p.getBucketCount();
for (Operator<? extends OperatorDesc> parentOp : parentOps) {
public boolean allFile;
CacheWriter cacheWriter = currentFileRead.getCacheWriter();
if (cnt > 0) {
if (reduceKeys.size() == 0) {
private final String disableMessage;
private void initReplLoad(ASTNode ast) throws SemanticException {
fastScaleUp(
try {
optCluster.invalidateMetadataQuery();
final Configuration conf1 = new Configuration();
LOG.info(ex.getLocalizedMessage());
private Operator<?> fileSink;
}
private static long makeIntPair(int first, int second) {
nullIndicatorPos =
None
String defaultValueText  = tokenStream.toOriginalString(defaultValueAST.getTokenStartIndex(),
if (random.nextBoolean() || verifyTable.getCount() == 0) {
pushedPredicate =
continue;
// specified
ArrayList<ColumnInfo> signature = inputRS.getSignature();
too long.
byte[] input;
}
"ETLSplitStrategy", /* 256 files x 1000 size for 9 splits */
public synchronized RootAllocator getOrCreateRootAllocator(long arrowAllocatorLimit) {
// does it need an additional MR job
writeId = txnMgr.getTableWriteId("default", "tab1");
private static byte[] writeToBytesColumnVector(int rowIdx, BytesColumnVector col, int writeSize, byte val) {
too long.
isPartitionOrderBy = true;
checkException(oomeStr, stackTraces.get(0));
Map<Integer, Integer> mapNewInputToProjOutputs = new HashMap<>();
for (Map.Entry<Integer, Byte> entry : hsr.getSparseMap().entrySet()) {
maxProbeSize = Math.max(maxProbeSize, wbSize);
queryText = "select \"SD_ID\", \"SKEWED_COL_NAME\" from " + SKEWED_COL_NAMES + ""
key = key ^ (key >>> 28);
for (int i = 0; i < this.columns.size(); i++) {
DriverManager.registerDriver(new FakeDerby());
String dagName = utils.createDagName(conf, queryPlan);
index = Hashing.consistentHash(hash1 + iter * hash2, locations.size());
return null;
int hashCode = (int)writeBuffers.unsafeReadNByteLong(Ref.getOffset(oldRef)
transient Set<String> blackListedConfEntries = null;
if (limit <= parentStats.getNumRows()) {
return new Text("Unvectorized");
* org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider
List listData = (List) datum;
for (AggregateCall oldCall : oldCalls) {
* org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider
Deadline.startTimer("getAggrPartitionColumnStatistics");
if (factoryClassName == null){
List<String> partitionCols = new ArrayList<>(referencedColumns.size());
// map priv being granted to required privileges
for(ASTNode nodeFilter : node.getFiltersForPushing().get(0) ) {
Set<CacheEntry> entriesToRemove = new HashSet<CacheEntry>();
return true;
return
LOG.warn("" + unmatchedRows + " unmatched rows are found: " + rowText);
DruidWritable writable = (DruidWritable) serDe.serialize(rowObject, inspector);
return (int)(hashPart & (1 << (position - 1)));
if(wantManyQuantiles) {
StorageDescriptor sd = new StorageDescriptor();
return FIELD_SCHEMAS;
String outputAsString = FileUtils.readFileToString(outFile);
if (null != indexDef) {
}
@Override
private DummyInputSplit() {
results = objectStore.getSchemaVersionsByColumns("gamma", null, null);
// an error in creation, and we want to delete it anyway.
for (ExprNodeDesc cn : en.getChildren()) {
throw new UnsupportedOperationException("Undefined descriptor");
args.add("-libjars");
srcOp = insertSelectForSemijoin(fields, srcOp);
* org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider
stmt1.execute("alter table t1 change column c1 c1 int");
testLazyBinaryMap(r);
System.setProperty(MetastoreConf.ConfVars.EXECUTE_SET_UGI.toString(), "true");
LOG.warn("Unable to clean direct buffers using Cleaner.");
None
too long.
objectStore.addNotificationEvent(event);
}
if (!taskInfo.isPendingUpdate) {
private String getBaseFileName(String string) {
// column pruner
sessionConf="hive.server2.enable.doAs=true";
if (dbName != null) {
TableSpec tablepart = new TableSpec(this.db, conf, root);
* org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider
CommandProcessorResponse cpr = driver.run(sql);
ALTERTABLE_ADDPARTS("ALTERTABLE_ADDPARTS", null, new Privilege[]{Privilege.CREATE}),
ArrayList<Object[]> result = new ArrayList<Object[]>();
assertNotNull(reader2.getError());
return PROGRESSED_PERCENTAGE;
if (isVectorDeserializeEligable) {
if (opCtx.op instanceof TableScanOperator) {
public static boolean isSame(ExprNodeDesc desc1, ExprNodeDesc desc2) {
if (FileUtils.isPathWithinSubtree(path,hdfsTmpDir) || FileUtils.isPathWithinSubtree(path,localTmpDir)) {
server.start();
* org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider
final ConcurrentMap<String, RelOptMaterialization> prevCq = materializedViews.putIfAbsent(
a = b = c = (0x00000000deadbeefL + length + initval) & INT_MASK;
List<String> columnNames = Arrays.asList(columnNameProperty.split(columnNameDelimiter));
return false;
// if the ast has 3 children, the second *has to* be partition spec
// so it doesn't matter if we wait for all inputs or any input to be ready.
// destf
}
JSONArray array = vertexObject.getJSONArray(key);
return IS_SET_SCHEDULING_POLICY;
totalSize += computeOnlineDataSize(bigInputStat);
verifyHighPrecisionMultiplySingle(a2, b2);
return IS_SET_QUERY_PARALLELISM;
UserGroupInformation endUserUgi = UserGroupInformation.createRemoteUser(endUser);
resp = driver.run("alter table t1 set owner role r1");
patialS.set(0, new BytesRefWritable("NULL".getBytes("UTF-8")));
tblNameOrPattern = PlanUtils.stripQuotes(ast.getChild(currNode).getChild(0).getText());
updateStats(stats, cardinality, true, gop, false);
for (Map.Entry<String, ArrayList<CombineFileSplit>> entry: aliasToSplitList.entrySet()) {
LOG.debug("Removing AppMasterEventOperator " + eventOp + " and TableScan " + ts);
@Test
HIVE_SERVER2_GLOBAL_INIT_FILE_LOCATION("hive.server2.global.init.file.location", "${env:HIVE_CONF_DIR}",
too long.
ArrayList<Task<? extends Serializable>> rootTasks =
bucketField = recIdInspector.getAllStructFieldRefs().get(1);
// these are from ColumnPrunerSelectProc
if (sd.getCols() != null) {
QB blankQb = new QB(null, null, false);
op2Priv.put(HiveOperationType.ALTERTABLE_ADDPARTS, PrivRequirement.newIOPrivRequirement
batch = getBatchThreeBooleanCols();
int valueLength = (int) hashMap.writeBuffers.readVLong(readPos);
writeBufferSize = writeBufferSize < minWbSize ? minWbSize : Math.min(maxWbSize / numPartitions, writeBufferSize);
return String.valueOf(number);
ImmutableBitSet.Builder builder = ImmutableBitSet.builder();
MoveWork mw = new MoveWork(null, null, null, null, false);
addIfService(amReporter);
MapJoinProcessor.genLocalWorkForMapJoin(newWork, newMapJoinOp, bigTablePosition);
private static Set<AbstractMetaStoreService> metaStoreServices = null;
input[i++] =
protected static RowResolver createSelectListRR(MatchPath evaluator,
} catch (Throwable th) {
DataOutputStream outStream = getOutputStream(showCreateTbl.getResFile());
if (ref != null && ref.getToken().getType() == HiveParser.Number) {
ArrayList<String> columnNames = new ArrayList<String>(columns);
PcrExprProcCtx pprCtx = new PcrExprProcCtx(tabAlias, parts, vcs);
for (ColumnInfo cinfo : curr.getSchema().getSignature()) {
str = BaseSemanticAnalyzer.unescapeIdentifier(expr.getText().toLowerCase());
sessionManagerHS2.shutdown();
if (rowIds.isEmpty()) {
String endUserName = Utils.getUGI().getShortUserName();
if (e.getValue() == null || e.getValue().length == 0) {
OptimizeTezProcContext procCtx = new OptimizeTezProcContext(conf, pCtx, inputs, outputs);
String columnTypeProperty = tbl.getProperty(serdeConstants.LIST_COLUMN_TYPES);
JoinOperator joinOp = getJoinOp(currTask);
None
continue;
if (createTask instanceof DDLTask) {
memoryManager.reserveMemory(dest.length << allocLog2);
assert mergerOptions.isCompacting() : "Expected to be called as part of compaction";
parent.replaceChild(child, fileSinkOp);
int numThreads = 5;        // set to 1 for single threading
SSLTestUtils.setBinaryConfOverlay(confOverlay);
AcidUtils.Directory dir = AcidUtils.getAcidState(location, conf, txns);
} else if (params.length == 1) {
too long.
too long.
if(node instanceof RexCall) {
oneRowWithConstant.add(oneRow.get(cselOpTocgbyOp.get(pos) - cgbyOp.getConf().getKeys().size()));
HiveDecimalV1 oldSubtractDec;
System.arraycopy(inputIsNull, 0, outputIsNull, 0, n);
this.maxRetries = maxRetries;
try {
if (forwardOp.getDone()) {
if (!isValueLengthSmall) {
if (AvroSerdeUtils.isNullableType(recordSchema)) {
assertEquals(1l, theMap.get("one"));
if(isEXISTS) {
public static String kerberosChallenge(String server) throws AuthenticationException
appName = appName.substring(1);
// * implies all properties needs to be inherited
defaultHiveConf.set("hive.dummyparam.test.server.specific.config.override",
float waves =
@Metric
initializeTables();
LOG.info("Hash table number " + idx + " is empty");
referenceArguments = argumentsAccepted;
ColStatistics.Range combinedRange = StatsUtils.combineRange(selColStat.getRange(), tsColStat.getRange());
// tablescan and join operators.
ResultSet res = dbmd.getProcedureColumns(null, null, null, null);
sessionState.resetThreadName();
String colTypeStr = table.getPartitionKeys().get(partColIndex).getType();
// nanosecond interval in 2 primitives) produces a type timestamp (TimestampColumnVector).
Token<? extends TokenIdentifier> accumuloToken = getHadoopToken(token);
static int checkAggOrWindowing(ASTNode expressionTree) throws SemanticException {
TxnStatus actualTxnStatus = findTxnState(txnid, stmt);
newTbl1.setOwner("role1");
if (SessionState.get().getATSDomainId() == null) {
baseFileNameMapping.put(getBaseFileName(inputPath), bucketBaseFileNames);
StringBuilder sb = new StringBuilder(testTable.getDataLocation().toString());
String queryText =
DynamicSerDeExtends jjtn000 = new DynamicSerDeExtends(JJTEXTENDS);
if (union.getInputs().size() != 2) {
// expressions for project operator
public abstract class VectorMapJoinFastBytesHashSet
private byte[] internalScratchBuffer;
* org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider
return;
int batchSize = conf.getIntVar(ConfVars.HIVE_MSCK_REPAIR_BATCH_SIZE);
String[] pathExpr = pathExprCache.get(pathString);
}
if (useRowDeserialize) {
int digitCount = 0;
String fakeFile0 = TEST_WAREHOUSE_DIR + "/" + (Table.NONACIDORCTBL).toString().toLowerCase() +
colAlias = unescapeIdentifier(selExpr.getChild(1).getText().toLowerCase());
if (event.getDbName().equalsIgnoreCase(dbName) && event.getEventType().equalsIgnoreCase("INSERT")) {
private static final String INSERT_OVERWRITE_COMMAND_FORMAT =
byte[] keyBytes = currentKey.getBytes();
ResultSetMetaData rsmd = rs.getMetaData();
Map<String, List<MyTestInnerStruct>> myMap;
if( backupChildrenTasks!= null) {
transport = authBridge.createClientTransport(null, store.getHost(),
client.updateTableColumnStatistics(colStats);
return 0.0f;
too long.
PARQUET_MEMORY_POOL_RATIO("parquet.memory.pool.ratio", 0.5f,
addDef(args, "user.name", runAs);
// for current query.
return HivePrivilegeObjectType.PARTITION;
if (b == 0) {
return;
}
// If the view is Inside another view, it should have at least one parent
// Only for incremental load, need to validate if event is newer than the database.
int noMatchCount = subtractFromInputSelected(
HiveMetaStore.HMSHandler.createDefaultCatalog(objectStore, new Warehouse(conf));
getIntegerProperty(table, Constants.DRUID_KAFKA_INGESTION_PROPERTY_PREFIX + "maxPendingPersists"),
List<BucketCol> bucketCols = extractBucketCols(rop, outputValues);
return connStack.pop();
too long.
break;
RecordWriter writer = storageFormatTest.getRecordWriter(readPath);
private long timeout;
slotPairs[pairIndex] = valueStore.addFirst(valueBytes, 0, valueLength);
// Note that for temp tables there is no need to rename directories
for (Entry<ErrorHeuristic, HeuristicStats> ent : heuristics.entrySet()) {
while ((significand & 1) == 0) { // i.e., significand is even
None
sb.append(customPath.substring(previousEndIndex, matcher.start()));
parameters.remove(statType);
if (statusServiceDriver != null) {
/**
map.testPutRow(key);
}
public final PartitionState partitionState;
too long.
// We don't want that.
int i = childOp.getParentOperators().indexOf(parentOp);
if (posToVertex.containsKey(key)) {
List<Operator<? extends OperatorDesc>> children =
int depthDiff = realPartitionPath.depth() - tmpPath.depth();
* org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider
LockComponent comp = new LockComponent(LockType.EXCLUSIVE, LockLevel.DB, "mydb");
if ((isMmTableWrite || isFullAcidTable) && loadPath.equals(newPartPath)) {
Converter varcharConverter = ObjectInspectorConverters.getConverter(
return dbVersion;
destinationBatch.cols[inclBatchIx++] = sourceBatch.cols[columnId];
private Tree fromTree, tableTree;
filterRel = genFilterLogicalPlan(qb, srcRel, aliasToRel, outerNameToPosMap, outerRR, false);
parentDir.deleteOnExit();
signum = (firstByte < 0) ? (byte) -1 : (byte) 1;
assertTrue(r.isNull[2]);
for(String f : files) {
if (c1.equals(Category.LIST)) {
context.currentUnionOperators.clear();
int headerIx = pos >>> minAllocLog2;
ObjectInspector foi = structField.getFieldObjectInspector();
RelDataType aggFnRetType = TypeConverter.convert(agg.m_returnType,
/*
assertTrue(ti1.isGuaranteed());
LazyBinaryUtils.readVInt(bytes, offset, vInt);
List<String> originalColumnNames =
public GenericUDAFMkCollectionEvaluator() {
if (cmdLine.contains("=")) {
too long.
final RelNode newInput = frame.r;
schema.setProperty(org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.BUCKET_COUNT,
MetastoreConf.setLongVar(conf, ConfVars.DIRECT_SQL_MAX_QUERY_LENGTH, 1);
public Boolean myBool;
too long.
Assert.assertEquals("closed file size mismatch", bucket0File.getLen(),
return (actualState.sent == actualState.target);
for (int idx = 0; idx < oldInvalidIds.length; ++idx) {
public class ParseError {
storeToken(token, ugi);
LOG.debug("SMB Join can't be performed due to bucketing version mismatch");
List<String> addedFamilies = new ArrayList<String>();
IntegerStringMapHolder o1 = new IntegerStringMapHolder();
HttpClient httpClient = mock(HttpClient.class);
BloomKFilter.mergeBloomFilterBytes(
"ETLSplitStrategy", /* 1 files x 100 size for 111 splits */
if (partitions.isEmpty()) {
NoFile,
if (arg2ColVector.isRepeating) {
ParseContext tempParseContext = getParseContext(pCtx, rootTasks);
m2 = new HashMap<>(2);
if (++nusedbins > nbins) {
too long.
if (leastConversionCost == 0) {
lastUncompressed = copyAndReplaceCandidateToNonCached(
printUsage();
validateWindowFrame(wdwSpec);
// create a table with multiple partitions
private final Object CACHE_TEARDOWN_LOCK = new Object();
for (FileSystem.Statistics statistics : FileSystem.getAllStatistics()) {
* org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider
// so we don't create an extra SD in the metastore db that has no references.
None
if (scaleUp < HIGHWORD_DECIMAL_DIGITS) {
* @throws Exception
session1.returnToSessionManager();
Map<String, ColumnStatistics> newStatsMap = new HashMap<>();
cpuCost += cardinality * cpuCost;
Long prevOffset = cache.floorKey(absOffset);
HashSet<String> poolsToRedistribute = new HashSet<>();
//the Group By args are passed to cardinality_violation to add the violating value to the error msg
int floor = 1 << 30;
int len = 0;
originalPredicate);
/**
if(klass.indexOf("vector") != -1 || klass.indexOf("Operator") != -1) {
vrg.projectionSize = originalProjectionSize;
b.cols[1] = r = new DecimalColumnVector(hiveDecimalValues.length, 5, 2);
assertFalse(ObjectInspectorUtils.compareSupported(uoi1));
HCatTable targetTable = targetMetaStore().deserializeTable(sourceMetaStore().serializeTable(sourceTable));
too long.
Double timestampDouble = TimestampUtils.getDouble(timestamp);
None
return NAME_TO_TYPE_PTR;
if (checkExpressions((SelectOperator)child)) {
} else if(rowIndex < batchSize) {
final List<RexNode> newVCLst = new ArrayList<RexNode>();
if (colStatObj == null) {
ArrayList<Object> acc = new ArrayList<Object>();
ExprNodeDesc column2 = new ExprNodeColumnDesc(TypeInfoFactory.stringTypeInfo, "rid", null,
processPositionAlias(ast);
LazyPrimitive<? extends ObjectInspector,? extends Writable> key = LazyFactory
VectorTaskColumnInfo vectorTaskColumnInfo = new VectorTaskColumnInfo();
aggrStatsCache.add(catName, dbName, tableName, colName, partsFound, colStatsAggr, bloomFilter);
private int findMSB(int n) {
if (bigTableValueExpressions != null) {
assertEquals(true, res.getBoolean(1));
t = pt.getRawType();
Set<Operator<?>> set = new HashSet<Operator<?>>();
private void removed(int index) {
throw new RuntimeException("Unsupported window Spec");
if (doUseFreeListDiscard && freeListIx > 0) {
clonedParentWork.setName(clonedParentWork.getName().replaceAll("^([a-zA-Z]+)(\\s+)(\\d+)",
too long.
assert (children.size() == 2);
public void exportCounters(AbstractMap<String, Long> counters) {
constantDesc = new ExprNodeConstantDesc(100);
return returnDecimalType;
// This method is used to validate check expression since check expression isn't allowed to have subquery
throw new IOException("Could not find status of job:" + rj.getID());
runStatementOnDriver("alter table "+ TableExtended.MMTBL + " compact 'MAJOR'");
too long.
Context ctx = new Context(newJob);
RexNode leftRef = rexBuilder.makeInputRef(
// Implicit -- use batchIndex.
diffScale = leftScale - rightScale;
String kerberosName = SecurityUtil
DiskRangeList current = findExactPosition(start, cOffset);
Writable[] convertTargetWritables;
for (Stage candidate : this.stages.values()) {
if (testDesc.bigTableKeyTypeInfos.length == 1) {
Collection<Token<? extends TokenIdentifier>> tokens = ugi.getTokens();
ArrayList<Object> struct = new ArrayList<Object>(3);
TransactionBatch txnBatch =  connection.fetchTransactionBatch(10, writer);
tsWrapper.deallocateTask(task1, true, null);
batchIndexToResult[evictedBatchIndex] = EXCLUDE;
FunctionRegistry.registerTemporaryUDF("tmp_concat", GenericUDFConcat.class, emptyResources);
for (String funcName : allFunctions) {
fs.mkdirs(partPath); // Attempt to make the path in case it does not exist before we check
String tbl_temp = "";
String qualifierName = colMap.qualifierName;
final int THREAD_COUNT = 2, ITER_COUNT = 1000, ATTEMPT_COUNT = 3;
PrincipalPrivilegeSet thrifPrivs = null;
dir.mkdirs();
too long.
try {
LOG.warn("Unexpected exception while adding " +ADMIN+" roles" , e);
return COLUMN_NAME;
if (e.dumpStateFuture != null) {
too long.
while (miniHS2_1.getOpenSessionsCount() != 0) {
currentDataColumnCount = currentVectorPartContext.getReaderDataColumnCount();
return FOREIGN_KEY_COLS;
value = new byte[random.nextInt(MAX_VALUE_LENGTH)];
Assert.assertEquals(bigInteger, deserializedBigInteger);
return null;
// and check HDFS before and after.
* org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider
final long startToEnd;
return FKTABLE_NAME;
functions = client.getFunctions(DEFAULT_DATABASE, "*_to_find_*|*_hidden_*");
Class<List<HivePrivilegeObject>> class_listPrivObjects = (Class) List.class;
if (!isUserAdmin()) {
/**
}
HIVE_SERVER2_THRIFT_RESULTSET_MAX_FETCH_SIZE("hive.server2.thrift.resultset.max.fetch.size",
// HiveChar.toString() returns getPaddedValue()
RowSchema rowSchema = parentRS.getParentOperators().get(0).getSchema();
}
if (gbInfo.containsDistinctAggr) {
for (int i = 0; i < outputKeyLength; i++) {
if (isCandidate && chAlias != null) {
TemporalAccessor accessor = FORMATTER.parse(s);
Map<String, List<String>> cookieMap = cookieManager.get(uri, Collections.<String, List<String>>emptyMap());
result[pos] = vector.traverse(pos);
too long.
public interface FileListProvider {
int am1Port = 123;
private String diagnostics;
if (inputObject.has("cboInfo")) {
too long.
/* 1. is defined with skewed columns and skewed values in metadata */
for (Operator<?> op : sr.discardableOps) {
null, // comment passed as table params
// LinkedHashMap to provide the same iteration order when selecting a random host.
String kerberosName;
DruidWritable writable = (DruidWritable) serDe.serialize(rowObject, inspector);
if (m == Mode.PARTIAL1 || m == Mode.COMPLETE) {
return INCLUDE_BITSET;
typeAffinity("typeaffinity1", TypeInfoFactory.dateTypeInfo, 1, DateWritableV2.class);
public static void initUnionPlan(GenMRProcContext opProcCtx, UnionOperator currUnionOp,
return IS_SET_DEFAULT_POOL_PATH;
String modifier = " with (updlock)";
List<ObjectInspector> unionOI =  new ArrayList<ObjectInspector>();
gather = false;
// we can bail out
while (ti.task.getParentTasks() != null && ti.task.getParentTasks().size() == 1) {
RexNode calciteJoinCond = null;
ArrayList<String> values = new ArrayList<String>(partColumnNames.size());
// Note: we could use RW lock to allow concurrent calls for different sessions, however all
protected static class DirectKeyValueWriter implements KeyValueHelper {
StdAgg myagg = (StdAgg) agg;
private Map<String, List<TableDesc>> eventSourceTableDescMap =
expr.setOutputTypeInfo(TypeInfoFactory.longTypeInfo);
}
"ETLSplitStrategy", /* 100 files x 1000 size for 99 splits */
ServiceUtils.cleanup(LOG, parentSession.getSessionState().out, parentSession.getSessionState().err);
this.nodeBlacklistConf = new NodeBlacklistConf(
ArrayList<ColumnInfo> outputCols = new ArrayList<ColumnInfo>();
"-9999999999999999",
private int index = -1;
int initialCapacity = shuffleInputs.size();
for (Entry<String, String> entry : modifiedConf.entrySet()) {
* org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider
Assert.assertEquals(4, status.length);
private boolean isVectorizationGroupByComplexTypesEnabled;
while (true) {
return OWNER_TYPE;
if (!LlapDaemonInfo.INSTANCE.isLlap()) {
None
if (sz >= 100000) {
if (condn.getChildCount() == 1) {
{"ColumnDivideScalar", "Divide", "long", "double", "/"},
String[] terms = internalName.split("\\.");
jlpi = new JoinLeafPredicateInfo(pe.getKind(), joinExprs,
bigDecimal = BigDecimal.ZERO;
String owner = SecurityUtils.getUser();
boolean allKeyInputColumnsRepeating;
int contextSize = Integer.parseInt( partial.get(partial.size()-1).toString() );
for (int stripeIxMod = 0; stripeIxMod < stripeRgs.length; ++stripeIxMod) {
reloadFolder = new File(hiveReloadPath);
if (useVectorizedInputFileFormat) {
too long.
ReaderPairAcid deltaPair = new ReaderPairAcid(key, deltaReader, minKey, maxKey, deltaEventOptions, conf);
batch = makeStringBatchForColColCompare();
// column stats for a group by column
// the set of dynamic partitions
}
// a copy is required to allow incremental replication to work correctly.
COMMIT_READY,
oldDec = oldDec.abs();
private static final ThreadLocal<TimeZone> LOCAL_TIMEZONE = new ThreadLocal<TimeZone>() {
Map<String, SessionTriggerProvider> allSessionProviders = wm.getAllSessionTriggerProviders();
for (int i = 0; i < writables.length; i += 1) {
HiveProject replacementProjectRel = HiveProject.create(obChild.getInput(), obChild
replicas,
// Move all the partition columns at the end of table columns.
private static class TestFSDataInputStream extends FSDataInputStream {
continue;
while (true) {
"-1000000000000000",
RexNode fetchRN = sort.getCluster().getRexBuilder()
/**
if (restrictedConfig != null) {
executeStatementOnDriver("INSERT INTO " + tblName + "(a,b) VALUES(2, 'bar')", driver);
checkRemainingPartitions(sourceTable, destTable,
// has the permissions on the table dir
// byte.
if (qb.isInsideView() && parentInput == null) {
Database dbRead = cachedStore.getDatabase(DEFAULT_CATALOG_NAME, dbName);
if (!ctx.getExplainLogical()) {
if (dest_type.intValue() == QBMetaData.DEST_TABLE
testAllocation(10, 1.0f,
return rawStore.getPartition(catName, dbName, tblName, part_vals);
// appended once all the session list are added to the url
// plans.
private transient Object[] result;
return USER_NAME;
boolean getResults(List res) throws IOException;
// need to do the work to detangle this
client.dropPartition(dbName, tblName, Arrays.asList("20160102"));
batch = getBatchThreeBooleanCols();
Map<Node, Object> outputMap = PrunerUtils.walkExprTree(pred, pprCtx, getColumnProcessor(),
ExprNodeGenericFuncDesc expr = null;
if (replicationSpec.isInReplicationScope()){
testIntCaseWithFail((String) testCase[0], trim);
ShimLoader.getHadoopShims().getMergedCredentials(jobConf);
* org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider
if (numBuckets < 0) {
if (!isRoundPortionAllZeroes) {
// is not expected further down the pipeline. see jira for more details
assertEquals(20, meta.getColumnDisplaySize(18));
config.setConnectionTimeout(connectionTimeout);
try {
List<ResourceUri> transformedUris = ImmutableList.copyOf(
vecExpr.transientInit();
private transient HashMap<String, VectorPartitionContext> fileToPartitionContextMap;
results = objectStore.getSchemaVersionsByColumns(null, "namespace=x", null);
private static final String TOPN_QUERY =
None
// (Otherwise, sub-directories produced by Hive UNION operations won't be readable.)
static Map<Integer, Integer> identityMap(int count) {
private AnalyzeRewriteContext analyzeRewrite;
partSpecs.add(new DropTableDesc.PartSpec(expr, partSpecKey));
None
abstract class FileRecordWriterContainer extends RecordWriterContainer {
public final Set<Operator<?>> clonedPruningTableScanSet;
super(vectorSMBJoinDesc, false);
colExprMap.put(field, grpByExprNode);
int i = numAliases - 1;
CommonCliOptions.splitAndSetLogger(propKey, confProps);
/**
public class LlapArrowBatchRecordReader extends LlapBaseRecordReader<ArrowWrapperWritable> {
AccumuloConnectionParameters cnxnParams = new AccumuloConnectionParameters(null);
// we will bail out; we do not want to end up with limits all over the tree
continue;
None
for (String columnName : columns) {
return new RegexFilterSet()
final DecimalTypeInfo decimalTypeInfo = (DecimalTypeInfo) field.typeInfo;
factor *= columnFactor > 1d ? 1d : columnFactor;
if (UserGroupInformation.isSecurityEnabled()) {
if (primary.nextRecord() == null ||
* c. Rebuilt the QueryDef.
None
case 3:
if (ReduceSinkDeDuplicationUtils.merge(cRS, pRS, dedupCtx.minReducer())) {
for (Entry<Object, Object> entry : storer.getProperties().entrySet()) {
}
rc = jobRef.monitorJob();
if (type.equals("miniMR")) {
StringBuilder pseudoPartName = new StringBuilder();
if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_IN_TEST) &&
txnHandler.cleanTxnToWriteIdTable();
return numRows / 2;
outputIsNull[i] = false;
maybeRegisterForVertexUpdates(src);
// clean request
if (null == root) {
too long.
throw new IOException(e);
for (ColumnStatisticsObj obj : csOld.getStatsObj()) {
// After this the KeyWrappers are properly set and hash code is computed
final RelBuilder relBuilder = ruleCall.builder();
writer.addRowBatch(batch);
protected long fastSerialize64(int scale) {
if (first) {
char separator = ':';
/**
if (fri1.getNumSelfAndUpstreamTasks() > fri2.getNumSelfAndUpstreamTasks()) {
if (!lastWasMasked) {
done = !needsPostEvaluation;
job.setNumReduceTasks(0);
if ((pti.getPrimitiveCategory() != PrimitiveCategory.DECIMAL)
too long.
return STARTED_TIME;
public static SparkPartitionPruningSinkOperator findReusableDPPSink(
case HiveParser.TOK_ALTERVIEW_DROPPARTS:
replica.load(replicatedDbName, tuple.dumpLocation);
incrementalLoadAndVerify(dbName, bootstrapDump.lastReplId, replDbName);
/**
openSession();
for(t = peek(); (t == null) || !t.text.equals(")"); t = expect(",",")")) {
parts = msdb.getPartitions(catName, dbname, name, -1);
for (int i = 1; i <= 5; i++) {
private String statsTmpDir;
LOG.warn("Using full partition scan :" + Arrays.toString(part.getPath()) + ".", e);
assertTrue(e.getMessage().contains("Invalid number of arguments"));
stmt.execute("create table " + partitionedTableName
// if the Hive configs are received from WITH clause in REPL LOAD or REPL STATUS commands.
return DEFAULT_CONSTRAINT_COLS;
final AtomicReference<WmTezSession> session3 = new AtomicReference<>(),
long repeatedOriginalWriteId = (originalWriteId != null) ? -1
public class TestLazyHBaseObject extends TestCase {
Partition resultPart = client.getPartition(destTable.getDbName(), destTable.getTableName(),
private OrcStruct extraValue;
//table or partition's statistics and table or partition's column statistics are accurate or not.
return TBL_PATTERNS;
None
runStatementOnDriver("alter table "+ Table.ACIDTBL + " compact 'MAJOR'");
fields.add(new FieldSchema("PaRT1", serdeConstants.STRING_TYPE_NAME, ""));
List<ResourceUri> resources = getResourceList(ast);
public abstract T execute() throws Exception;
ImmutableMap<String, Integer> hiveColNameCalcitePosMap = buildHiveToCalciteColumnMap(
FastBitSet bitset = GroupByOperator.groupingSet2BitSet(groupingSet, groupingSetsPosition);
// And, their types.
writeId = txnMgr2.getTableWriteId("default", "target");
if (i == maxBatchesRG - 1) {
AuthenticationToken token = ConfiguratorBase.getAuthenticationToken(
do {} while (!isClosed && !isInterrupted && !queue.offer(o, 100, TimeUnit.MILLISECONDS));
t2.join(6000);
isDeleteRecordAvailable = deleteRecords.next(deleteRecordKey, deleteRecordValue);
"not(lteq(id, 13))",                  /* 13 < id or */
None
if (options.getTableProperties() != null) {
private Map<String, PoolState> pools;
return SERDE_TYPE;
public final void startAbortChecks() {
return false;
String columnValue = partKVs.get(columnName);
private Token<JobTokenIdentifier> token;
double decimalmin= 0;
out.write(TEST_BYTE_ARRAY);
private static final String TIMESERIES_QUERY =
ArrayList<ExprNodeDesc> newValExprs = new ArrayList<ExprNodeDesc>();
too long.
server = new HiveServer2();
MapWork bigMapWork = null;
/**
checkNoScan(tree);
LOG.error("Could not stop tez dags: ", e);
neededVirtualColumnSet = new HashSet<VirtualColumn>();
result.schema(rowSchema);
Mockito.when(helper.hasKerberosCredentials(ugi)).thenReturn(false);
None
return FOREIGN_DB_NAME;
return getTypeInfoForPrimitiveCategory((PrimitiveTypeInfo)a, (PrimitiveTypeInfo)b, pcA);
String HADOOP_PROXY_USER = "HADOOP_PROXY_USER";
verifyMapping(wm, conf, mappingInput("u0", groups("g0")), "u0");
None
// 2. If the outputOI has all fields settable, return it
return Character.isDigit(buf[offset]);
ColumnInfo col = new ColumnInfo(sf.getFieldName(),
return VALIDATE_CSTR;
if (this.fitsInt32() && o.fitsInt32()
String colType = indexDef.getColType(cf, cq);
// Since we are creating with scale 0, no fraction digits to zero trim.
VectorizedRowBatch batch = getBatch1Long3BytesVectors();
// TODO This should be passed in the TaskAttemptContext instead
if (objectInspector.getPrimitiveCategory() != PrimitiveCategory.STRING && ColumnEncoding.BINARY == encoding) {
byteSizeStart = byteStream.getLength();
expr = new StringSubstrColStart(0, -6, 1);
EximUtil.doCheckCompatibility(
String[] unptn_data = new String[]{ "eleven" };
System.err.println("Warning in pre-upgrade script " + preUpgradeScript + ": "
// System.out.println(v1);
RemoveSessionResult rr = checkAndRemoveSessionFromItsPool(
CommandProcessorResponse cpr = runStatementOnDriverNegative(
partValues[i] = null;
runStatementOnDriver("insert into " + TableExtended.MMTBL + "(a,b) values(1,2)");
DiskRangeList prev = cc.prev;
helper.updateOutputFormatConfWithAccumuloToken(jobConf, ugi, cnxnParams);
if (cnt > 1) {
String addedJars = Utilities.getResourceFiles(conf, SessionState.ResourceType.JAR);
too long.
Dispatcher disp = new DefaultRuleDispatcher(BucketingSortingOpProcFactory.getDefaultProc(),
MockFileSystem fs = new MockFileSystem(conf,
newDistinctKeyLists.remove(i);
}
taskListInConditionalTask = ((ConditionalTask) nd).getListTasks();
int b1 = arg1[i + start1] & 0xff;
String query = "select * from " + tableName;
}
if ( wExprsInDest != null &&
private transient StandardListObjectInspector loi;
public AtomicInteger getUsers() {
func.apply(entry.getValue(), fields);
lcv3.isRepeating = true;
assertFalse(mm.reserveMemory(1, false));
}
if (inputFileChangeSenstive) {
None
// So, min(txn_id) would be a non-zero txnid.
rqst.setReplPolicy(replPolicy);
LazyBinaryUtils.readVInt(bytes, offset, tempVInt);
too long.
MergeJoinWork mergeJoinWork = null;
HiveConf conf = queryState.getConf();
// returns whether a record was forwarded
jobConf = new JobConf(jobContext.getConfiguration());
for (int i = 0; i < fieldSchemas.size(); i++) {
ExprNodeDesc column = new ExprNodeColumnDesc(TypeInfoFactory.intTypeInfo, "key", null, false);
try {
path = new Path(path,"_dummy");
when(mockedAuthorizer.filterListCmdObjects(any(List.class),
for(String file : extraFiles) {
if (result.getTaskError() instanceof HiveException) {
DruidWritable writable = (DruidWritable) serDe.serialize(rowObject, inspector);
None
TransactionBatch txnBatch1 =  connection.fetchTransactionBatch(10, writer);
SELECT, INSERT, UPDATE, DELETE;
public boolean[] isNull;
CacheChunk replacedChunk = toDecompress.get(i);
int idx = line.indexOf(' ');
HttpClient httpClient = mock(HttpClient.class);
batchSize = 0;
@VisibleForTesting
}
* @see org.apache.hive.service.cli.CLIServiceTest#setUp()
LOG.trace("Verbose estimation for collection {} from {}", fieldObj.getClass().getName(),
currentPartDeserializer = null;
options.addOption(OptionBuilder
too long.
for (Partition p : partitionsAdded) {
private static final String REPL_EVENTS_MISSING_IN_METASTORE = "Notification events are missing in the meta store.";
return MAPPINGS;
if (MetaStoreUtils.isFastStatsSame(oldTmpPart, tmpPart)) {
INITIALIZED,
tblRead = objectStore.getTable(DEFAULT_CATALOG_NAME, dbName, tblName1);
"there should be " + String.valueOf(expectedNumOCleanedFiles) + " deleted files in cm root",
try {
this.unscaledValue.addDestructiveScaleTen(right.unscaledValue,
boolean isTxnTable = conf.getBoolean(hive_metastoreConstants.TABLE_IS_TRANSACTIONAL, false);
files.addAll(otherFiles);
PrunedPartitionList prevTsOpPPList = pctx.getPrunedPartitions(tsOp1);
fastResult.fastSignum = -1;
/**
runStatementOnDriver("alter table "+ TableExtended.MMTBL + " compact 'MINOR'");
@SuppressWarnings("unused")
KeyBuffer keyBuffer;
ColumnMapping columnMapping = ColumnMappingFactory.get(columnMappingStr, defaultEncoding,
clusterSpecificConfiguration
try {
too long.
}
assertEquals("Expected empty set of partitions.",
// Request task2 (task1 already started at previously set time)
// metastore and so some partitions may have no data based on other filters.
if (forwardOp.getDone()) {
for (BaseWork child : children) {
GenericUDAFResolver udaf =
HiveRulesRegistry registry = call.getPlanner().
List<String> cachedCatalogs = cachedStore.getCatalogs();
mr.setupConfiguration(getHiveConf());
r.enforceMaxLength(getCharacterMaxLength(type));
String confVarPatternStr = Joiner.on("|").join(convertVarsToRegex(sqlStdAuthSafeVarNames));
long col3NumTrues = 100;
ret[i] = ((ConstantObjectInspector) oi).getWritableConstantValue();
projsJoinKeysInJoinSchema.add(projsJoinKeysInChildSchema.get(0));
}
/**
private ArrayWritable valueObj = null;
return POOL_PATH;
for (Map.Entry<String, ExprNodeDesc> mapEntry : reduceSinkOp.getColumnExprMap().entrySet()) {
server.stop();
addDependentMoveTasks(mvTask, hconf, task, dependencyTask);
MapJoinDesc mapJoinDesc = MapJoinTestConfig.createMapJoinDesc(testDesc);
@Override
* @see org.apache.hive.service.cli.CLIServiceTest#setUp()
None
return;
STATIC_LOG.debug("Table " + tabIdName + " is not found in walkASTMarkTABREF.");
result = new CheckResult();
if (!outputColNames && !outputColSchemas) {
public Node peekNode() {
private static JobRequestExecutor<List<JobItemBean>> jobRequest =
long col2MaxColLen = 100;
client.createTable(table);
float hashtableMemoryUsage;
for (Long lockId : expiredLocks) {
queryText = "select \"PART_ID\", \"PARAM_KEY\", \"PARAM_VALUE\" from " + PARTITION_PARAMS + ""
}
// while and should be done when we start up.
tblRead = cachedStore.getTable(DEFAULT_CATALOG_NAME, dbName, tblName1);
if (!(nd instanceof ExprNodeGenericFuncDesc)) {
continue;
List<Long> fileIds = determineFileIdsToQuery(files, result, posMap);
if (numColumns < readColIds.size())
if (wrappedIf != null) {
try {
hll.merge(hll5);
