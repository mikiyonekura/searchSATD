search_string,predict_under_code,line_no,similarity,filepath
"// TODO these bytes should be versioned","DataOutputBuffer port_dob = new DataOutputBuffer();",387,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/ShuffleHandler.java
"// TODO 1.0 miniCluster is slow this test times out, make it work","// renaming test to make test framework skip it",210,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatDynamicPartitioned.java
"/*    * Send dropped table notifications. Subscribers can receive these notifications for   * dropped tables by listening on topic "HCAT" with message selector string   * {@value org.apache.hive.hcatalog.common.HCatConstants#HCAT_EVENT} =   * {@value org.apache.hive.hcatalog.common.HCatConstants#HCAT_DROP_TABLE_EVENT}   * </br>   * TODO: DataNucleus 2.0.3, currently used by the HiveMetaStore for persistence, has been   * found to throw NPE when serializing objects that contain null. For this reason we override   * some fields in the StorageDescriptor of this notification. This should be fixed after   * HIVE-2084 "Upgrade datanucleus from 2.0.3 to 3.0.1" is resolved.    */","too long.",0,0,0
"/*  * This utility is designed to help with upgrading to Hive 3.0.  On-disk layout for transactional * tables has changed in 3.0 and require pre-processing before upgrade to ensure they are readable * by Hive 3.0.  Some transactional tables (identified by this utility) require Major compaction * to be run on them before upgrading to 3.0.  Once this compaction starts, no more * update/delete/merge statements may be executed on these tables until upgrade is finished. * * Additionally, a new type of transactional tables was added in 3.0 - insert-only tables.  These * tables support ACID semantics and work with any Input/OutputFormat.  Any Managed tables may * be made insert-only transactional table. These tables don't support Update/Delete/Merge commands. * * This utility works in 2 modes: preUpgrade and postUpgrade. * In preUpgrade mode it has to have 2.x Hive jars on the classpath.  It will perform analysis on * existing transactional tables, determine which require compaction and generate a set of SQL * commands to launch all of these compactions. * * Note that depending on the number of tables/partitions and amount of data in them compactions * may take a significant amount of time and resources.  The script output by this utility includes * some heuristics that may help estimate the time required.  If no script is produced, no action * is needed.  For compactions to run an instance of standalone Hive Metastore must be running. * Please make sure hive.compactor.worker.threads is sufficiently high - this specifies the limit * of concurrent compactions that may be run.  Each compaction job is a Map-Reduce job. * hive.compactor.job.queue may be used to set a Yarn queue ame where all compaction jobs will be * submitted. * * In postUpgrade mode, Hive 3.0 jars/hive-site.xml should be on the classpath. This utility will * find all the tables that may be made transactional (with ful CRUD support) and generate * Alter Table commands to do so.  It will also find all tables that may not support full CRUD * but can be made insert-only transactional tables and generate corresponding Alter Table commands. * * TODO: rename files * * "execute" option may be supplied in both modes to have the utility automatically execute the * equivalent of the generated commands * * "location" option may be supplied followed by a path to set the location for the generated * scripts.  */","too long.",0,0,0
"//  srcs = new FileStatus[0]; Why is this needed?","}",3919,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
"//  TODO: should probably throw an exception here.","return;",261,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/jdbc/src/java/org/apache/hive/jdbc/HiveQueryResultSet.java
"//  2. Convert Agg Fn args and type of args to Calcite   TODO: Does HQL allows expressions as aggregate args or can it only be","// projections from child?",3354,0.5957446808510638,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
"//  TODO: note that the token is not renewable right now and will last for 2 weeks by default.","Token<LlapTokenIdentifier> token = new Token<LlapTokenIdentifier>(llapId, this);",251,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-common/src/java/org/apache/hadoop/hive/llap/security/SecretManager.java
"//  TODO: Should this be also TOP_DOWN?","for (RelOptRule r : rules)",2249,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
"/*  * TODO: Most of the code in this class is ripped from ZooKeeper tests. Instead * of redoing it, we should contribute updates to their code which let us more * easily access testing helper objects. * *XXX: copied from the only used class by qtestutil from hbase-tests  */","too long.",0,0,0
"//  If it's constant = constant or column = column, we can't fetch any ranges   TODO We can try to be smarter and push up the value to some node which","// we can generate ranges from e.g. rowid > (4 + 5)",248,0.6927374301675978,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/predicate/AccumuloRangeGenerator.java
"//  TODO: Implement propConstDistUDAFParams","constantPropDistinctUDAFParam = SemanticAnalyzer",919,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/HiveGBOpConvUtil.java
"//  TODO: implement implicit AsyncRDDActions conversion instead of jc.monitor()?   TODO: how to handle stage failures?","String clientId = getClientId(jobId);",505,0.8255813953488372,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/spark-client/src/main/java/org/apache/hive/spark/client/RemoteDriver.java
"//  TODO: should we check isAssignableFrom?","if (ifName.equals(format)) {",313,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java
"//  For replication add-ptns, we need to follow a insert-if-not-exist, alter-if-exists scenario.   TODO : ideally, we should push this mechanism to the metastore, because, otherwise, we have   no choice but to iterate over the partitions here.","too long.",0,0,0
"//  TODO: should we create the batch from vrbctx, and reuse the vectors, like below? Future work.","inputVrb.cols[ixInVrb] = cvb.cols[ixInReadSet];",351,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java
"//  this is a hacky way of doing the quotes since it will match any 2 of   these, so   "[ hello this is something to split [" would be considered to be quoted.","schema.setProperty(serdeConstants.QUOTE_CHAR, "(\"|\\[|\\])");",322,0.6214689265536724,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/serde/src/test/org/apache/hadoop/hive/serde2/TestTCTLSeparatedProtocol.java
"//  TODO: Do the type checking of the expressions","List<String> tabBucketCols = tab.getBucketCols();",11054,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
"//  Make a tree out of the filter.   TODO: this is all pretty ugly. The only reason we need all these transformations         is to maintain support for simple filters for HCat users that query metastore.         If forcing everyone to use thick client is out of the question, maybe we could         parse the filter into standard hive expressions and not all this separate tree","too long.",0,0,0
"//  Not safe to continue for RS-GBY-GBY-LIM kind of pipelines. See HIVE-10607 for more.","return false;",137,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/optimizer/LimitPushdownOptimizer.java
"//  Hack for tables with no columns   Treat it as a table with a single column called "col"","cachedObjectInspector = ObjectInspectorFactory",100,0.5656565656565656,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/itests/test-serde/src/main/java/org/apache/hadoop/hive/serde2/TestSerDe.java
"//  TODO: if we expect one dir why don't we enforce it?","for (FileStatus fileStatus : contents) {",1182,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorMR.java
"//  Post serialization, separators are automatically inserted between different fields in the   struct. Currently there is not way to disable that. So the work around here is to pad the","// data with the separator bytes before creating a "Put" object",943,0.6923076923076923,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/hbase-handler/src/test/org/apache/hadoop/hive/hbase/TestHBaseSerDe.java
"//  We only support limited unselected column following by order by.   TODO: support unselected columns in genericUDTF and windowing functions.   We examine the order by in this query block and adds in column needed   by order by in select list.","if (obAST != null && !(selForWindow != null && selExprList.getToken().getType() == HiveParser.TOK_SELECTDI) && !isAllColRefRewrite) {",4588,0.4453125,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
"//  TODO: if we want to be explicit about this dump not being a replication dump, we can   uncomment this else section, but currently unneeded. Will require a lot of golden file   regen if we do so.","return table;",97,0.6167400881057269,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/io/TableSerializer.java
"//  TODO: refactor this in HIVE-6366","final JobConf cloneJobConf = new JobConf(jobConf);",187,0.7317073170731707,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/ProjectionPusher.java
"//  TODO: (a = 1) and NOT (a is NULL) can be potentially folded earlier into a NO-OP","// fall through",161,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/optimizer/FixedBucketPruningOptimizer.java
"//  @todo: remove this. 8/28/14 hb   for now adding because RelOptUtil.classifyFilters has an assertion about   column counts that is not true for semiJoins.","if (joinRel instanceof SemiJoin) {",199,0.35443037974683544,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/stats/HiveRelMdRowCount.java
"//  TODO: this seems to indicate that priorities change too little...         perhaps we need to adjust the policy.","for (int j = 0; j < 10; ++j) {",349,0.7891156462585034,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestLowLevelLrfuCachePolicy.java
"//  Note - we need srcFs rather than fs, because it is possible that the _files lists files   which are from a different filesystem than the fs where the _files file itself was loaded   from. Currently, it is possible, for eg., to do REPL LOAD hdfs://<ip>/dir/ and for the _files   in it to contain hdfs://<name>/ entries, and/or vice-versa, and this causes errors.   It might also be possible that there will be a mix of them in a given _files file.   TODO: revisit close to the end of replv2 dev, to see if our assumption now still holds,   and if not so, optimize.","too long.",0,0,0
"//  Note: with some trickery, we could add logic for each type in ConfVars; for now the   potential spurious mismatches (e.g. 0 and 0.0 for float) should be easy to work around.","validateRestrictedConfigValues(var.varname, userValue, serverValue);",60,0.6572769953051644,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/RestrictedConfigChecker.java
"//  FIXME: hiveServer2SiteUrl is not settable?","ctx.hiveConf = new HiveConf(IDriver.class);",150,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/test/org/apache/hive/testutils/HiveTestEnvSetup.java
"//  The following check is only a guard against failures.   TODO: Knowing which expr is constant in GBY's aggregation function   arguments could be better done using Metadata provider of Calcite.  check the corresponding expression in exprs to see if it is literal","too long.",0,0,0
"//  TODO: Remove in Hive 0.16.   This is required only to support the deprecated HCatAddPartitionDesc.Builder interfaces.","// This is only required to support the deprecated methods in HCatAddPartitionDesc.Builder.",489,0.3779527559055118,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/hcatalog/webhcat/java-client/src/main/java/org/apache/hive/hcatalog/api/HCatClientHMSImpl.java
"// TODO: if partitions are loaded lazily via the iterator then we will have to avoid conversion of everything here as it defeats the purpose.","for (Partition partition : metadata.getPartitions()) {",83,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/bootstrap/events/filesystem/FSTableEvent.java
"/*      * Figures out the aliases for whom it is safe to push predicates based on     * ANSI SQL semantics. The join conditions are left associative so "a     * RIGHT OUTER JOIN b LEFT OUTER JOIN c INNER JOIN d" is interpreted as     * "((a RIGHT OUTER JOIN b) LEFT OUTER JOIN c) INNER JOIN d".  For inner     * joins, both the left and right join subexpressions are considered for     * pushing down aliases, for the right outer join, the right subexpression     * is considered and the left ignored and for the left outer join, the     * left subexpression is considered and the left ignored. Here, aliases b     * and d are eligible to be pushed up.     *     * TODO: further optimization opportunity for the case a.c1 = b.c1 and b.c2     * = c.c2 a and b are first joined and then the result with c. But the     * second join op currently treats a and b as separate aliases and thus     * disallowing predicate expr containing both tables a and b (such as a.c3     * + a.c4 > 20). Such predicates also can be pushed just above the second     * join and below the first join     *     * @param op     *          Join Operator     * @param rr     *          Row resolver     * @return set of qualified aliases      */","too long.",0,0,0
"//  TODO: not stopping umbilical explicitly as some taskKill requests may get scheduled during queryComplete   which will be using the umbilical. HIVE-16021 should fix this, until then leave umbilical open and wait for   it to be closed after max idle timeout (10s default)","too long.",0,0,0
"//  TODO [MM gap?]: by design; no-one seems to use LB tables. They will work, but not convert.                   It's possible to work around this by re-creating and re-inserting the table.","throw new HiveException("Converting list bucketed tables stored as subdirectories "",4413,0.7004608294930875,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
"//  TODO: Should be moved out.","if (oldtbl",339,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/hcatalog/webhcat/java-client/src/main/java/org/apache/hive/hcatalog/api/HCatClientHMSImpl.java
"//  TODO: I/O threadpool could be here - one thread per stripe; for now, linear.","boolean hasFileId = this.fileKey != null;",348,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java
"//  Optionally, do some filtering of rows...   UNDONE","// From the value arrays and our isRepeated, selected, isNull arrays, generate the batch!",284,0.925,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/util/batchgen/VectorBatchGenerator.java
"//  TODO: Should have a check on the server side. Embedded metastore throws   InvalidObjectException, remote throws TApplicationException","// NullPointerException, remote throws TTransportException",218,0.6888888888888889,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestFunctions.java
"//  FIXME: Support pruning dynamic partitioning.","LOG.info("DP can be rewritten to SP!");",1285,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java
"//  FIXME: sideeffect will leave the last query set at the session level","if (SessionState.get() != null) {",557,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
"//  TODO: use faster non-sync inputstream","try {",208,0.6194690265486725,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/standalone-metastore/src/main/java/org/apache/hadoop/hive/common/ndv/hll/HyperLogLogUtils.java
"//  TODO: should this rather use a threadlocal for NUMA affinity?","private final FixedSizedObjectPool<IoTrace> tracePool;",63,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcColumnVectorProducer.java
"// TODO: this object is created once to call one method and then immediately destroyed.  So it's basically just a roundabout way to pass arguments to a static method. Simplify?","public class TableExport {",54,0.6666666666666666,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/TableExport.java
"//  TODO:pc implement max","@Override",2843,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java
"//  TODO Make sure this method is eventually used to find the prep / batch scripts.","public String getApplyPathScriptPath() {",230,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/testutils/ptest2/src/main/java/org/apache/hive/ptest/execution/conf/TestConfiguration.java
"/*      * This doesn't throw any exceptions because we don't want the Compaction to appear as failed     * if stats gathering fails since this prevents Cleaner from doing it's job and if there are     * multiple failures, auto initiated compactions will stop which leads to problems that are     * much worse than stale stats.     *     * todo: longer term we should write something COMPACTION_QUEUE.CQ_META_INFO.  This is a binary     * field so need to figure out the msg format and how to surface it in SHOW COMPACTIONS, etc      */","too long.",0,0,0
"//  TODO: transitive dependencies warning?","String[] jarPaths = auxJars.split(delimiter);",477,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/cli/LlapServiceDriver.java
"//  We are in HS2, get the token locally.   TODO: coordinator should be passed in; HIVE-13698. Must be initialized for now.","coordinator = LlapCoordinator.getInstance();",412,0.48484848484848486,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java
"//  TODO: Ideally, AcidUtils class and various constants should be in common.","private static final String BASE_PREFIX = "base_", DELTA_PREFIX = "delta_",",306,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-common/src/java/org/apache/hadoop/hive/llap/LlapUtil.java
"//  HACK: We actually need BlockMissingException, but that is not available","// in all hadoop versions.",354,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/io/RCFile.java
"//  TODO HIVE-14042. Handling of dummyOps, and propagating abort information to them","dummyOp.initialize(jconf, null);",334,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java
"//  TODO This - at least for the session pool - will always be the hive user. How does doAs above this affect things ?","UserGroupInformation ugi = Utils.getUGI();",274,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java
"//  TODO: this is currently broken. We need to set memory manager to a bogus implementation         to avoid problems with memory manager actually tracking the usage.","return OrcFile.createWriter(path, createOrcWriterOptions(oi, conf, cacheWriter, allocSize));",1512,0.7246376811594203,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/SerDeEncodedDataReader.java
"//  TODO: for one-block case, we could move notification for the last block out of the loop.","long evicted = evictor.evictSomeBlocks(remainingToReserve);",79,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheMemoryManager.java
"//  TODO: figure out a better data structure for node list(?)","private List<AggrColStats> nodes = new ArrayList<>();",507,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/AggregateStatsCache.java
"// why isn't PPD working.... - it is working but storage layer doesn't do row level filtering; only row group level","//this uses VectorizedOrcAcidRowBatchReader",631,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/test/org/apache/hadoop/hive/ql/TestTxnNoBuckets.java
"//  Note : Currently, this implementation does not "fall back" to regular copy if distcp   is tried and it fails. We depend upon that behaviour in cases like replication,   wherein if distcp fails, there is good reason to not plod along with a trivial   implementation, and fail instead.","too long.",0,0,0
"//  TODO: this is never used","localDirs = conf.getTrimmedStrings(SHUFFLE_HANDLER_LOCAL_DIRS);",277,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/ShuffleHandler.java
"//  TODO: versions could also be picked at build time.","List<String> versionFiles = QTestUtil.getVersionFiles(queryDirectory, tname);",120,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/itests/util/src/main/java/org/apache/hadoop/hive/cli/control/CoreCompareCliDriver.java
"//  A previous solution is based on tableAlias and colAlias, which is   unsafe, esp. when CBO generates derived table names. see HIVE-13602.   For correctness purpose, we only trust colExpMap.   We assume that CBO can do the constantPropagation before this function is   called to help improve the performance.   UnionOperator, LimitOperator and FilterOperator are special, they should already be   column-position aligned.","too long.",0,0,0
"//  Hack!! - refactor once the metadata APIs with types are ready","// Finally add the partitioning columns",10990,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
"//  TODO: if this cannot evict enough, it will spin infinitely. Terminate at some point?","int badCallCount = 0;",64,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheMemoryManager.java
"//  but it is not OK to convert if the join is on (a,c)","return sortColNames.subList(0, joinCols.size()).containsAll(joinCols);",373,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/optimizer/AbstractSMBJoinProc.java
"//  TODO: ideally we should have a test for session itself.","verify(sessionState).ensureLocalResources(any(Configuration.class), eq(inputOutputJars));",246,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestTezTask.java
"//  TODO : if needed, verify that recordschema entry for fieldname matches appropriate type.","return get(fieldName, recordSchema);",54,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/HCatRecord.java
"//  FIXME: null value is treated differently on the other end..when those filter will be","// moved...this may change",68,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/itests/util/src/main/java/org/apache/hadoop/hive/cli/control/AbstractCliConfig.java
"/*  * TODO:<br> * 1. Change the output col/ExprNodeColumn names to external names.<br> * 2. Verify if we need to use the "KEY."/"VALUE." in RS cols; switch to * external names if possible.<br> * 3. In ExprNode & in ColumnInfo the tableAlias/VirtualColumn is specified * differently for different GB/RS in pipeline. Remove the different treatments. * 4. VirtualColMap needs to be maintained *  */","too long.",0,0,0
"//  Replace the entire current DiskRange with new cached range.   In case of an inexact match in either of the below it may throw. We do not currently   support the case where the caller requests a single cache buffer via multiple smaller   sub-ranges; if that happens, this may throw. Noone does it now, though.   TODO: should we actively assert here for cache buffers larger than range?","too long.",0,0,0
"//  TODO: Fill in when PARTITION_DONE_EVENT is supported.","Assert.assertTrue("Unexpected: HCAT_PARTITION_DONE_EVENT not supported (yet).", false);",256,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/hcatalog/server-extensions/src/test/java/org/apache/hive/hcatalog/listener/TestNotificationListener.java
"//  TODO For now, this affects non broadcast unsorted cases as well. Make use of the edge   property when it's available.","for (IOSpecProto inputSpec : vertex.getInputSpecsList()) {",424,0.8470588235294118,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java
"//  TODO: most other options are probably unrecoverable... throw?","caughtException = tae;",230,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/RetryingMetaStoreClient.java
"//  this is a temporary hack to fix things that are not fixed in the compiler","Integer numReducersFromWork = rWork == null ? 0 : rWork.getNumReduceTasks();",408,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapRedTask.java
"//  Should be fixed in Accumulo 1.5.2 and 1.6.1","if (null == rangeSplit.getIterators()",221,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/mr/HiveAccumuloTableInputFormat.java
"//  This is hackery, but having hive-common depend on standalone-metastore is really bad   because it will pull all of the metastore code into every module.  We need to check that   we aren't using the standalone metastore.  If we are, we should treat it the same as a","too long.",0,0,0
"// todo: fix this - it has to run in 3.0 since tables may be unbucketed","pw.println("-- These commands may be executed by Hive 1.x later");",475,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/upgrade-acid/src/main/java/org/apache/hadoop/hive/upgrade/acid/UpgradeTool.java
"//  Not sure why this method doesn't throw any exceptions,   but since the interface doesn't allow it we'll just swallow them and   move on.  This OK-ish since releaseLocks() is only called for RO/AC queries; it  would be really bad to eat exceptions here for write operations","too long.",0,0,0
"//  TODO: Implement this when tez is upgraded. TEZ-3550","private static ExprNodeDesc propConstDistUDAFParams() {",1281,0.6060606060606061,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/HiveGBOpConvUtil.java
"//  TODO: there should be a better way to do this, code just needs to be modified","OrcProto.ColumnEncoding columnEncoding = encodings.get(columnIndex);",2306,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java
"//  TODO Session re-use completely disabled for doAs=true. Always launches a new session.","boolean nonDefaultUser = conf.getBoolVar(HiveConf.ConfVars.HIVE_SERVER2_ENABLE_DOAS);",275,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java
"//  TODO In case of a failure to heartbeat, tasks for the specific DAG should ideally be KILLED","/*",79,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java
"//  TODO Check if all required tables are allowed, if so, get it from cache","if (!isBlacklistWhitelistEmpty(conf) || !isCachePrewarmed.get()) {",1102,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java
"//  TODO: Only the qualified name should be left here","return super.explainTerms(pw)",131,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/reloperators/HiveTableScan.java
"//  FIXME: this is a secret contract; reusein getAggrKey() creates a more closer relation to the StatsGatherer","// prefix = work.getAggKey();",199,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/stats/BasicStatsTask.java
"//  Note: it's not quite clear why this is done inside this if. Seems like it should be on the top level.","if (context.linkChildOpWithDummyOp.containsKey(mj)) {",272,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java
"//  TODO: why is this synchronized?","public synchronized String getDelegationTokenFromMetaStore(String owner)",567,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/service/src/java/org/apache/hive/service/cli/CLIService.java
"//  TODO: local cache is created once, so the configs for future queries will not be honored.","if (cacheStripeDetails) {",675,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java
"//  Support for dynamic partitions can be added later   The following is not optimized:   insert overwrite table T1(ds='1', hr) select key, value, hr from T2 where ds = '1';   where T1 and T2 are bucketed by the same keys and partitioned by ds. hr","if (fsOp.getConf().getDynPartCtx() != null) {",422,0.3651452282157676,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/optimizer/BucketingSortingReduceSinkOptimizer.java
"//  FIXME: using real scaling by new/old ration might yield better results?","ret.numRows = newRowCount;",324,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/plan/Statistics.java
"// todo: try this with acid default - it seem making table acid in listener is too late","runStatementOnDriver("create table myctas2 stored as ORC TBLPROPERTIES ('transactional" +",205,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/test/org/apache/hadoop/hive/ql/TestTxnNoBuckets.java
"/*  * Context class for operator tree walker for partition pruner. * TODO: this class may be not useful.  */","public class OpWalkerCtx implements NodeProcessorCtx {",28,0.7571428571428571,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/OpWalkerCtx.java
"//  Thrift cannot write read-only buffers... oh well.   TODO: actually thrift never writes to the buffer, so we could use reflection to         unset the unnecessary read-only flag if allocation/copy perf becomes a problem.","ByteBuffer copy = ByteBuffer.allocate(bb.capacity());",7621,0.4,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
"//  TODO should be replaced by CliServiceClient","private TCLIService.Iface client;",134,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java
"//  @deprecated in favour of {@link #Builder(HCatTable, boolean)}. To be removed in Hive 0.16.","private Builder(String dbName, String tableName, List<HCatFieldSchema> columns) {",243,0.935672514619883,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/hcatalog/webhcat/java-client/src/main/java/org/apache/hive/hcatalog/api/HCatCreateTableDesc.java
"//  TODO: Change ExprNodeConverter to be independent of Partition Expr","ExprNodeConverter exprConv = new ExprNodeConverter(inputTabAlias, inputRel.getRowType(),",928,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/HiveCalciteUtil.java
"//  We are not going to verify SD for each partition. Just verify for the table.   ToDo: we need verify the partition column instead","validateTableCols(table, colNames);",8583,0.7529411764705882,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java
"// todo: ConditionalTask#addDependentTask(Task) doesn't do the right thing: HIVE-18978","t.addDependentTask(alterTable);",309,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/parse/UpdateDeleteSemanticAnalyzer.java
"// TODO: Can columns retain virtualness out of union","// 3. Return result",608,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/HiveOpConverter.java
"//  TODO: why do we invent our own error path op top of the one from Future.get?","setOperationException(e);",318,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java
"//  TODO is there a more correct way to get the literal value for the Object?","return new Text(objInspector.getWritableConstantValue().toString());",360,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/predicate/AccumuloRangeGenerator.java
"/*  * todo: This need review re: thread safety.  Various places (see callsers of * {@link SessionState#setCurrentSessionState(SessionState)}) pass SessionState to forked threads. * Currently it looks like those threads only read metadata but this is fragile. * Also, maps (in SessionState) where tempt table metadata is stored are concurrent and so * any put/get crosses a memory barrier and so does using most {@code java.util.concurrent.*} * so the readers of the objects in these maps should have the most recent view of the object. * But again, could be fragile.  */","too long.",0,0,0
"//  arguments then we can use a more efficient form.","final RelMetadataQuery mq = call.getMetadataQuery();",164,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveExpandDistinctAggregatesRule.java
"//  TODO: Something is preventing the process from terminating after main(), adding exit() as hacky solution.","System.exit(rc);",139,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/util/HiveStrictManagedMigration.java
"//  TODO: write error to the channel? there's no mechanism for that now.","ctx.close();",228,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/llap/LlapOutputFormatService.java
"//  TODO: reuse columnvector-s on hasBatch - save the array by column? take apart each list.","ColumnStreamData[] datas = ecb.getColumnData(colIx);",1695,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/SerDeEncodedDataReader.java
"//  todo: hold onto this predicate, so that we don't add it to the Filter Operator.","break;",3167,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
"//  TODO Ideally, remove elements from this once it's known that no tasks are linked to the instance (all deallocated)","// Tracks tasks which could not be allocated immediately.",170,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java
"//  TODO: can we blindly copy sort trait? What if inputs changed and we   are now sorting by different cols","RelCollation canonizedCollation = traitSet.canonize(newCollation);",75,0.8028169014084507,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/reloperators/HiveSortLimit.java
"//  TODO: This works different in remote and embedded mode.   In embedded mode, no exception happens.","public void testAddPartitionEmptyValue() throws Exception {",575,0.7424242424242424,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestAddPartitions.java
"//  UNDONE: Need to copy the object.","resultObjects[rowIndex++] = scrqtchRow[0];",454,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorCastStatement.java
"//  did remove those and gave CBO the proper AST. That is kinda hacky.","ASTNode queryForCbo = ast;",418,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
"//  FIXME: consider other operator info as well..not just conf?","SignatureUtils.write(sigMap, op.getConf());",52,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/optimizer/signature/OpSignature.java
"//  check # of dp   TODO: add an option to skip this if number of partitions checks is done by Triggers via   CREATED_DYNAMIC_PARTITION counter","if (valToPaths.size() > maxPartitions) {",1182,0.1889763779527559,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
"/*  vertex is started, but not complete  */","if (failed > 0) {",109,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/monitoring/RenderStrategy.java
"//  TODO: need the description of how these maps are kept consistent.","@VisibleForTesting",881,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java
"//  UNDONE: Need to copy the object?","resultObjects[rowIndex++] = scrqtchRow[0];",401,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorDateDiff.java
"//  TODO filter->expr   TODO functionCache   TODO constraintCache   TODO need sd nested copy?   TODO String intern   TODO monitor event queue   TODO initial load slow?   TODO size estimation","public class CachedStore implements RawStore, Configurable {",131,0.21428571428571427,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java
"//  TODO not 100% sure about this.  This call doesn't set the compression type in the conf   file the way getHiveRecordWriter does, as ORC appears to read the value for itself.  Not   sure if this is correct or not.","return getRecordUpdater(jc, acidOutputFormat,",333,0.5950413223140496,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/io/HiveFileFormatUtils.java
"/*    * TODO : Refactor   *   * There is an upcoming patch that refactors this bit of code. Currently, the idea is the following:   *   * By default, ReplCopyWork will behave similarly to CopyWork, and simply copy   * along data from the source to destination.   * If the flag readSrcAsFilesList is set, changes the source behaviour of this CopyTask, and   * instead of copying explicit files, this will then fall back to a behaviour wherein an _files is   * read from the source, and the files specified by the _files are then copied to the destination.   *   * This allows us a lazy-copy-on-source and a pull-from destination semantic that we want   * to use from replication.    */","too long.",0,0,0
"/*  A scratch variable is created here. This could be optimized in the future     * by perhaps using thread-local storage to allocate this scratch field.      */","UnsignedInt128 scratch = new UnsignedInt128();",1907,0.6666666666666666,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/common/src/java/org/apache/hadoop/hive/common/type/Decimal128.java
"//  Ideally there should be a better way to determine that the followingWork contains   a dynamic partitioned hash join, but in some cases (createReduceWork()) it looks like   the work must be created/connected first, before the GenTezProcContext can be updated   with the mapjoin/work relationship.","too long.",0,0,0
"// Future thought: this may be expensive so consider having a thread pool run in parallel","clean(compactId2CompactInfoMap.get(queueEntry.getKey()));",168,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java
"/*      * Number of rows processed between checks for minReductionHashAggr factor     * TODO: there is overlap between numRowsCompareHashAggr and checkInterval      */","private long numRowsCompareHashAggr;",347,0.649746192893401,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java
"//  TODO: LlapNodeId is just a host+port pair; we could make this class more generic.","import org.apache.hadoop.io.retry.RetryPolicies;",41,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-common/src/java/org/apache/hadoop/hive/llap/AsyncPbRpcProxy.java
"//  Note: we assume here that plan has been validated beforehand, so we don't verify","//       that fractions or query parallelism add up, etc.",979,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/WorkloadManager.java
"//  Implement in future, if needed.","}",152,0.5918367346938775,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/MultiValuedColumnVector.java
"//  TODO: move this into ctor? EW would need to create CacheWriter then","List<Integer> cwColIds = writer.isOnlyWritingIncludedColumns() ? splitColumnIds : columnIds;",1417,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/SerDeEncodedDataReader.java
"//  After SPARK-2321, we only use JobMetricsListener to get job metrics   TODO: remove it when the new API provides equivalent functionality","private JobMetricsListener jobMetricsListener;",51,0.6779661016949152,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/impl/LocalSparkJobStatus.java
"//  TODO: could we do this only if the OF is actually used?","String attemptId = Converters.createTaskAttemptId(context).toString();",111,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java
"//  TODO: we may add app name, etc. later","public MappingInput(String userName, List<String> groups, String wmPool, String appName) {",60,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/UserPoolMapping.java
"//  TODO no fk across catalogs","}",464,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestPrimaryKey.java
"//  TODO: This does not work because materialized views need the creation metadata   to be updated in case tables used were replicated to a different database.  run("CREATE MATERIALIZED VIEW " + dbName + ".mat_view AS SELECT a FROM " + dbName + ".ptned where b=1", driver);  verifySetup("SELECT a from " + dbName + ".mat_view", ptn_data_1, driver);","too long.",0,0,0
"//  FIXME: oss seems to contain duplicates","if (fos.size() > 0 && oss.size() > 0) {",120,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/test/org/apache/hadoop/hive/ql/plan/mapping/TestReOptimization.java
"//  TODO: danger of stack overflow... needs a retry limit?","return;",318,0.7741935483870968,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java
"//  Cannot drop db1 because mv1 uses one of its tables   TODO: Error message coming from metastore is currently not very concise   (foreign key violation), we should make it easily understandable","client.dropDatabase(dbName1, true, true, true);",2871,0.4236453201970443,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java
"//  TODO: ideally, QueryTracker should have fragment-to-query mapping.","QueryIdentifier queryId = executorService.findQueryByFragment(fragmentId);",424,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/ContainerRunnerImpl.java
"//  TODO: WTF? The old code seems to just drop the ball here.","throw new AssertionError("Unsupported mode");",358,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java
"//  Hack to initialize cache with 0 expiry time causing it to return a new hive client every time   Otherwise the cache doesn't play well with the second test method with the client gets closed() in the   tearDown() of the previous test","HCatUtil.getHiveMetastoreClient(hiveConf);",146,0.582089552238806,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/HCatMapReduceTest.java
"//  @deprecated in favour of {@link HCatPartition.#getLocation()}. To be removed in Hive 0.16.","public String getLocation() {",71,0.9364161849710982,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/hcatalog/webhcat/java-client/src/main/java/org/apache/hive/hcatalog/api/HCatAddPartitionDesc.java
"//  TODO HIVE-12449. Make use of progress notifications once Hive starts sending them out.   progressNotified = task.getAndClearProgressNotification();","if (sendCounters) {",409,0.7307692307692307,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapTaskReporter.java
"//  TODO Evil!  Need to figure out a way to remove this sleep.","Assert.assertEquals(initialCount + 1,",148,0.8421052631578947,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreMetrics.java
"//  TODO: all the extrapolation logic should be moved out of this class,","// only mechanical data retrieval should remain here.",1589,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java
"//  FIXME: this add seems suspicious...10 lines below the value returned by this method used as betterDS","stats.addToDataSize(getDataSizeFromColumnStats(nr, columnStats));",487,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java
"//  this is a hack for now to handle the group by case","if (children.contains(null)) {",1456,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java
"//  All other distinct keys will just be forwarded. This could be optimized...","for (int i = 1; i < numDistinctExprs; i++) {",376,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java
"//  This is a workaround for DERBY-6358 and Oracle bug; it is pretty horrible.","tableValue += (" and " + TBLS + ".\"TBL_NAME\" = ? and " + DBS + ".\"NAME\" = ? and "",1321,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java
"// @TODO This is fetching all the rows at once from broker or multiple historical nodes   Move to use scan query to avoid GC back pressure on the nodes","// https://issues.apache.org/jira/browse/HIVE-17627",193,0.7461139896373057,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/druid-handler/src/java/org/apache/hadoop/hive/druid/io/DruidQueryBasedInputFormat.java
"// todo: strictly speaking there is a bug here.  heartbeat*() commits but both heartbeat and  checkLock() are in the same retry block, so if checkLock() throws, heartbeat is also retired","//extra heartbeat is logically harmless, but ...",2146,0.6637931034482759,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java
"//  @deprecated in favour of {@link HCatTable.#partCols(List<FieldSchema>)}. To be removed in Hive 0.16.","public Builder partCols(List<HCatFieldSchema> partCols) {",272,0.9430051813471503,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/hcatalog/webhcat/java-client/src/main/java/org/apache/hive/hcatalog/api/HCatCreateTableDesc.java
"//  Wow, something's really wrong.","throw new IOException("Couldn't write to node " + id, nfe);",157,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/ZooKeeperStorage.java
"//  UNDONE: Does this random range need to go as high as 38?","int formatScale = 0 + r.nextInt(38);",2559,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/storage-api/src/test/org/apache/hadoop/hive/common/type/TestHiveDecimal.java
"//  TODO : Java 1.7+ support using String with switches, but IDEs don't all seem to know that.   If casing is fine for now. But we should eventually remove this. Also, I didn't want to   create another enum just for this.","String eventType = event.getEventType();",42,0.6031746031746031,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/hcatalog/webhcat/java-client/src/test/java/org/apache/hive/hcatalog/api/repl/TestReplicationTask.java
"//  We have found an invalid decimal value while enforcing precision and   scale. Ideally,   we would replace it with null here, which is what Hive does. However,   we need to plumb   this thru up somehow, because otherwise having different expression   type in AST causes   the plan generation to fail after CBO, probably due to some residual   state in SA/QB.   For now, we will not run CBO in the presence of invalid decimal","too long.",0,0,0
"//  Code initially inspired by Google ObjectExplorer.   TODO: roll in the direct-only estimators from fields. Various other optimizations possible.","Deque<Object> stack = createWorkStack(rootObj, byType);",64,0.5348837209302325,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/storage-api/src/java/org/apache/hadoop/hive/ql/util/IncrementalObjectSizeEstimator.java
"//  Dirty hack as this will throw away spaces and other things - find a better","// way!",776,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
"//  TODO: should local cache also be by fileId? Preserve the original logic for now.","assert result.length == files.size();",94,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/io/orc/LocalCache.java
"//  TODO: do we need to get to child?","int wndSpecASTIndx = getWindowSpecIndx(windowProjAst);",4116,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
"//  TODO: refactor with cache impl? it has the same merge logic","if (cacheEncodings == null) {",1200,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/SerDeEncodedDataReader.java
"/*    * Dirty hack to set the environment variables using reflection code. This method is for testing   * purposes only and should not be used elsewhere    */","private final static void setEnv(Map<String, String> newenv) throws Exception {",118,0.7843137254901961,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/test/org/apache/hadoop/hive/ql/processors/TestSetProcessor.java
"//  TODO: Decorelation of subquery should be done before attempting   Partition Pruning; otherwise Expression evaluation may try to execute   corelated sub query.","PerfLogger perfLogger = SessionState.getPerfLogger();",1912,0.5906735751295337,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
"/*    * TODO: 1) isSamplingPred 2) sampleDesc 3) isSortedFilter    */","OpAttr visit(HiveFilter filterRel) throws SemanticException {",533,0.9615384615384616,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/HiveOpConverter.java
"// @TODO it will be nice to refactor it","RelDataTypeFactory dtFactory = cluster.getRexBuilder().getTypeFactory();",361,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMaterializedViewsRegistry.java
"// todo: this should not throw  todo: this should take "comment" as parameter to set in CC_META_INFO to provide some context for the failure","None",None,None,None
"//  TODO: replace this with a Map?","for (OperationType opType : values()) {",46,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/service/src/java/org/apache/hive/service/cli/OperationType.java
"//  TODO: remove some of these fields as needed?","/** Linked list pointers for LRFU/LRU cache policies. Given that each block is in cache",34,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LlapCacheableBuffer.java
"//  TODO: this is not valid. Function names for built-in UDFs are specified in   FunctionRegistry, and only happen to match annotations. For user UDFs, the   name is what user specifies at creation time (annotation can be absent,","// different, or duplicate some other function).",306,0.5079365079365079,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/SqlFunctionConverter.java
"//  TODO: verify that this is correct","return "";",381,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/jdbc/src/java/org/apache/hive/jdbc/HiveDatabaseMetaData.java
"//  TODO: we could try to get the declaring object and infer argument... stupid Java.","LOG.trace("Cannot determine map type: {}", field);",257,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/storage-api/src/java/org/apache/hadoop/hive/ql/util/IncrementalObjectSizeEstimator.java
"//  close&destroy is used in seq coupling most of the time - the difference is either not clear; or not relevant - remove?","@Override",67,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/IDriver.java
"// todo: this doesn;t check if compaction is already running (even though Initiator does but we","// don't go  through Initiator for user initiated compactions)",77,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java
"//  This is a little bit weird. We'll do the MS call outside of the lock. Our caller calls us   under lock, so we'd preserve the lock state for them; their finally block will release the","// lock correctly. See the comment on the lock field - the locking needs to be reworked.",643,0.6606334841628959,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/Registry.java
"// todo: we really need some comments to explain exactly why each of these is removed","removeEnv.add("HADOOP_ROOT_LOGGER");",163,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/LaunchMapper.java
"//  ### FIXME: this is broken for multi-line SQL","success = sql(line.substring("all ".length())) && success;",1705,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/beeline/src/java/org/apache/hive/beeline/Commands.java
"// Not implemented","}",225,0.9696969696969697,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/common/src/java/org/apache/hadoop/hive/common/metrics/LegacyMetrics.java
"//  Slice boundaries may not match split boundaries due to torn rows in either direction,   so this counter may not be consistent with splits. This is also why we increment   requested bytes here, instead of based on the split - we don't want the metrics to be   inconsistent with each other. No matter what we determine here, at least we'll account   for both in the same manner.","too long.",0,0,0
"//  This is kinda hacky - we "know" these are LlaSerDeDataBuffer-s.","newCacheDataForCol[streamIx] = stream.data.toArray(new LlapSerDeDataBuffer[stream.data.size()]);",1161,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/SerDeEncodedDataReader.java
"//  TODO: perhaps can be made more efficient by creating a byte[] directly","private static byte[] join(String[] items, char separator) {",237,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/DelimitedInputWriter.java
"// this is not strictly accurate, but 'type' cannot be null.","if(ci.type == null) { ci.type = CompactionType.MINOR; }",1037,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java
"//  TODO: for non columnar we don't need to do this... might as well update all stats.","if (isAllParts) {",343,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUpdaterThread.java
"//  TODO: this should ideally not create AddPartitionDesc per partition","AddPartitionDesc partsDesc = getBaseAddPartitionDescFromPartition(fromPath, dbname, tblDesc, partition);",276,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java
"//  TODO: should we also whitelist input formats here? from mapred.input.format.class","Path scratchDir = utils.createTezDir(ctx.getMRScratchDir(), job);",391,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFGetSplits.java
"//  Not public since we must have the deserialize read object.","private VectorDeserializeRow() {",105,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java
"//  TODO: checking 2 children is useless, compare already does that.","} else if (isCompare && (func.getChildren().size() == 2)) {",266,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/RexNodeConverter.java
"//  TODO: why does the original code not just use _dataStream that it passes in as stream?","InStream stream = ((StringDirectTreeReader) reader).getStream();",279,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java
"//  This is a bogus hack because it copies the contents of the SQL file   intended for creating derby databases, and thus will inexorably get   out of date with it.  I'm open to any suggestions on how to make this   read the file in a build friendly way.","Connection conn = null;",66,0.4392156862745098,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnDbUtil.java
"//  This if/else chain looks ugly in the inner loop, but given that it will be 100% the same   for a given operator branch prediction should work quite nicely on it.   RecordUpdateer expects to get the actual row, not a serialized version of it.  Thus we","too long.",0,0,0
"// TODO this has to find a better home, it's also hardcoded as default in hive would be nice","// if the default was decided by the serde",143,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/InternalUtil.java
"//  @deprecated in favour of {@link HCatTable.#sortCols(ArrayList<Order>)}. To be removed in Hive 0.16.","public Builder sortCols(ArrayList<Order> sortCols) {",346,0.9424083769633508,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/hcatalog/webhcat/java-client/src/main/java/org/apache/hive/hcatalog/api/HCatCreateTableDesc.java
"//  TODO? add upstream?","sendError(ctx, FORBIDDEN);",754,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/ShuffleHandler.java
"//  TODO: Should have a check on the server side. Embedded metastore throws   NullPointerException, remote throws MetaException","// NullPointerException, remote throws TTransportException",218,0.7294117647058823,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestFunctions.java
"//  load the list of DP partitions and return the list of partition specs   TODO: In a follow-up to HIVE-1361, we should refactor loadDynamicPartitions   to use Utilities.getFullDPSpecs() to get the list of full partSpecs.   After that check the number of DPs created to not exceed the limit and   iterate over it and call loadPartition() here.   The reason we don't do inside HIVE-1361 is the latter is large and we","too long.",0,0,0
"//  There are 3 options for this ConditionalTask:   1) Merge the partitions   2) Move the partitions (i.e. don't merge the partitions)   3) Merge some partitions and move other partitions (i.e. merge some partitions and don't   merge others) in this case the merge is done first followed by the move to prevent   conflicts.   TODO: if we are not dealing with concatenate DDL, we should not create a merge+move path","too long.",0,0,0
"/*    * TODO: this would be more flexible doing a SQL select statement rather than using InputFormat directly   * see {@link org.apache.hive.hcatalog.streaming.TestStreaming#checkDataWritten2(Path, long, long, int, String, String...)}   * @param numSplitsExpected   * @return   * @throws Exception    */","too long.",0,0,0
"//  @deprecated in favour of {@link HCatTable.#getDbName()}. To be removed in Hive 0.16.","public String getDatabaseName() {",219,0.9316770186335404,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/hcatalog/webhcat/java-client/src/main/java/org/apache/hive/hcatalog/api/HCatCreateTableDesc.java
"//  TODO HIVE-16134. Differentiate between EXTERNAL_PREEMPTION_WAITQUEU vs EXTERNAL_PREEMPTION_FINISHABLE?","if (endReason != null && EnumSet",1212,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java
"//  TODO: pointless","threadPool.submit(new DateTestCallable(bad, timeZone)).get();",228,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/serde/src/test/org/apache/hadoop/hive/serde2/io/TestDateWritableV2.java
"//  This is kind of not pretty, but this is how we detect whether buffer was cached.   We would always set this for lookups at put time.","if (buffer.declaredCachedLength != LlapDataBuffer.UNKNOWN_CACHED_LENGTH) {",377,0.7701149425287356,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java
"//  TODO: move these test parameters to more specific places... there's no need to have them here","void init(AtomicBoolean stop, AtomicBoolean looped) throws MetaException;",49,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/MetaStoreThread.java
"//  TODO: fs checksum only available on hdfs, need to         find a solution for other fs (eg, local fs, s3, etc)","String checksumString = null;",274,0.671875,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/ReplChangeManager.java
"//  FIXME: possible alternative: move both OpSignature/OpTreeSignature into   under some class as nested ones; and that way this factory level caching can be made "transparent"","class Direct implements OpTreeSignatureFactory {",44,0.6203703703703703,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/optimizer/signature/OpTreeSignatureFactory.java
"//  Neither "expired" nor "olderThan" criteria selected. This better not be an attempt to delete tokens.","if (opType == OpType.DELETE) {",148,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/security/DelegationTokenTool.java
"//  FIXME: Hadoop3 made the incompatible change for dfs.client.datanode-restart.timeout   while spark2 is still using Hadoop2.   Spark requires Hive to support Hadoop3 first then Spark can start   working on Hadoop3 support. Remove this after Spark supports Hadoop3.","too long.",0,0,0
"/*    * Variables used by LLAP daemons.   * TODO: Eventually auto-populate this based on prefixes. The conf variables   * will need to be renamed for this.    */","private static final Set<String> llapDaemonVarsSet;",340,0.3660130718954248,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
"//  TODO: setFileMetadata could just create schema. Called in two places; clean up later.","this.evolution = sef.createSchemaEvolution(fileMetadata.getSchema());",232,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java
"//  @deprecated in favour of {@link HCatTable.#comment()}. To be removed in Hive 0.16.","public String getComments() {",147,0.9299363057324841,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/hcatalog/webhcat/java-client/src/main/java/org/apache/hive/hcatalog/api/HCatCreateTableDesc.java
"//  this is workaround for hadoop-17 - libjars are not added to classpath of the","// child process. so we add it here explicitly",749,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java
"/* todo: we need some sort of validation phase over original AST to make things user friendly; for example, if     original command refers to a column that doesn't exist, this will be caught when processing the rewritten query but     the errors will point at locations that the user can't map to anything     - VALUES clause must have the same number of values as target table (including partition cols).  Part cols go last in Select clause of Insert as Select     todo: do we care to preserve comments in original SQL?     todo: check if identifiers are propertly escaped/quoted in the generated SQL - it's currently inconsistent      Look at UnparseTranslator.addIdentifierTranslation() - it does unescape + unparse...     todo: consider "WHEN NOT MATCHED BY SOURCE THEN UPDATE SET TargetTable.Col1 = SourceTable.Col1 "; what happens when source is empty?  This should be a runtime error - maybe not      the outer side of ROJ is empty => the join produces 0 rows.  If supporting WHEN NOT MATCHED BY SOURCE, then this should be a runtime error     */","too long.",0,0,0
"//  TODO: wtf? This doesn't do anything.","HashPartition hashPartition = new HashPartition(1024, (float) 0.75, 524288, 1, true, null);",27,0.5,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/test/org/apache/hadoop/hive/ql/exec/persistence/TestHashPartition.java
"//  TODO: why is this needed?","File qf = new File(outDir, fileName);",1304,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java
"/*  * NOTE: this whole logic is replicated from Calcite's RelDecorrelator *  and is exteneded to make it suitable for HIVE *    We should get rid of this and replace it with Calcite's RelDecorrelator *    once that works with Join, Project etc instead of Join, Project. *    At this point this has differed from Calcite's version significantly so cannot *    get rid of this. * * RelDecorrelator replaces all correlated expressions (corExp) in a relational * expression (RelNode) tree with non-correlated expressions that are produced * from joining the RelNode that produces the corExp with the RelNode that * references it. * * <p>TODO:</p> * <ul> *   <li>replace {@code CorelMap} constructor parameter with a RelNode *   <li>make {@link #currentRel} immutable (would require a fresh *      RelDecorrelator for each node being decorrelated)</li> *   <li>make fields of {@code CorelMap} immutable</li> *   <li>make sub-class rules static, and have them create their own *   de-correlator</li> * </ul>  */","too long.",0,0,0
"//  @deprecated in favour of {@link HCatTable.#serdeParam(Map<String, String>)}.   To be removed in Hive 0.16.","None",None,None,None
"//  CONCERN: Leaking scratch column?","VectorExpression[] child = new VectorExpression[1];",1822,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java
"//  TODO: this is an ugly hack; see the same in LlapTaskCommunicator for discussion.","//       This only lives for the duration of the service init.",157,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java
"//  Derby and Oracle do not interpret filters ANSI-properly in some cases and need a workaround.","boolean dbHasJoinCastBug = DatabaseProduct.hasJoinOperationOrderBug(dbType);",494,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java
"//  TODO : currently not testing the following scenarios:     a) Multi-db wh-level REPL LOAD - need to add that     b) Insert into tables - quite a few cases need to be enumerated there, including dyn adds.","public void testConstraints() throws IOException {",2695,0.45714285714285713,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java
"/*    * TODO: 1. PPD needs to get pushed in to TS   *   * @param scanRel   * @return    */","OpAttr visit(HiveTableScan scanRel) {",197,0.7096774193548387,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/HiveOpConverter.java
"//  2. Convert NONACIDORCTBL to ACID table.  //todo: remove trans_prop after HIVE-17089","runStatementOnDriver("alter table " + Table.NONACIDORCTBL + " SET TBLPROPERTIES ('transactional'='true', 'transactional_properties'='default')");",324,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java
"//  FIXME: possibly the distinction between table/partition is not need; however it was like this before....will change it later","public PPart(Table table, Partition partiton) {",137,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/stats/Partish.java
"//  don't take directories into account for quick stats TODO: wtf?","if (!status.isDir()) {",724,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/utils/MetaStoreUtils.java
"/*    * Fixup the children and parents of a new vector child.   *   * 1) Add new vector child to the vector parent's children list.   *   * 2) Copy and fixup the parent list of the original child instead of just assuming a 1:1   *    relationship.   *   *    a) When the child is MapJoinOperator, it will have an extra parent HashTableDummyOperator   *       for the MapJoinOperator's small table.  It needs to be fixed up, too.    */","too long.",0,0,0
"//  TODO: The copy of data is unnecessary, but there is no work-around","// since we cannot directly set the private byte[] field inside Text.",234,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazySimpleSerDe.java
"/*    * Get the list of partitions that need to update statistics.   * TODO: we should reuse the Partitions generated at compile time   * since getting the list of partitions is quite expensive.   *   * @return a list of partitions that need to update statistics.   * @throws HiveException    */","too long.",0,0,0
"//  TODO: This test should be removed once ACID tables replication is supported.","@Test",2834,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java
"//  TODO: there is no easy and reliable way to compute the memory used by the executor threads and on-heap cache.","// Assuming the used memory is equally divided among all executors.",899,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java
"//  TODO: normally, the result is not necessary; might make sense to pass false","for (org.apache.hadoop.hive.metastore.api.Partition outPart",2464,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
"//  Remove new-alloc flag on first use. Full unlock after that would imply force-discarding   this buffer is acceptable. This is kind of an ugly compact between the cache and us.","newValue = State.switchFlag(newValue, State.FLAG_NEW_ALLOC);",98,0.6905829596412556,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LlapAllocatorBuffer.java
"//  Only attempt to do this, if cmd was successful.   FIXME: it would be probably better to move this to an after-execution","int rc = setFSPermsNGrp(ss, driver.getConf());",54,0.5857142857142857,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/HCatDriver.java
"//  TODO Cleanup pending tasks etc, so that the next dag is not affected.","}",1031,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java
"//  TODO: does this include partition columns?","boolean[] included = new boolean[schema.size()];",98,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/io/BatchToRowReader.java
"//            work on BytesColumnVector output columns???","outputColVector.init();",85,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorCoalesce.java
"//  Two ReduceSinkOperators are correlated means that   they have same sorting columns (key columns), same partitioning columns,   same sorting orders, and no conflict on the numbers of reducers.   TODO: we should relax this condition   TODO: we need to handle aggregation functions with distinct keyword. In this case,   distinct columns will be added to the key columns.","too long.",0,0,0
"//  FIXME: move TestJsonSerDe from hcat to serde2","public class JsonSerDe extends AbstractSerDe {",87,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/serde/src/java/org/apache/hadoop/hive/serde2/JsonSerDe.java
"//  TODO Confirm this is safe","tmpNoNulls = getMaxNulls(stats, ((ExprNodeFieldDesc) pred).getDesc());",809,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java
"//  TODO: execute errors like this currently don't return good error","// codes and messages. This should be fixed.",1046,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/itests/hive-unit/src/test/java/org/apache/hive/jdbc/cbo_rp_TestJdbcDriver2.java
"//  THIS IS NOT WORKING workaround is to set it as part of java opts -Duser.timezone="UTC"","JSON_MAPPER.setTimeZone(TimeZone.getTimeZone("UTC"));",205,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/druid-handler/src/java/org/apache/hadoop/hive/druid/DruidStorageHandlerUtils.java
"//  simply get the next day and go back half a day. This is not ideal but seems to work.","if (!doesTimeMatter) return daysToMillis(d + 1) - (MILLIS_PER_DAY >> 1);",163,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/storage-api/src/java/org/apache/hadoop/hive/serde2/io/DateWritable.java
"//  TODO: maybe we should throw this as-is too. ThriftCLIService currently catches Exception,         so the combination determines what would kill the HS2 executor thread. For now,         let's only allow OOM to propagate.","throw new RuntimeException(e.getCause());",85,0.625,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/service/src/java/org/apache/hive/service/cli/session/HiveSessionProxy.java
"//  TODO May be possible to do finer grained locks.","synchronized (watchesPerAttempt) {",160,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/DirWatcher.java
"//  TODO: this check is somewhat bogus as the maxJvmMemory != Xmx parameters (see annotation in LlapServiceDriver)","Preconditions.checkState(maxJvmMemory >= memRequired,",212,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapDaemon.java
"//  TODO: Check if maximum size compatible with AbsoluteKeyOffset.maxSize.","this.writeBuffers = writeBuffers;",165,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastKeyStore.java
"//  @deprecated in favour of {@link HCatTable.#escapeChar()}.","public Builder escapeChar(char escapeChar) {",386,0.907563025210084,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/hcatalog/webhcat/java-client/src/main/java/org/apache/hive/hcatalog/api/HCatCreateTableDesc.java
"//  If the sorted columns is a superset of bucketed columns, store this fact.   It can be later used to   optimize some group-by queries. Note that, the order does not matter as   long as it in the first","// 'n' columns where 'n' is the length of the bucketed columns.",833,0.5675675675675675,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/plan/CreateTableDesc.java
"//  FIXME : replace with hive copy once that is copied","private final static String tid =",101,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java
"//  TODO: revisit the fence","if (ndvToBeSmoothed > 3)",186,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/stats/HiveRelMdSelectivity.java
"//  TODO: 1) How to handle collisions? 2) Should we be cloning ColumnInfo or not?","private static boolean add(RowResolver rrToAddTo, RowResolver rrToAddFrom,",353,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/parse/RowResolver.java
"//  Optimize the scenario when there are no grouping keys and no distinct - 2   map-reduce jobs are not needed","// For eg: select count(1) from T where t.ds = ....",6701,0.8266666666666667,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
"//  Join key expression is likely some expression involving functions/operators, so there   is no actual table column for this. But the ReduceSink operator should still have an   output column corresponding to this expression, using the columnInternalName.   TODO: does tableAlias matter for this kind of expression?","too long.",0,0,0
"//  TODO: why does Tez API use "Object" for this?","if (task instanceof TaskAttempt) {",1115,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java
"//  TODO: this log statement looks wrong","LOG.debug("Found " + deltas.size() + " delta files, threshold is " + deltaNumThreshold +",339,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java
"//  Relying on a task succeeding to reset the exponent.   There's no notifications on whether a task gets accepted or not. That would be ideal to   reset this.","cumulativeBackoffFactor = cumulativeBackoffFactor * blacklistConf.backoffFactor;",2522,0.5232558139534884,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java
"//  We need to override these methods due to difference in nullability between Hive and   Calcite for the return types of the aggregation  (in particular, for COUNT and SUM0).   TODO: We should close the semantics gaps between Hive and Calcite for nullability of   aggregation calls return types. This might be useful to trigger some additional   rewriting rules that would remove unnecessary predicates, etc.","too long.",0,0,0
"//  Cancel job if the monitor found job submission timeout.   TODO: If the timeout is because of lack of resources in the cluster, we should   ideally also cancel the app request here. But w/o facilities from Spark or YARN,   it's difficult to do it on hive side alone. See HIVE-12650.","too long.",0,0,0
"//  TODO: Should be checked on server side. On Embedded metastore it throws   NullPointerException, on Remote metastore it throws TTransportException","Assert.fail("Expected a NullPointerException or TTransportException to be thrown");",957,0.656084656084656,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestTablesCreateDropAlterTruncate.java
"//  TODO: do we ever need the port? we could just do away with nodeId altogether.","LlapNodeId nodeId = LlapNodeId.getInstance(hostname, port);",712,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java
"/*    * Metastore related options that the db is initialized against. When a conf   * var in this is list is changed, the metastore instance for the CLI will   * be recreated so that the change will take effect.   * TODO - I suspect the vast majority of these don't need to be here.  But it requires testing   * before just pulling them out.    */","too long.",0,0,0
"//  Originals split won't work due to MAPREDUCE-7086 issue in FileInputFormat.","boolean isBrokenUntilMapreduce7086 = "TEXTFILE".equals(format);",999,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java
"//  TODO: VectorizedParquetRecordReader doesn't support map, array now, the value of   ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR should be updated after support these data   types.","conf.setBoolean(ColumnProjectionUtils.READ_ALL_COLUMNS, false);",331,0.6239316239316239,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/itests/hive-jmh/src/main/java/org/apache/hive/benchmark/storage/ColumnarStorageBench.java
"//  TODO: if there are more fields perhaps there should be an array of class.","TezAttemptArray aw = new TezAttemptArray();",407,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java
"//  Some columns in select are pruned. This may happen if those are constants.   TODO: the best solution is to hook the operator before fs with the select operator.    See smb_mapjoin_20.q for more details.","return null;",623,0.5541125541125541,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/optimizer/BucketingSortingReduceSinkOptimizer.java
"//  TODO: getUserFromAuthenticator?","String loggedInUserName = SessionState.get().getUserName();",48,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFLoggedInUser.java
"//  TODO: this seems wrong (following what Hive Regular does)","if (!distParamInRefsToOutputPos.containsKey(argLst.get(i))",253,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/HiveGBOpConvUtil.java
"//  TODO: we're asking the metastore what its configuration for this var is - we may   want to revisit to pull from client side instead. The reason I have it this way   is because the metastore is more likely to have a reasonable config for this than   an arbitrary client.","too long.",0,0,0
"//  TODO: can this result in cross-thread reuse of session state?","SessionState.setCurrentSessionState(parentSessionState);",308,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java
"//  This condition-check could have been avoided, but to honour the old   default of not calling if it wasn't set, we retain that behaviour.   TODO:cleanup after verification that the outer if isn't really needed here","if (isLocationSet) {",259,0.4978902953586498,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java
"//  TODO support only non nested case","private PrimitiveType getElementType(Type type) {",474,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/vector/VectorizedParquetRecordReader.java
"//  @deprecated in favour of {@link HCatTable.#mapKeysTerminatedBy()}. To be removed in Hive 0.16.","public Builder mapKeysTerminatedBy(char delimiter) {",400,0.9392265193370166,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/hcatalog/webhcat/java-client/src/main/java/org/apache/hive/hcatalog/api/HCatCreateTableDesc.java
"//  TODO: get rid of this","private final QueryFragmentCounters counters;",71,0.5507246376811594,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java
"// todo: this is not remotely accurate if you have many (relevant) original files","return true;",624,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/upgrade-acid/src/main/java/org/apache/hadoop/hive/upgrade/acid/UpgradeTool.java
"//  TODO: we could remove extra copy for isUncompressed case by copying directly to cache.","// We need to consolidate 2 or more buffers into one to decompress.",1535,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java
"//  TODO HIVE-11687 It's possible for a bunch of tasks to come in around the same time, without the   actual executor threads picking up any work. This will lead to unnecessary rejection of tasks.","// The wait queue should be able to fit at least (waitQueue + currentFreeExecutor slots)",450,0.6666666666666666,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java
"//  conflict when loaded. Some issue with framework which needs to be relook into later.","primary.run("drop database if exists " + primaryDbName + " cascade");",1057,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java
"//  @deprecated in favour of {@link HCatTable.#fileFormat()}. To be removed in Hive 0.16.","public String getFileFormat() {",209,0.9325153374233128,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/hcatalog/webhcat/java-client/src/main/java/org/apache/hive/hcatalog/api/HCatCreateTableDesc.java
"//  dangerous; let's explicitly add an incomplete CB.","check.replaceSelfWith(new IncompleteCb(check.getOffset(), check.getEnd()));",82,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/io/metadata/OrcFileEstimateErrors.java
"//  This is not a complete list, barely make information schema work","switch (((PrimitiveTypeInfo)columnTypes.get(i)).getTypeName()) {",78,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/jdbc-handler/src/main/java/org/apache/hive/storage/jdbc/dao/JdbcRecordIterator.java
"//  TODO: perhaps we should also summarize the triggers pointing to invalid pools.","if (!unusedTriggers.isEmpty()) {",922,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/MetaDataFormatUtils.java
"//  TODO: could we instead get FS from path here and add normal files for every UGI?","MockFileSystem.clearGlobalFiles();",2701,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java
"//  We cannot get to root TableScan operator, likely because there is a join or group-by   between topOp and root TableScan operator. We don't handle that case, and simply return","return false;",226,0.6605504587155964,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/optimizer/AbstractBucketJoinProc.java
"//  REVIEW:  are we supposed to be applying the getReadColumnIDs","// same as in getRecordReader?",326,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseTableInputFormat.java
"/*    * todo: this should accept a file of table names to exclude from non-acid to acid conversion   * todo: change script comments to a preamble instead of a footer   *   * how does rename script work?  "hadoop fs -mv oldname newname"    * and what what about S3?   * How does this actually get executed?   * all other actions are done via embedded JDBC   *   *    */","too long.",0,0,0
"//  TODO: the global lock might be to coarse here.","private final Object lock = new Object();",68,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/llap/LlapOutputFormatService.java
"// todo: enums? that have both field name and value list","private interface Field {",1640,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java
"//  if the result is not boolean and not all partition agree on the   result, we don't remove the condition. Potentially, it can miss   the case like "where ds % 3 == 1 or ds % 3 == 2"   TODO: handle this case by making result vector to handle all   constant values.","too long.",0,0,0
"//  Need to remove this static hack. But this is the way currently to get a session.","SessionState ss = SessionState.get();",146,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java
"//  TODO : code section copied over from SerDeUtils because of non-standard json production there   should use quotes for all field names. We should fix this there, and then remove this copy.   See http://jackson.codehaus.org/1.7.3/javadoc/org/codehaus/jackson/JsonParser.Feature.html#ALLOW_UNQUOTED_FIELD_NAMES   for details - trying to enable Jackson to ignore that doesn't seem to work(compilation failure   when attempting to use that feature, so having to change the production itself.","too long.",0,0,0
"//  Assume we should have the exact same object.   TODO: we could also compare the schema and SerDe, and pass only those to the call         instead; most of the time these would be the same and LLAP IO can handle that.","None",None,None,None
"// todo: HIVE-15549  SORT_PARTITION_EDGE","None",None,None,None
"//  TODO - Allow the branch to be specified as a parameter to ptest, rather than requiring a separate property file.","// (will need to handle an alternate work-dir as well in this case - derive from branch?)",245,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/testutils/ptest2/src/main/java/org/apache/hive/ptest/execution/conf/TestConfiguration.java
"//  Note that the tableExists flag as used by Auth is kinda a hack and   assumes only 1 table will ever be imported - this assumption is broken by   REPL LOAD.     However, we've not chosen to expand this to a map of tables/etc, since   we have expanded how auth works with REPL DUMP / REPL LOAD to simply   require ADMIN privileges, rather than checking each object, which   quickly becomes untenable, and even more so, costly on memory.","too long.",0,0,0
"//  TODO Change after HIVE-9987. For now, there's no rack matching.","if (requestedHosts != null && requestedHosts.length != 0) {",2710,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java
"/*    * Only a small set of operations is allowed inside an explicit transactions, e.g. DML on   * Acid tables or ops w/o persistent side effects like USE DATABASE, SHOW TABLES, etc so   * that rollback is meaningful   * todo: mark all operations appropriately    */","too long.",0,0,0
"// todo: once multistatement txns are supported, add a test to run next 2 statements in a single txn","runStatementOnDriver("delete from " + Table.ACIDTBL + " where a in(select a from " + Table.NONACIDORCTBL + ")");",919,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java
"//  TODO HiveQueryId extraction by parsing the Processor payload is ugly. This can be improved   once TEZ-2672 is fixed.","String hiveQueryId;",354,0.8876404494382022,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java
"//  UNDONE: Why don't these methods take decimalPlaces?","public static void floor(int i, HiveDecimal input, DecimalColumnVector outputColVector) {",285,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java
"/*     * Serializes decimal64 up to the maximum 64-bit precision (18 decimal digits).    *    * NOTE: Major assumption: the fast decimal has already been bounds checked and a least    * has a precision <= DECIMAL64_DECIMAL_DIGITS.  We do not bounds check here for better    * performance.     */","too long.",0,0,0
"//  TODO: Refactor this and do in a more object oriented manner","private boolean isExprResolver;",54,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/parse/RowResolver.java
"//  Some data is missing from the stream for PPD uncompressed read (because index offset is   relative to the entire stream and we only read part of stream if RGs are filtered; unlike   with compressed data where PPD only filters CBs, so we always get full CB, and index offset   is relative to CB). To take care of the case when UncompressedStream goes seeking around by   its incorrect (relative to partial stream) index offset, we will increase the length by our   offset-relative-to-the-stream, and also account for it in buffers (see createDiskRangeInfo).   So, index offset now works; as long as noone seeks into this data before the RG (why would   they), everything works. This is hacky... Stream shouldn't depend on having all the data.","too long.",0,0,0
"//  TODO HIVE-15865 Ideally sort these by completion time, once that is available.","boolean isFirst = true;",501,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/cli/LlapStatusServiceDriver.java
"/*    * Generate a temporary path for dynamic partition pruning in Spark branch   * TODO: no longer need this if we use accumulator!   * @param basePath   * @param id   * @return    */","public static Path generateTmpPathForPartitionPruning(Path basePath, String id) {",142,0.6169154228855721,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkUtilities.java
"//  TODO: probably temporary before HIVE-13698; after that we may create one per session.","private static final Cache<String, LlapTokenLocalClient> localClientCache = CacheBuilder",66,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-client/src/java/org/apache/hadoop/hive/llap/coordinator/LlapCoordinator.java
"//  FIXME manager's EndOfBatch threadlocal can be deleted","// LOG4J2-1292 utilize gc-free Layout.encode() method: taken care of in superclass",98,0.6666666666666666,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/log/HushableRandomAccessFileAppender.java
"//  TODO: most of the time, there's no in-memory. Use an array?","for (int i = 0; i < locInfo.length; i++) {",1619,0.816,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/SerDeEncodedDataReader.java
"//  TODO: could we tell the policy that we don't care about these and have them evicted? or we         could just deallocate them when unlocked, and free memory + handle that in eviction.         For now, just abandon the blocks - eventually, they'll get evicted.","too long.",0,0,0
"//  A single concurrent request per node is currently hardcoded. The node includes a port number   so different AMs on the same host count as different nodes; we only have one request type,   and it is not useful to send more than one in parallel.","super(LlapPluginEndpointClientImpl.class.getSimpleName(),",54,0.5755395683453237,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/LlapPluginEndpointClientImpl.java
"//  TODO MS-SPLIT Switch this back once HiveMetaStoreClient is moved.  req.setCapabilities(HiveMetaStoreClient.TEST_VERSION);","req.setCapabilities(new ClientCapabilities(",65,0.686046511627907,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/events/InsertEvent.java
"//  @deprecated in favour of {@link HCatTable.#getTableType()}. To be removed in Hive 0.16.","public boolean getExternal() {",177,0.9341317365269461,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/hcatalog/webhcat/java-client/src/main/java/org/apache/hive/hcatalog/api/HCatCreateTableDesc.java
"//  Currently MAP type is not supported. Add it back when Arrow 1.0 is released.","// See: SPARK-21187",52,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcWithMiniLlapArrow.java
"//  TODO: why CAS if the result is not checked?","shouldRun.compareAndSet(true, false);",189,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-common/src/java/org/apache/hadoop/hive/llap/AsyncPbRpcProxy.java
"//  TODO: Shouldn't we propgate vc? is it vc col from tab or all vc","return new OpAttr("", new HashSet<Integer>(), rsGBOp2);",829,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/HiveGBOpConvUtil.java
"//  TODO: readEncodedColumns is not supposed to throw; errors should be propagated thru   consumer. It is potentially holding locked buffers, and must perform its own cleanup.   Also, currently readEncodedColumns is not stoppable. The consumer will discard the   data it receives for one stripe. We could probably interrupt it, if it checked that.","too long.",0,0,0
"//  REVIEW jvs 29-Oct-2007:  Shouldn't it also be incorporating   the flavor attribute into the description?","/** Planner rule that adjusts projects when counts are added. */",2808,0.7412587412587412,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveRelDecorrelator.java
"//  Requirements: for Bucket, bucketed by their keys on both sides and fitting in memory   Obtain number of buckets  TODO: Incase of non bucketed splits would be computed based on data size/max part size","// What we need is a way to get buckets not splits",385,0.6108786610878661,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/cost/HiveOnTezCostModel.java
"//  Note: later we may be able to set multiple things together (except LIKE).","if (queryParallelism == null && likeName == null) {",909,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
"//  May be null in tests   TODO: see javadoc","None",None,None,None
"//  XXX: makes no sense for me... possibly not needed anymore","if (work.isClearAggregatorStats()) {",159,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/stats/BasicStatsTask.java
"//        So, the indices should line up... to be fixed in SE v2?","List<Integer> filePhysicalColumnIds = readerLogicalColumnIds;",590,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java
"/*     * The method for altering table props; may set the table to MM, non-MM, or not affect MM.    * todo: All such validation logic should be TransactionValidationListener    * @param tbl object image before alter table command (or null if not retrieved yet).    * @param props prop values set in this alter table command     */","too long.",0,0,0
"//  TODO : verify if skipping charset here is okay","String line;",185,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/ReplCopyTask.java
"//  TODO: support dynamic partition for CTAS","throw new SemanticException(ErrorMsg.CTAS_PARCOL_COEXISTENCE.getMsg());",13048,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
"//  TODO :  Some extra validation can also be added as this is a user provided parameter.","this.parameters.put(ReplChangeManager.SOURCE_OF_REPLICATION, parameters.get(key));",126,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/model/MDatabase.java
"//  todo: add time of abort, which is not currently tracked.","// Requires schema change",4256,0.8376068376068376,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java
"//  TODO: dump the end if wrapping around?","for (int i = 0; i < logSize; ++i) {",311,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/common/src/java/org/apache/hive/common/util/FixedSizedObjectPool.java
"//  TODO: Currently ignores GBY and PTF which may also buffer data in memory.","if (sr.dataSize > sr.maxDataSize) {",1111,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java
"//  The queryId could either be picked up from the current request being processed, or   generated. The current request isn't exactly correct since the query is 'done' once we   return the results. Generating a new one has the added benefit of working once this   is moved out of a UDTF into a proper API.   Setting this to the generated AppId which is unique.   Despite the differences in TaskSpec, the vertex spec should be the same.","too long.",0,0,0
"//  Workaround for DN bug on Postgres:   http://www.datanucleus.org/servlet/forum/viewthread_thread,7985_offset","* @deprecated Use MetastoreConf.DATANUCLEUS_INIT_COL_INFO",936,0.4696969696969697,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
"//  If necessary, divide and multiply to get rid of fractional digits.","if (fastScale == 0) {",5036,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/storage-api/src/java/org/apache/hadoop/hive/common/type/FastHiveDecimalImpl.java
"// TODO: use common thread pool later?","int threadCount = HiveConf.getIntVar(pctx.getConf(),",475,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/optimizer/SimpleFetchOptimizer.java
"//  register all permanent functions. need improvement","private void registerAllFunctionsOnce() throws HiveException {",245,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
"//  TODO: perhaps this could use a better implementation... for now even the Hive query result         set doesn't support this, so assume the user knows what he's doing when calling us.","return (T) super.getObject(columnIndex);",191,0.6933333333333334,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/jdbc/src/java/org/apache/hive/jdbc/HiveDatabaseMetaData.java
"//  This is our problem -- it means the configuration was wrong.","e.printStackTrace();",136,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/tool/TestTempletonUtils.java
"//  to be used by clients of ServiceRegistry TODO: this is unnecessary","private DynamicServiceInstanceSet instances;",74,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java
"//  TODO: should this also handle ACID operation, etc.? seems to miss a lot of stuff from HIF.","List<Path> finalDirs = null, dirsWithMmOriginals = null;",140,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/io/BucketizedHiveInputFormat.java
"//  TODO Will these checks work if some other user logs in. Isn't a doAs check required somewhere here as well.   Should a doAs check happen here instead of after the user test.   With HiveServer2 - who is the incoming user in terms of UGI (the hive user itself, or the user who actually submitted the query)","too long.",0,0,0
"//  TODO: get rid of deepCopy after making sure callers don't use references","if (maxCacheSizeInBytes > 0) {",394,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/cache/SharedCache.java
"//  TODO: this is fishy - we init object inspectors based on first tag. We         should either init for each tag, or if rowInspector doesn't really         matter, then we can create this in ctor and get rid of firstRow.","if (LOG.isInfoEnabled()) {",298,0.5221238938053098,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java
"//  Snapshot was outdated when locks were acquired, hence regenerate context,   txn list and retry   TODO: Lock acquisition should be moved before analyze, this is a bit hackish.   Currently, we acquire a snapshot, we compile the query wrt that snapshot,   and then, we acquire locks. If snapshot is still valid, we continue as usual.","too long.",0,0,0
"//  TODO: option to allow converting ORC file to insert-only transactional?","LOG.info("Converting {} to full transactional table", getQualifiedName(tableObj));",627,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/util/HiveStrictManagedMigration.java
"//  TODO: calculate this instead. Just because we're writing to the location doesn't mean that it'll   always be wanted in the meta store right away.","List<Partition> partitionEntries = metaStoreClient.listPartitions(table.getDbName(), table.getTableName(),",178,0.8155339805825242,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/hcatalog/streaming/src/test/org/apache/hive/hcatalog/streaming/mutate/StreamingAssert.java
"//  TODO: propagate this error to TezJobMonitor somehow? Without using killQuery","syncWork.toRestartInUse.add(session);",954,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/WorkloadManager.java
"//  All users belong to public role implicitly, add that role   TODO MS-SPLIT Change this back to HiveMetaStore.PUBLIC once HiveMetaStore has moved to   stand-alone metastore.  MRole publicRole = new MRole(HiveMetaStore.PUBLIC, 0, HiveMetaStore.PUBLIC);","too long.",0,0,0
"/*    * Create a bare TableSnapshotRegionSplit. Needed because Writables require a   * default-constructed instance to hydrate from the DataInput.   *   * TODO: remove once HBASE-11555 is fixed.    */","public static InputSplit createTableSnapshotRegionSplit() {",81,0.5877192982456141,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseTableSnapshotInputFormatUtil.java
"//  TODO: do better with handling types of Exception here","errorMessage = "FAILED: Hive Internal Error: " + Utilities.getNameMessage(e);",2479,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
"//  todo: returns json string. should recreate object from it?","return value;",459,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/jdbc/src/java/org/apache/hive/jdbc/HiveBaseResultSet.java
"//  TODO: in these methods, do we really need to deepcopy?","deepCopyPartitions(r.getPartitions(), result);",1349,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClientPreCatalog.java
"//  TODO: numTasksToPreempt is currently always 1.","preemptedTaskList = preemptTasksFromMap(speculativeTasks, forPriority, forVertex,",1988,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java
"//  TODO Optimization: Add a check to see if there's any capacity available. No point in","// walking through all active nodes, if they don't have potential capacity.",1761,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java
"//  Wait a while for existing tasks to terminate   XXX this will wait forever... :)","while (!threadPool.awaitTermination(10, TimeUnit.SECONDS)) {",369,0.7428571428571429,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/stats/BasicStatsNoJobTask.java
"//  todo replace below with joda-time, which supports timezone","if (expr.getType() == HiveParser.TOK_DATELITERAL) {",510,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java
"//  TODO: why is this inconsistent with what we get by names?","LOG.debug("Done retrieving all objects for getPartitionsViaOrmFilter");",3391,0.8099173553719008,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java
"// Not implemented.","}",225,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/common/src/java/org/apache/hadoop/hive/common/metrics/LegacyMetrics.java
"// TODO this should be returning a class not just an int","DataInputByteBuffer in = new DataInputByteBuffer();",399,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/ShuffleHandler.java
"//  there are 2 or more distincts, or distinct is not on count   TODO: may be the same count(distinct key), count(distinct key)   TODO: deal with duplicate count distinct key","return -1;",159,0.5157894736842106,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/optimizer/CountDistinctRewriteProc.java
"//  id - TODO: use a non-zero index to check for offset errors.","private static final int[] BUCKET_COLUMN_INDEXES = new int[] { 0 };",34,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/hcatalog/streaming/src/test/org/apache/hive/hcatalog/streaming/mutate/worker/TestBucketIdResolverImpl.java
"//  @deprecated in favour of {@link HCatTable.#comment(String)}. To be removed in Hive 0.16.","public Builder comments(String comment) {",322,0.9349112426035503,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/hcatalog/webhcat/java-client/src/main/java/org/apache/hive/hcatalog/api/HCatCreateTableDesc.java
"//  TODO:pc need to enhance this with complex fields and getType_all function","@Test",1171,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java
"//  Hive adds the same mapping twice... I wish we could fix stuff like that.","if (existing == null) {",404,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/parse/RowResolver.java
"// todo: this should be moved to be an inner class of ReaderWrite as that is the only place it    is used","public static final byte NULL = 1;",40,0.9615384615384616,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/DataType.java
"//  HarFileSystem has a bug where this method does not work properly   if the underlying FS is HDFS. See MAPREDUCE-1877 for more   information. This method is from FileSystem.","FileStatus status = getFileStatus(f);",47,0.5628140703517588,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/shims/common/src/main/java/org/apache/hadoop/hive/shims/HiveHarFileSystem.java
"// TODO: Clean up/refactor assumptions","boolean includeGrpSetInGBDesc = (gbInfo.grpSets.size() > 0)",957,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/HiveGBOpConvUtil.java
"//  TODO This executor seems unnecessary. Here and TezChild","executor = new StatsRecordingThreadPool(1, 1,",215,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java
"//  TODO: Verify GB having is not a seperate filter (if so we shouldn't   introduce derived table)","None",None,None,None
"/*  This is the TestPerformance Cli Driver for integrating performance regression tests as part of the Hive Unit tests. Currently this includes support for : 1. Running explain plans for TPCDS workload (non-partitioned dataset)  on 30TB scaleset. TODO : 1. Support for partitioned data set 2. Use HBase Metastore instead of DerbyThis suite differs from TestCliDriver w.r.t the fact that we modify the underlying metastoredatabase to reflect the dataset before running the queries. */","too long.",0,0,0
"//  Hack note - different split strategies return differently typed lists, yay Java.   This works purely by magic, because we know which strategy produces which type.","if (splitStrategy instanceof ETLSplitStrategy) {",1820,0.6794258373205742,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java
"//  TODO: deriveExplainAttributes should be called here, code is too fragile to move it around.","if (task instanceof ConditionalTask) {",902,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
"/*  * Stores binary key/value in sorted manner to get top-n key/value * TODO: rename to TopNHeap?  */","public class TopNHash {",43,0.8029197080291971,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/TopNHash.java
"//  FIXME: isNull is not updated; which might cause problems","@Deprecated",311,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorHashKeyWrapper.java
"//  TODO: Should we use grpbyExprNDesc.getTypeInfo()? what if expr is   UDF","int i = gbExprNDescLst.size();",3493,0.9752066115702479,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
"//  TODO: we should really probably throw. Keep the existing logic for now.","LOG.warn("Skipping unknown directory " + dirName",2078,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
"/*  * It's column level Parquet reader which is used to read a batch of records for a list column. * TODO Currently List type only support non nested case.  */","public class VectorizedListColumnReader extends BaseVectorizedColumnReader {",36,0.7487684729064039,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/vector/VectorizedListColumnReader.java
"//  TODO: not clear why we don't do the rest of the cleanup if dagClient is not created.         E.g. jobClose will be called if we fail after dagClient creation but no before...","//       DagClient as such should have no bearing on jobClose.",311,0.6730769230769231,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java
"// TODO HIVE-16862: Implement a similar feature like "hive.tez.dynamic.semijoin.reduction" in hive on spark","semiJoin = false;",155,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/optimizer/DynamicPartitionPruningOptimization.java
"//  TODO: Currently we do not expose any runtime info for non-streaming tables.   In future extend this add more information regarding table status.   e.g. Total size of segments in druid, loadstatus of table on historical nodes etc.","return null;",1135,0.5116279069767442,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/druid-handler/src/java/org/apache/hadoop/hive/druid/DruidStorageHandler.java
"//  This is rather obscure. The end of last row cached is precisely at the split end offset.   If the split is in the middle of the file, LRR would read one more row after that,   therefore as unfortunate as it is, we have to do a one-row read. However, for that to   have happened, someone should have supplied a split that ends inside the last row, i.e.   a few bytes earlier than the current split, which is pretty unlikely. What is more likely   is that the split, and the last row, both end at the end of file. Check for this.","too long.",0,0,0
"//  TODO: this should also happen on any error. Right now this task will just fail.","StatsSetupConst.setBasicStatsState(parameters, StatsSetupConst.FALSE);",131,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/stats/BasicStatsTask.java
"//  TODO null can also mean that this operation was interrupted. Should we really try to re-create the session in that case ?","if (client == null) {",361,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java
"//  TODO: Should we wait for the entry to actually be deleted from HDFS? Would have to   poll the reader count, waiting for it to reach 0, at which point cleanup should occur.","if (hasSpaceForCacheEntry(entry, size)) {",835,0.6602870813397129,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/cache/results/QueryResultsCache.java
"//  FIXME for ctas this is still needed because location is not set sometimes","if (tbl.getSd().getLocation() == null) {",123,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/StatsTask.java
"//  TODO Consolidate this code with TezChild.","runtimeWatch.start();",224,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java
"//  TODO: is this check even needed given what the caller checks?","if (!enablePreemption || preemptionQueue.isEmpty()) {",718,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java
"//  FIXME: file paths in strings should be changed to either File or Path ... anything but String","private String resultsDirectory;",56,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/itests/util/src/main/java/org/apache/hadoop/hive/cli/control/AbstractCliConfig.java
"//  TODO: why is "&& !isAcidIUDoperation" needed here?","if (!isTxnTable && ((loadFileType == LoadFileType.REPLACE_ALL) || (oldPart == null && !isAcidIUDoperation))) {",1803,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
"//  Get all simple fields for partitions and related objects, which we can map one-on-one.   We will do this in 2 queries to use different existing indices for each one.   We do not get table and DB name, assuming they are the same as we are using to filter.   TODO: We might want to tune the indexes instead. With current ones MySQL performs   poorly, esp. with 'order by' w/o index on large tables, even if the number of actual   results is small (query that returns 8 out of 32k partitions can go 4sec. to 0sec. by   just adding a \"PART_ID\" IN (...) filter that doesn't alter the results to it, probably","too long.",0,0,0
"//  Tez session relies on a threadlocal for open... If we are on some non-session thread,   just use the same SessionState we used for the initial sessions.   Technically, given that all pool sessions are initially based on this state, shoudln't   we also set this at all times and not rely on an external session stuff? We should   probably just get rid of the thread local usage in TezSessionState.","too long.",0,0,0
"//  hive input format doesn't handle the special condition of no paths + 1   split correctly.","inpFormat = CombineHiveInputFormat.class.getName();",241,0.8888888888888888,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java
"//     not external itself. Is that the case? Why?","{",636,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java
"//  TODO: this is a stopgap fix. We really need to change all mappings by unique node ID,         or at least (in this case) track the latest unique ID for LlapNode and retry all","//       older-node tasks proactively. For now let the heartbeats fail them.",729,0.6859903381642513,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java
"//  FIXME: druid storage handler relies on query.id to maintain some staging directories   expose queryid to session level","if (hiveConf != null) {",212,0.847457627118644,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/QueryState.java
"//  TODO - we need to speed this up for the normal path where all partitions are under   the table and we don't have to stat every partition","firePreEvent(new PreDropPartitionEvent(tbl, part, deleteData, this));",4318,0.7640449438202247,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
"//  if this function is frequently used, we need to optimize this.","return Float.parseFloat(toFormalString());",1659,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/common/src/java/org/apache/hadoop/hive/common/type/Decimal128.java
"//  This is ugly in two ways...   1) We assume that LlapWrappableInputFormatInterface has NullWritable as first parameter.      Since we are using Java and not, say, a programming language, there's no way to check.   2) We ignore the fact that 2nd arg is completely incompatible (VRB -> Writable), because      vectorization currently works by magic, getting VRB from IF with non-VRB value param.   So we just cast blindly and hope for the best (which is obviously what happens).","too long.",0,0,0
"//  TODO: two possible improvements         1) Right now we kill all the queries here; we could just kill -qpDelta.         2) After the queries are killed queued queries would take their place.            If we could somehow restart queries we could instead put them at the front","too long.",0,0,0
"//  This method is inefficient. It's only used when something crosses buffer boundaries.","ponderNextBufferToRead(readPos);",168,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/serde/src/java/org/apache/hadoop/hive/serde2/WriteBuffers.java
"//  TODO: why doesn't this check class name rather than toString?","String sh = tbl.getStorageHandler().toString();",5365,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
"//  TODO: Modify Thrift IDL to generate export stage if needed","return StageType.REPL_DUMP;",70,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/ExportTask.java
"//  Workaround for testing since tests can't set the env vars.","path = System.getProperty(TEST_ENV_WORKAROUND + envVar);",1247,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/conf/MetastoreConf.java
"//  TODO: the memory release could be optimized - we could release original buffers after we         are fully done with each original buffer from disk. For now release all at the end;         it doesn't increase the total amount of memory we hold, just the duration a bit.         This is much simpler - we can just remember original ranges after reading them, and         release them at the end. In a few cases where it's easy to determine that a buffer         can be freed in advance, we remove it from the map.","too long.",0,0,0
"//  TODO: perhaps add counters for separate things and multiple buffer cases.","sb.append("\nMetadata cache state: ").append(metadata.size()).append(",133,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/io/metadata/MetadataCache.java
"//  TODO: this might only be applicable to TezSessionPoolManager; try moving it there?","newSession.getConf().set(TezConfiguration.TEZ_QUEUE_NAME, queueName);",263,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPool.java
"//  TODO CAT - a number of these need to be updated.  Don't bother with deprecated methods as   this is just an internal class.  Wait until we're ready to move all the catalog stuff up   into ql.","@Override",106,0.6548672566371682,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/metadata/SessionHiveMetaStoreClient.java
"//  in case of outer joins, we need to pull in records from the sides we still   need to produce output for apart from the big table. for e.g. full outer join   TODO: this reproduces the logic of the loop that was here before, assuming","// firstFetchHappened == true. In reality it almost always calls joinOneGroup. Fix it?",469,0.4939271255060729,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/CommonMergeJoinOperator.java
"//  TODO: not clear why this check and skipSeek are needed.","if (_dataStream != null && _dataStream.available() > 0) {",281,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java
"//  TODO: is this a safe assumption (name collision, external names...)","SelectDesc sd = new SelectDesc(exprCols, exprNames);",312,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/HiveOpConverter.java
"//  TODO: Cast Function in Calcite have a bug where it infer type on cast throws","// an exception",355,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/RexNodeConverter.java
"// @TODO FIX this, we actually do not need this anymore,","// in addition to that Druid allow numeric dimensions now so this check is not accurate",2779,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
"//  @deprecated in favour of {@link HCatTable.#getTableName()}. To be removed in Hive 0.16.","public String getTableName() {",102,0.9341317365269461,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/hcatalog/webhcat/java-client/src/main/java/org/apache/hive/hcatalog/api/HCatCreateTableDesc.java
"//  Based on UserGroupInformation::createProxyUser.   TODO: use a proper method after we can depend on HADOOP-13081.","if (getSubjectMethod == null) {",1418,0.6438356164383562,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java
"//  TODO : isn't there a prior impl of an isDirectory utility PathFilter so users don't have to write their own?","return new PathFilter() {",408,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java
"//  get rid of trivial case first, so that we can safely assume non-null","}",45,0.90625,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/hcatalog/webhcat/java-client/src/main/java/org/apache/hive/hcatalog/api/repl/HCatReplicationTaskIterator.java
"//  get the tmp URI path; it will be a hdfs path if not local mode   TODO [MM gap?]: this doesn't work, however this is MR only.        The path for writer and reader mismatch:        Dump the side-table for tag ... -local-10004/HashTable-Stage-1/MapJoin-a-00-(ds%3D2008-04-08)mm_2.hashtable        Load back 1 hashtable file      -local-10004/HashTable-Stage-1/MapJoin-a-00-srcsortbucket3outof4.txt.hashtable","too long.",0,0,0
"//  TODO: this duplicates a method in ORC, but the method should actually be here.","public final static String stringifyDiskRanges(DiskRangeList range) {",67,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/storage-api/src/java/org/apache/hadoop/hive/common/io/DiskRangeList.java
"//  TODO: will this work correctly with ACID?","boolean[] readerIncludes = OrcInputFormat.genIncludedColumns(",622,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java
"//  filter columns may have -1 as index which could be partition column in SARG.   TODO: should this then be >=?","if (i > 0) {",310,0.8441558441558441,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java
"//  TODO: this interface is ugly. The two implementations are so far apart feature-wise","//       after all the perf changes that we might was well hardcode them separately.",1461,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/SerDeEncodedDataReader.java
"//  @deprecated in favour of {@link HCatTable.#linesTerminatedBy()}. To be removed in Hive 0.16.","public Builder linesTerminatedBy(char delimiter) {",407,0.9378531073446328,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/hcatalog/webhcat/java-client/src/main/java/org/apache/hive/hcatalog/api/HCatCreateTableDesc.java
"//  validate is false by default if we enable the constraint   TODO: A constraint like NOT NULL could be enabled using ALTER but VALIDATE remains    false in such cases. Ideally VALIDATE should be set to true to validate existing data","validate = false;",1012,0.4117647058823529,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
"//  TODO: this needs to be enhanced once change management based filesystem is implemented","// Currently using fileuri#checksum#cmrooturi#subdirs as the format",355,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/ReplChangeManager.java
"// todo: it would be nice to check the contents of the files... could use orc.FileDump - it has   methods to print to a supplied stream but those are package private","runStatementOnDriver("alter table nobuckets compact 'major'");",133,0.7333333333333333,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/test/org/apache/hadoop/hive/ql/TestTxnNoBuckets.java
"/*      * Why no null and class checks?     * With the new design a WindowingSpec must contain a WindowFunctionSpec.     * todo: cleanup datastructs.      */","WindowFunctionSpec wFn = (WindowFunctionSpec) getWindowExpressions().get(0);",93,0.3546099290780142,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/parse/WindowingSpec.java
"/*      * RW lock ensures we have a consistent view of the file data, which is important given that     * we generate "stripe" boundaries arbitrarily. Reading buffer data itself doesn't require     * that this lock is held; however, everything else in stripes list does.     * TODO: make more granular? We only care that each one reader sees consistent boundaries.     *       So, we could shallow-copy the stripes list, then have individual locks inside each.      */","too long.",0,0,0
"//  TODO: wtf?","HashPartition hashPartition = new HashPartition(1024, (float) 0.75, 524288, 1, true, null);",27,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/test/org/apache/hadoop/hive/ql/exec/persistence/TestHashPartition.java
"//  TODO Avoid reading this from the environment","user = System.getenv(ApplicationConstants.Environment.USER.name());",171,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java
"//  TODO: we wish we could cache the Hive object, but it's not thread safe, and each         threadlocal we "cache" would need to be reinitialized for every query. This is         a huge PITA. Hive object will be cached internally, but the compat check will be         done every time inside get().","too long.",0,0,0
"//  TODO: can we merge neighboring splits? So we don't init so many readers.","FileSplit sliceSplit = new FileSplit(split.getPath(), split.getStart(),",807,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/SerDeEncodedDataReader.java
"//  Don't break! We might find a better match later.","} else {",1275,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
"//  No stats exist for this key; add a new object to the cache   TODO: get rid of deepCopy after making sure callers don't use references","if (maxCacheSizeInBytes > 0) {",393,0.6114649681528662,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/cache/SharedCache.java
"//  TODO: in the current impl, triggers are added to RP. For tez, no pool triggers (mapping between trigger name and   pool name) will exist which means all triggers applies to tez. For LLAP, pool triggers has to exist for attaching   triggers to specific pools.   For usability,   Provide a way for triggers sharing/inheritance possibly with following modes   ONLY - only to pool   INHERIT - child pools inherit from parent","too long.",0,0,0
"//  TODO: Using 0 might be wrong; might need to walk down to find the","// proper index of a dummy.",400,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/PlanModifierForASTConv.java
"//  TODO: this should come through RelBuilder to the constructor as opposed to","// set method. This requires calcite change",183,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/reloperators/HiveProject.java
"//  TODO : the contains message check is fragile, we should refactor SemanticException to be   queriable for error code, and not simply have a message   NOTE : IF_EXISTS might also want to invoke this, but there's a good possibility   that IF_EXISTS is stricter about table existence, and applies only to the ptn.   Therefore, ignoring IF_EXISTS here.","too long.",0,0,0
"//  TODO : we were discussing an iter interface, and also a LazyTuple   change this when plans for that solidifies.","return t;",65,0.7516778523489933,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hive/hcatalog/pig/HCatBaseLoader.java
"//  TODO: add method to UDFBridge to say if it is a cast func","if (sp >= 0 & (sp + 1) < udfClassName.length()) {",373,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/RexNodeConverter.java
"//  TODO Unregister the task for state updates, which could in turn unregister the node.","getContext().taskKilled(taskAttemptId,",857,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java
"//  Note: we assume 0-length split is correct given now LRR interprets offsets (reading an   extra row). Should we instead assume 1+ chars and add 1 for isUnfortunate?","FileSplit splitPart = new FileSplit(split.getPath(), uncachedSuffixStart,",838,0.7075471698113207,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/SerDeEncodedDataReader.java
"//  TODO Maybe add the YARN URL for the app.","appStatusBuilder.setAmInfo(",320,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/cli/LlapStatusServiceDriver.java
"//  Note: this particular bit will not work for MM tables, as there can be multiple         directories for different MM IDs. We could put the path here that would account         for the current MM ID being written, but it will not guarantee that other MM IDs         have the correct buckets. The existing code discards the inferred data when the","too long.",0,0,0
"//  TODO: not having aliases for path usually means some bug. Should it give up?","LOG.warn("Couldn't find aliases for " + splitPath);",94,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/ProjectionPusher.java
"//  TODO Get rid of MapOutputInfo if possible","MapOutputInfo outputInfo = new MapOutputInfo(pathInfo.dataPath, info);",870,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/ShuffleHandler.java
"//  TODO: this object is created once to call one method and then immediately destroyed.         So it's basically just a roundabout way to pass arguments to a static method. Simplify?","public class TableExport {",54,0.6666666666666666,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/TableExport.java
"//  TODO This should be passed in the TaskAttemptContext instead","dataSchema = hcatSplit.getDataSchema();",103,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/HCatRecordReader.java
"//  FIXME: use a different exception type?","throw new IllegalArgumentException("Expected match count is 1; but got:" + all);",252,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/plan/mapper/PlanMapper.java
"/*      * This is little complicated.  First we look for our own config values on this.  If those     * aren't set we use the Hive ones.  But Hive also has multiple ways to do this, so we need to     * look in both of theirs as well.  We can't use theirs directly because they wrap the     * codahale reporters in their own and we do not.      */","too long.",0,0,0
"// TODO should replace with BytesWritable.copyData() once Hive  removes support for the Hadoop 0.20 series.","return Arrays.copyOf(sourceBw.getBytes(), sourceBw.getLength());",428,0.7482993197278912,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyUtils.java
"//  If the import statement specified that we're importing to an external   table, we seem to be doing the following:      a) We don't allow replacement in an unpartitioned pre-existing table      b) We don't allow replacement in a partitioned pre-existing table where that table is external   TODO : Does this simply mean we don't allow replacement in external tables if they already exist?      If so(i.e. the check is superfluous and wrong), this can be a simpler check. If not, then      what we seem to be saying is that the only case we allow is to allow an IMPORT into an EXTERNAL      table in the statement, if a destination partitioned table exists, so long as it is actually","too long.",0,0,0
"// todo: handle Insert Overwrite as well: HIVE-18154","return false;",380,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java
"//  TODO HIVE-15865: Include information about pending requests, and last   allocation time once YARN Service provides this information.","break;",570,0.7039106145251397,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/cli/LlapStatusServiceDriver.java
"//  TODO: At this point we don't know the slot number of the requested host, so can't rollover to next available","if (requestedHostIdx == -1) {",1465,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java
"//  will this be true here?   Don't create a new object if we are already out of memory","throw (OutOfMemoryError) e;",133,0.4772727272727273,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecMapper.java
"//  TODO Replace this with a ExceptionHandler / ShutdownHook","LOG.error("Failed to start LLAP Daemon with exception", t);",545,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapDaemon.java
"//  NOTE: this can be called outside of HS2, without calling setupPool. Basically it should be         able to handle not being initialized. Perhaps we should get rid of the instance and","//       move the setupPool code to ctor. For now, at least hasInitialSessions will be false.",229,0.6933333333333334,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java
"/*    * RowResolver of outer query. This is used to resolve co-rrelated columns in Filter   * TODO:   *  this currently will only be able to resolve reference to parent query's column   *  this will not work for references to grand-parent column    */","private RowResolver outerRR;",41,0.5263157894736842,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckCtx.java
"//  TODO use stripe statistics to jump over stripes","recordReader = reader.rowsOptions(options, conf);",245,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRawRecordMerger.java
"//  @deprecated in favour of {@link HCatTable.#fileFormat(String)}. To be removed in Hive 0.16.","public Builder fileFormat(String format) {",371,0.9371428571428572,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/hcatalog/webhcat/java-client/src/main/java/org/apache/hive/hcatalog/api/HCatCreateTableDesc.java
"//  TODO: Why is this needed (doesn't represent any cols)","String udafName = SemanticAnalyzer.getColumnInternalName(reduceKeys.size());",723,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/HiveGBOpConvUtil.java
"//  TODO: this is invalid for SMB. Keep this for now for legacy reasons. See the other overload.","MapWork mapWork = Utilities.getMapWork(hiveConf);",261,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java
"//  TODO: fix this","//    assertEquals(null,stats.getSum());",1465,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestOrcFile.java
"//  This probably should not happen; but it does... at least also stop the consumer.","LlapIoImpl.LOG.error("decodeBatch threw", ex);",83,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/EncodedDataConsumer.java
"//  This seems like a very wrong implementation.","private Map<String, Object> makeOneTablePartition(String partIdent)",373,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/JsonMetaDataFormatter.java
"// todo: should make abortTxns() write something into TXNS.TXN_META_INFO about this","if (abortTxns(dbConn, Collections.singletonList(txnid), true) != 1) {",968,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java
"//  registry again just in case. TODO: maybe we should enforce that.","configureAmRegistry(newSession);",266,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPool.java
"//  TODO: lossy conversion, distance is considered seconds similar to timestamp","}",618,0.8831168831168831,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/ValueBoundaryScanner.java
"//  TODO: This is fraught with peril.","if (input != colSrcRR) {",3708,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
"//  TODO Not the best way to share the address","private final AtomicReference<InetSocketAddress> srvAddress = new AtomicReference<>(),",118,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapDaemon.java
"/*  * This maps a split (path + offset) to an index based on the number of locations provided. * * If locations do not change across jobs, the intention is to map the same split to the same node. * * A big problem is when nodes change (added, removed, temporarily removed and re-added) etc. That changes * the number of locations / position of locations - and will cause the cache to be almost completely invalidated. * * TODO: Support for consistent hashing when combining the split location generator and the ServiceRegistry. *  */","too long.",0,0,0
"//  The list is empty.   Too many concurrent operations; spurious failure.   List is drained and recreated concurrently.   Same for the OTHER list; spurious.   TODO: the fact that concurrent re-creation of other list necessitates full stop is not         ideal... the reason is that the list NOT being re-created still uses the list         being re-created for boundary check; it needs the old value of the other marker.         However, NO_DELTA means the other marker was already set to a new value. For now,         assume concurrent re-creation is rare and the gap before commit is tiny.","too long.",0,0,0
"//  The record count from these counters may not be correct if the input vertex has   edges to more than one vertex, since this value counts the records going to all   destination vertices.","String intermediateRecordsCounterName = formattedName(",102,0.6118721461187214,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/monitoring/DAGSummary.java
"//  TODO: why is this copy-pasted from HiveInputFormat?","InputFormat format = inputFormats.get(inputFormatClass.getName());",223,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java
"//  negative length should take precedence over positive value?","maxLength = -1;",701,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
"//  TODO: use DiskRangeList instead","long totalLength;",29,0.6976744186046512,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/storage-api/src/java/org/apache/hadoop/hive/common/DiskRangeInfo.java
"// todo: shouldn't ignoreEmptyFiles be set based on ExecutionEngine?","AcidUtils.Directory dirInfo = AcidUtils.getAcidState(",1206,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java
"//  All ErrorAndSolutions that ErrorHeuristic has generated. For the same error, they   should be the same though it's possible that different file paths etc","// could generate different error messages",78,0.7156862745098039,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/errors/TaskLogProcessor.java
"//  Different paths if running locally vs a remote fileSystem. Ideally this difference should not exist.","Path warehousePath;",410,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java
"// todo: should this not be passed in the c'tor?","public void setStmtId(int stmtId) {",243,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/plan/LoadTableDesc.java
"//  Currently, deserialization of complex types is not supported","case LIST:",261,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/contrib/src/java/org/apache/hadoop/hive/contrib/serde2/TypedBytesSerDe.java
"//  TODO: what about partitions not in the default location?","checkAndSetFileOwnerPermissions(fs, tablePath,",435,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/util/HiveStrictManagedMigration.java
"//  TODO: we currently put task info everywhere before we submit it and know the "real" node id.         Therefore, we are going to store this separately. Ideally, we should roll uniqueness","//       into LlapNodeId. We get node info from registry; that should (or can) include it.",888,0.6842105263157895,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java
"//  It would be nice if OI could return typeInfo...","TypeInfo typeInfo = TypeInfoUtils.getTypeInfoFromTypeString(",131,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinBytesTableContainer.java
"// TODO replace IgnoreKeyTextOutputFormat with a  HiveOutputFormatWrapper in StorageHandler","Properties props = outputJobInfo.getTableInfo().getStorerInfo().getProperties();",486,0.688,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatUtil.java
"//  @deprecated in favour of {@link HCatTable.#getNumBuckets()}.","public int getNumBuckets() {",137,0.912,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/hcatalog/webhcat/java-client/src/main/java/org/apache/hive/hcatalog/api/HCatCreateTableDesc.java
"// TODO This line can be removed once precommit jenkins jobs move to Java 8","System.setProperty("https.protocols", "TLSv1,TLSv1.1,TLSv1.2");",286,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/testutils/ptest2/src/main/java/org/apache/hive/ptest/api/client/PTestClient.java
"//  Recovery is not implemented yet for PPD path.","ByteBuffer restored = OrcInputFormatForTest.caches.cache.get(key).data;",651,0.8791208791208791,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestOrcSplitElimination.java
"//  ShimLoader.getHadoopShims().isSecurityEnabled() will only check that   hadoopAuth is not simple, it does not guarantee it is kerberos","hadoopAuth = conf.get(HADOOP_SECURITY_AUTHENTICATION, "simple");",77,0.717391304347826,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java
"//  Find the class that has this method.   Note that Method.getDeclaringClass() may not work here because the method","// can be inherited from a base class.",85,0.5,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/DefaultUDAFEvaluatorResolver.java
"//  @deprecated in favour of {@link HCatAddPartitionDesc.#create(HCatPartition)}. To be removed in Hive 0.16.","public static Builder create(String dbName,",121,0.9458128078817734,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/hcatalog/webhcat/java-client/src/main/java/org/apache/hive/hcatalog/api/HCatAddPartitionDesc.java
"//  fs.permission.AccessControlException removed by HADOOP-11356, but Hive users on older   Hadoop versions may still see this exception .. have to reference by name.","if (curErr instanceof org.apache.hadoop.security.AccessControlException",942,0.7181818181818181,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java
"// this is dumb. HiveOperation is not always set. see HIVE-16447/HIVE-16443","switch (queryState.getHiveOperation() == null ? HiveOperation.QUERY : queryState.getHiveOperation()) {",928,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
"//  TODO: Once HIVE-18948 is in, should be able to retrieve writeIdList from the conf.  cachedWriteIdList = AcidUtils.getValidTxnWriteIdList(conf);","List<String> transactionalTables = tablesFromReadEntities(inputs)",14661,0.7135678391959799,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
"//  This is a non-pool session, get rid of it.","}",1630,0.7741935483870968,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/WorkloadManager.java
"//  TODO: remove this","}",41,0.7555555555555555,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/service/src/java/org/apache/hive/service/cli/TableSchema.java
"//  Not implemented","}",225,0.9696969696969697,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/common/src/java/org/apache/hadoop/hive/common/metrics/LegacyMetrics.java
"//  TODO: we only ever use one row of these at a time. Why do we need to cache multiple?","protected transient Object[][] cachedKeys;",133,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java
"/*    * getDeserializer   *   * Get the Deserializer for a table.   *   * @param conf   *          - hadoop config   * @param table   *          the table   * @return   *   Returns instantiated deserializer by looking up class name of deserializer stored in   *   storage descriptor of passed in table. Also, initializes the deserializer with schema   *   of table.   * @exception MetaException   *              if any problems instantiating the Deserializer   *   *              todo - this should move somewhere into serde.jar   *    */","too long.",0,0,0
"//  Not pretty, but we need a way to get the size","try {",373,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFBloomFilter.java
"//  See ctor comment.   TODO: we should get rid of this","None",None,None,None
"//  in addition to that Druid allow numeric dimensions now so this check is not accurate","for (RelDataTypeField field : rowType.getFieldList()) {",2780,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
"//  converted = true; // [TODO]: should we check & convert type to String and set it to true?","if (tableFieldTypeInfo.getCategory() != Category.PRIMITIVE) {",8070,0.33707865168539325,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
"//  TODO: ideally we should make a special form of insert overwrite so that we:         1) Could use fast merge path for ORC and RC.         2) Didn't have to create a table.","String query = "insert overwrite table " + tmpName + " ";",510,0.6736842105263158,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorMR.java
"//  TODO: policy on deserialization errors","String message = null;",559,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java
"//  TODO: Do we need to keep track of RR, ColNameToPosMap for every op or","// just last one.",1626,1.0,/Users/yonekuramiki/Desktop/resarch/searchSATD-underCode/clone/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
