private static boolean areMergeable(ParseContext pctx, SharedWorkOptimizerCache optimizerCache,
State s;
bucketColumns.add(new ExprNodeColumnDesc(ci));
} while (result != null);
too long.
private final SemanticAnalyzer                              semanticAnalyzer;
for (Map.Entry<Long, Long> missingChunk : chunksInThisRead.entrySet()) {
None
too long.
System.out.println("Getting versions from " + queryDir);
throw new HiveException(ex, ErrorMsg.DATABSAE_ALREADY_EXISTS, crtDb.getName());
private PreInsertTableDesc preInsertTableDesc;
throw new RuntimeException("ContainerInfo not found for container: " + containerId +
byte[] bytes = hiveVarchar.getValue().getBytes();
update();
public Map<String, String> getTblProps() {
public void testTriggerSlowQueryExecutionTime() throws Exception {
int nParts = partNames.size();
//       supports all types
private FooterCache footerCache;
String parts[] = jobIdString.split("_");
desc = new FileSinkDesc(basePath, tableDesc, false, 1, false,
s = "select count(*) from COMPLETED_TXN_COMPONENTS where CTC_TXNID = " + txnid;
// Assert.assertEquals("expected uri", api.getAddedResource("jar"));
if (this.canUpdateFinishable) {
RowResolver inputRR = this.relToHiveRR.get(srcRel), starRR = inputRR;
if (allPartitions != null) {
public class HiveCost implements RelOptCost {
selectivity = computeFunctionSelectivity(call) * (call.operands.size() - 1);
//       for TOK_JOIN and TOK_FULLOUTERJOIN.
if (!resp.isIsSupported()) {
public Builder location(String location) {
for(ReadEntity re : partitionsRead) {
while (command.charAt(startPosition++) != '`' && startPosition < command.length()){
None
too long.
// First, we find the SELECT closest to the top.
@Override
HIVE_IN_TEST_REPL("hive.in.repl.test", false, "internal usage only, true in replication test mode", true),
String currDb = SessionState.get().getCurrentDatabase();
Set<String> result = new LinkedHashSet<>();
RowIndex[] getRowIndexes();
return new Partition();
theMRInput = (MRInputLegacy) inp.getValue();
return valuesReader.readBytes().getBytesUnsafe();
return Maps.fromProperties(properties);
too long.
@Test
case "-":
too long.
private static boolean hadBadBloomFilters(TypeDescription.Category category,
int numBuckets = (conf.getTable() != null) ? conf.getTable().getNumBuckets()
checkAndSetFileOwnerPermissions(fs, subFile, userName, groupName, dirPerms, filePerms, dryRun, recurse);
sb.append("\tFAILED container: ");
// args as child of func?
if (e instanceof MetaException) {
//       somewhere like ZK? Try to randomize it a bit for now...
too long.
setFsRelatedProperties(conf, fs.getScheme().equals("file"),fs);
private void initHS2(boolean enableXSRFFilter) throws Exception {
return new String((byte[])obj);
too long.
public String getDatabaseName() {
List<String> partitionNames = client.listPartitionNames(DB_NAME, TABLE_NAME,
public List<Order> getSortCols() {
throw new SerDeException("TypeInfo [" + typeInfo.getTypeName()
private Map<Integer, String> parentToInput = new HashMap<Integer, String>();
public class LlapProtocolClientImpl implements LlapProtocolBlockingPB {
}
return StructStreamReader.builder()
MapWork mergeMapWork = (MapWork) mergeWork;
// can prevent updates from being sent out to the new node.
caughtException = (TException)t;
}
if (ss != null && HiveConf.getVar(job, ConfVars.HIVE_EXECUTION_ENGINE).equals("tez")) {
too long.
too long.
curr = genPostGroupByBodyPlan(groupByOperatorInfo, dest, qb, aliasToOpInfo, null);
if (type.equals(Type.JOB)) {
conf.setBoolean("tez.runtime.optimize.local.fetch", true);
assert root.equals(orcSplit.getRootDir()) : "root mismatch: baseDir=" + orcSplit.getRootDir() +
}
// if (bb.isDirect()) {
this.createViewDesc.setViewOriginalText(originalText);
}
MemoryBuffer[] tailBufferArray = tailBuffers.getMultipleBuffers();
too long.
// SettableTreeReader so that we can avoid this check.
if (newCacheData == null) {
short twoScaleDown = (short) -exponent;
too long.
deleteDeltaIfExists(newPartitionPath, table.getWriteId(), newBucketId);
throw new RuntimeException("SubmissionState in response is expected!");
private static final LlapCoordinator INSTANCE = new LlapCoordinator();
TStatus tStatus = new TStatus(TStatusCode.ERROR_STATUS);
// TODO HIVE-16134. Differentiate between EXTERNAL_PREEMPTION_WAITQUEU vs EXTERNAL_PREEMPTION_FINISHABLE?
// This also gets us around the Enum issue since we just take the value
private static HCatSchema getOutputSchema(Configuration conf)
List<String> materializedViews = getTables(dbName, ".*", TableType.MATERIALIZED_VIEW);
too long.
output.add(new IntWritable(Integer.valueOf((String) value)));
private void checkForZKDTSMBug() {
super(LlapProtocolClientProxy.class.getSimpleName(), numThreads, conf, llapToken,
client.dropTable("no_such_database", table.getTableName());
}
streams.out.close();
if (useDelimitedJSON) {
public void createCatalogWithBadLocation() throws TException {
FileUtils.readFully(stream, length, bb);
TypeInfo tgtDT = null;
// TODO: we should be able to enable caches separately
private org.apache.hadoop.hive.ql.plan.TableDesc table;
too long.
assert isSplitUpdate : "should be true in Hive 3.0";
too long.
public VectorMapJoinOptimizedMultiKeyHashMap(boolean isOuterJoin,
copyName = generateOldPlanName(newName, ++i);
private void dumpFunctionMetadata(String dbName, Path dumpRoot) throws Exception {
return compile_resp;
assert !isRepeated;
String isExternal = table.getParameters().get("EXTERNAL");
return 0;
private Hive sessionHive;
pw.println(
expr = (ASTNode) child.getChild(0);
Utilities.clearWorkMap(conf);
too long.
// means serializing another instance.
private static class FetchInputFormatSplit extends HiveInputFormat.HiveInputSplit {
public Builder collectionItemsTerminatedBy(char delimiter) {
static String convertScanToString(Scan scan) throws IOException {
for (String partName : partNames) {
LOG.warn("Error during analyzeReplDump", e);
ShowCompactResponse currentCompactions = txnHandler.showCompact(new ShowCompactRequest());
too long.
too long.
e.printStackTrace();
newCall =
String tabAlias = addEmptyTabAlias ? "" : null;
boolean lastSeenRightOuterJoin = false;
private static final String HDFS_ID_PATH_PREFIX = "/.reserved/.inodes/";
if (lbDirSuffix.startsWith(Path.SEPARATOR)) {
too long.
// authorization checks passed.
hookContext = new PrivateHookContext(plan, queryState, ctx.getPathToCS(), SessionState.get().getUserName(),
public class HiveRelMdPredicates implements MetadataHandler<BuiltInMetadata.Predicates> {
if (LOG.isTraceEnabled()) {
// in type system for this.
too long.
too long.
}
return (files.size() > otherFiles.size())
String topicName = getTopicPrefix(tableEvent.getIHMSHandler().getConf()) + "." + table.getDbName().toLowerCase();
private SessionState parentSessionState;
boolean useNontaged = conf.getBoolVar(
srv.set(kv.getKey(), kv.getValue());
return null;
final List<OrcProto.Type> schemaTypes = OrcUtils.getOrcTypes(schema);
too long.
return new DoubleStreamReader(columnIndex, present, data, isFileCompressed, vectors);
this.version = version;
case '%':
// once the feature is stable
TGetResultSetMetadataResp  metadataResp;
// Don't acquire locks for any of these, we have already asked for them in DDLSemanticAnalyzer.
/*
LOG.warn("Session configuration is null for " + wmTezSession);
too long.
this.orcCvp = new OrcColumnVectorProducer(
too long.
for (int i = 0; i < includes.getReaderLogicalColumnIds().size(); ++i) {
HashPartition hashPartition = new HashPartition(1024, (float) 0.75, 524288, 1, true, null);
// those tables directory.
aggArgRelDTBldr.add(TypeConverter.convert(expr.getTypeInfo(), dtFactory));
return false;
LOG.debug("Unable to stat file as current user, trying as table owner");
if ("string".equalsIgnoreCase(type)) {
client.add_partitions_pspec(null);
if(tempDecimalBuffer == null || tempDecimalBuffer.length < length) {
}
zkConf.set(ZK_DTSM_ZK_AUTH_TYPE, "sasl");
if (! (data instanceof List)) {
throw new IllegalStateException("Unable to initialize Warehouse from clientside.");
}
footerCache = useExternalCache ? metaCache : localCache;
isBigTable =
None
final Set<Operator<?>> workOps1 = findWorkOperators(optimizerCache, op1);
break;
c.isRepeating = false;
boolean retval = true;
synchronized (CACHE_TEARDOWN_LOCK) {
Path keyfile= new Path(getPath(type) + "/" + id + "/" + key);
too long.
} catch (Exception e) {
int scale = HiveDecimalUtils.getScaleForType(ptinfo);
return FETCH_NEXT;
this.metrics = LlapTaskSchedulerMetrics.create(displayName, sessionId);
public Builder isTableExternal(boolean isExternal) {
}
return cpr;
private void moveWork(SparkWork sparkWork, BaseWork work, SparkWork targetWork) {
JSONObject configs = createConfigJson(containerSize, cache, xmx, java_home);
boolean isManagedTable = part.getTable().getTableType() == TableType.MANAGED_TABLE;
boolean alreadyCommitted = rs2.next() && rs2.getInt(1) > 0;
too long.
too long.
too long.
assert info.lastSetGuaranteed == null;
"there should be " + String.valueOf(expectedNumOCleanedFiles) + " deleted files in cm root",
public String getLocation() {
return false;
tableCols = new ArrayList<>();
}
private Deserializer inputKeyDeserializer;
private static class ReflectiveProgressHelper {
TezSessionPoolManager.closeIfNotDefault(ss.getTezSession(), true);
collSep = LazyUtils.getByte(tbl.getProperty(COLLECTION_DELIM),
private boolean hadCommFailure = false;
public class ProtobufProxy<BlockingInterface> {
LLAP_DAEMON_CONTAINER_ID("hive.llap.daemon.container.id", null,
too long.
@Override
// with the RS parent based on its position in the list of parents.
checkAcidConstraints(qb, table_desc, dest_tab);
keys[current].setFirst(JoinUtil.computeKeys(nextRow.o, keyFields, keyFieldOIs));
too long.
LOG.info("Could not make " + Warehouse.getQualifiedName(newTable) + " acid: it's " +
JoinAlgorithm oldAlgo = join.getJoinAlgorithm();
if (converter.getWindowFunctionSpec() != null) {
sb.append('=');
//which is not tracked directly but available on /jobs/<id> node via "mtime" in Stat
if (op1 instanceof ReduceSinkOperator) {
public class JoinCondTypeCheckProcFactory extends TypeCheckProcFactory {
private final static class CasLog {
final String mrValue = conf.get(dep.getKey());
gbKeys.addAll(ExprNodeDescUtils.genExprNodeDesc(rs, groupingSetsColPosition,
if (sinks.contains(table)) {
public static boolean isInsertOnlyTable(Map<String, String> params, boolean isCtas) {
//todo: rename files case
if (session != null) {
TreeMap<Object, Object> tm1 = new TreeMap<Object, Object>(m1);
private enum TableType {
client.add_partitions_pspec(null);
too long.
throw new IllegalStateException("Uninitialized Warehouse from MetastoreHandler");
public class LBExprProcCtx implements NodeProcessorCtx{
public Builder nullDefinedAs(char nullChar) {
private final static CasLog casLog = null; //new CasLog();
public class HBaseTableSnapshotInputFormatUtil {
replicas,
if (lastReduceKeyColName != null) {
//       Remains here as the legacy of the original higher-level interface (getInstance).
* @throws IOException
containsWindowing = true;
if (joinCond.getType() == HiveParser.TOK_TABCOLNAME
@VisibleForTesting
super.visit(node, ordinal, parent);
too long.
return SHIMS.cloneUgi(baseUgi);
return vertex;
String name =
conf = (configuration instanceof HiveConf) ? (HiveConf)configuration :
@Override
ecode = qt.executeClient(versionFile, fname);
if (candidateCached != null) {
if (commonTypeInfo instanceof DecimalTypeInfo) {
return null;
int numReducers = -1;
case DECIMAL:
enum OptionConstants {
None
private static final class DataLossLogger {
for (Task<? extends Serializable> task : plan.getRootTasks()) {
invokeFlag = false;
if (!(fs instanceof DistributedFileSystem)) return;
Ref<Integer> endpointVersion = new Ref<>(-1);
public List<ColumnStatistics> getPartitionColumnStatistics(String catName, String dbName, String tblName,
if (startPosition == null || arrayLength + 1 == startPosition.length) {
case BINARY:
// The hiveJarDir can be determined once per client.
if (hasIndexOnlyCols && (rgs == null)) {
if (getContext() == null) {
StringBuilder buf = new StringBuilder();
@Override
return;
int numThreads = HiveConf.getIntVar(conf, HiveConf.ConfVars.LLAP_IO_THREADPOOL_SIZE);
too long.
public String getStatsAggPrefix() {
processorContext.waitForAllInputsReady(li);
too long.
.run("create table t1 (i int, j int) partitioned by (load_date date) "
File qf = new File(outDir, fileName);
@VisibleForTesting
FileSystem fs = deltaDirectory.getFileSystem(conf);
inputExpression =
too long.
String table = rsMeta.getTableName(col + 1);
RPC.stopProxy(umbilical);
});
Class<? extends MetaException> exClass = JavaUtils.getClass(
return deltaFiles;
long partitionId = extractSqlLong(fields[0]);
public Builder bucketCols(List<String> bucketCols, int buckets) {
// if (oi instanceof TypeInfoBasedObjectInspector) {
result = new HiveVarchar();
throw new SemanticException("Activate a resource plan to enable workload management");
//         (* and $expr)
Map<String, ModuleConfig> moduleConfigs = extractModuleConfigs();
too long.
work.getPartitionDescs().remove(desc);
if (assertsEnabled) {
too long.
too long.
synchronized void setAssignmentInfo(NodeInfo nodeInfo, ContainerId containerId, long startTime) {
Path scratchDir = utils.createTezDir(ctx.getMRScratchDir(), conf);
public void testOperatorNames() throws Exception {
LOG.debug("Replacing SETCOLREF with ALLCOLREF because of the nested node "
FileSystem fs = dbRoot.getFileSystem(conf);
CodedInputStream cis = CodedInputStream.newInstance(
too long.
public Map<String, String> getPartitionSpec() {
public class HiveFilterProjectTSTransposeRule extends RelOptRule {
logRefreshError("Unable to localize jars: ", jars, t);
public String getStorageHandler() {
org.apache.hadoop.hive.metastore.api.Table table = getTempTable(dbname, name);
//authorize against the table operation so that location permissions can be checked if any
String mappedPool = mapping.mapSessionToPoolName(input, allowAnyPool, null);
if (fetchOrientation.equals(FetchOrientation.FETCH_FIRST)) {
too long.
OrcTail orcTail = ReaderImpl.extractFileTail(fileMetadata);
too long.
qt.shutdown();
// For now, this can simply be fetched from a single registry instance.
this.timeoutTimer = timeoutPool.schedule(
@Override
}
public List<String> getBucketCols() {
Field[] fields = c.getDeclaredFields();
if (!work.isExplicitAnalyze() && !followedColStats1) {
Partition part =
loadTableWork.setInheritTableSpecs(false);
clusterType = MiniClusterType.valueForString(modeStr);
private static final Logger LOG = LoggerFactory.getLogger(ContainerRunnerImpl.class);
// Table operations.
@Test(expected = NullPointerException.class)
// Find the bucket id, and switch buckets if need to
too long.
String confQueueName = conf.get(TezConfiguration.TEZ_QUEUE_NAME);
private TezSessionPoolSession createAndInitSession(
None
boolean selectStar = false;
RexCall equals = (RexCall) conj;
if (!leftPosListOfLastRightOuterJoin.contains(condn.getRight())) {
TezSessionPoolManager.getInstance().closeNonDefaultSessions();
PartitionDesc partDesc = pathToPartitionInfo.get(path);
List<String> materializedViews = getTables(dbName, ".*", TableType.MATERIALIZED_VIEW);
// use multiple lines for statements not terminated by the delimiter
too long.
txnHandler = getTxnHandler();
private static ExprNodeDesc propConstDistUDAFParams() {
if (left[leftOffset + length - 1] != rightBuffer[rightFrom + length - 1]) {
public EmbeddedCLIServiceClient(ICLIService cliService, Configuration conf) {
boolean mergeJoins = !pctx.getConf().getBoolVar(HIVECONVERTJOIN) &&
too long.
private static final String APPLY_PATCH_SCRIPT_PATH = "applyPatchScriptPath";
None
private List<ShowLocksResponseElement> getLocksWithFilterOptions(HiveTxnManager txnMgr,
public Builder fieldsTerminatedBy(char delimiter) {
populateQuickStats(fileStatus, params);
too long.
abstract boolean useHive130DeltaDirName();
jobProperties.put("mapred.output.dir", path);
LOG.trace("Non-parametrized map type: {}", field);
this.metrics = ms == null ? null : WmPoolMetrics.create(fullName, ms);
switch (schema.getCategory()) {
if (parent instanceof Filter || parent instanceof Join || parent instanceof SetOp ||
too long.
static DiskRangeList planIndexReading(TypeDescription fileSchema,
if (fields == null) {
try {
byte[] floatBytes = Float.toString((float) inV.vector[i]).getBytes();
throw new SemanticException(
processSetColsNode((ASTNode)child, searcher);
reduceWork.getEdgePropRef().setAutoReduce(null, false, 0, 0, 0);
ChannelFuture lastMap = null;
nonRecConf.setBoolean("mapreduce.input.fileinputformat.input.dir.nonrecursive.ignore.subdirs", true);
int urlIx = lastKnownGoodUrl, lastUrlIx = ((urlIx == 0) ? rmNodes.length : urlIx) - 1;
throw new HiveException(ErrorMsg.GENERIC_ERROR.getErrorCodedMsg());
/*
if (LlapIoImpl.LOG.isTraceEnabled()) {
pairs.add(new Pair<Text,Text>(new Text(mapMapping.getColumnFamily()), null));
// how to get around that.
Pattern internalPattern = Pattern.compile("_col([0-9]+)");
public List<HCatFieldSchema> getPartitionCols() {
switch (hiveTypeCategory) {
// ^(TOK_LATERAL_VIEW ^(TOK_SELECT ^(TOK_SELEXPR ^(TOK_FUNCTION Identifier["inline"] valuesClause) identifier* tableAlias)))
too long.
return ETypeConverter.ETIMESTAMP_CONVERTER.getConverter(type, index, parent, hiveTypeInfo);
if (data.stripes == null || data.stripes.isEmpty()) {
if (qb.getParseInfo().getAlias() != null) {
byte[] bytes = hiveVarchar.getValue().getBytes();
too long.
return value;
@Override
if (Double.isNaN((Double) value)) {
if (defaultPoolSize > 0) {
perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.TEZ_INIT_OPERATORS);
ShowCompactResponse allCompactions = db.showCompactions();
PrimitiveTypeEntry typeEntry = getTypeFor(method.getReturnType());
pm.deletePersistentAll(mfunc);
private static class IndexStream extends InputStream {
}
FunctionRegistry.unregisterTemporaryUDF("test_udaf");
List<String> partNames = null;
public enum GenerateCategory {
public List<HCatFieldSchema> getCols() {
if (pathToPartInfo == null) {
private static final Map<Map<Path, PartitionDesc>, Map<Path, PartitionDesc>> cache =
old_dep.setExpr(null);
public static Builder create(String dbName, String tableName, List<HCatFieldSchema> columns) {
}
/*perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.OPTIMIZER);
Collections.sort(batchToAbort);//easier to read logs
// Package permission so that HadoopThriftAuthBridge can construct it but others cannot.
private final Path indexPath;
DDLWork work = new DDLWork(new HashSet<>(), new HashSet<>(), createDbDesc);
//       This computes stats and should be in stats (de-duplicated too).
too long.
// Use RW, not PRIVATE because the copy-on-write is irrelevant for a deleted file
Map<?, ?> map = inputOI.getMap(input);
TableDesc keyTableDesc = mapJoinDesc.getKeyTblDesc();
OrcInputFormat.setSearchArgument(readerOptions, schemaTypes, conf, true);
int[] filterColumns = RecordReaderImpl.mapSargColumnsToOrcInternalColIdx(
too long.
PerfLogger perfLogger = SessionState.getPerfLogger();
SerDeEncodedDataReader reader = new SerDeEncodedDataReader(cache, bufferManager, conf,
public String getTableName() {
if (LOG.isDebugEnabled()) {
too long.
if (!validSetopParent(rel, parent))
return JAVA64;
public class HiveJoin extends Join implements HiveRelNode {
locks = lockMgr.getLocks(false, isExt);
InputStream errorStream = connection.getErrorStream();
sessionState.close(false);
final Reader.Options readOptions = OrcInputFormat.createOptionsForReader(conf);
sessionState.setIsHiveServerQuery(true);
UserPayload servicePluginPayload = TezUtils.createUserPayloadFromConf(tezConfig);
// to writing an instrumentation agent for object size estimation
switch (primitiveCategory) {
// We will estimate collection as an object (only if it's a field).
too long.
if (o instanceof java.sql.Date) {
checkAbortCondition();
return plan.getStatus() == Status.ACTIVE ? fullFromMResourcePlan(plan) : null;
ExecutorService executor = Executors.newFixedThreadPool(THREAD_COUNT);
Integer genColListRegex(String colRegex, String tabAlias, ASTNode sel,
public void deriveExplainAttributes() {
too long.
TableDesc ret = getDefaultTableDesc(Integer.toString(Utilities.ctrlaCode), cols,
MapJoinDesc mapJoinDesc = MapJoinTestConfig.createMapJoinDesc(testDesc);
if (parts.isEmpty()) {
String defaultTestSrcTables = "src,src1,srcbucket,srcbucket2,src_json,src_thrift," +
if (oldtbl
List<FieldSchema> partCols = tTable.getPartitionKeys();
return;
if (!isInitOk) {
for (String cmd : cmds) {
if (metadataCache != null) {
@SuppressWarnings("unused")
too long.
throw new IllegalArgumentException("Negations not yet implemented");
disabledNodesQueue.add(nodeInfo);
List<Partition> partitions = client.getPartitionsByNames(DB_NAME, TABLE_NAME,
public static byte[] decodeIfNeeded(byte[] recv) {
Pattern regex = null;
too long.
too long.
char nextChar = result.charAt(i + 1);
None
}
pathCreated = wh.renameDir(sourcePath, destPath, false);
return LlapFixedRegistryImpl.this.port;
too long.
sb.append(serdeConstants.MAP_TYPE_NAME).append("<").append(serdeConstants.STRING_TYPE_NAME)
too long.
@Test
too long.
String path = HiveHFileOutputFormat.getFamilyPath(jobConf, tableProperties);
warehousePath = new Path(fsUriString, "/build/ql/test/data/warehouse/");
if (types != null) {
Assert.fail("Expected an NullPointerException or TTransportException to be thrown");
taskWrapper.getTaskRunnerCallable().killTask();
too long.
private BufferedRows getConfInternal(boolean call) {
too long.
if (LOG.isInfoEnabled()) {
env.putAll(localEnv);
public final boolean logicalEqualsTree(Operator<?> o) {
